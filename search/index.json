[{"content":"给你一台服务器，怎样能够全面评估它的性能？需要测试哪些指标？请写出每个指标的具体测试原理和测试代码。假设这台服务器完全处于线下。\n这个问题看似平常，但是细细审题的话，发现还是不一样的。我们过多关注线上机器的性能，但是如果单独拿出来一台服务器，它的性能怎样呢？\n评估和压榨一台服务器性能的话，找到评估的指标，然后进行压测加观察的方式，得到性能参数。\n 观察哪些指标，意义何在？ 如何观察这些指标，测试原理，观测工具/统计代码 如何进行压测，实现测试场景，压测工具/压测代码  bench可以分类为macro bench和micro bench;对于macro bench，很多时候我们得到的是一个整体而粗略的结果，通过top我们可以看到系统负载，这些对于我们定位线上问题，分析应用程序的性能热点很有帮助，然而这并不能精确衡量一台Linux Server的性能；应用程序多种多样，线上系统目的各不相同，所以macro bench一般用于case by case的性能分析，用于解决应用的性能瓶颈；而micro bench可以定量的分析一台机器+操作系统的性能，采用相同的测试基准，通过无干扰的大量重复基本操作，比如从L1 Cache读取单个字节，耗时一个CPU时钟，在大量重复中得到较为宏观的结果，再排除loop损耗、取运行时间的损耗，可以用于基本衡量一台server的运行效率。\n1 2 3 4 5 6 7  ./bin/x86_64-linux-gnu/enough 结果为n=322229, u=5172，执行322229次 TEN( p = *p )， TEN(T)表示循环展开执行10次任务T，可使loop开销对单次执行结果的影响降低1/10； 总共用时5172000ns，单次平均0.623ns；CPU主频1999.830MHz，折合一个cycle 0.500ns，数据基本可信。   提升性能主要是把CPU喂饱，所有的性能都是从CPU的角度来衡量；内存读写快慢，单纯比较数据从内存的一个位置移动到另一个位置，这是设备厂商用来做广告用的，不是计算机系统来评估性能的；把数据从内存读到CPU，然后写到另一个地址，数据流经过CPU即是经过了计算机系统，测量这段时间才是有意义的。以数据流动为基础，其他的bench，比如pipe的性能，需要排除数据流动的时间，排除loop等时间，才是单纯pipe的带宽性能；比如context switch速度，在做bench时，需要用pipe来驱动切换进程，这里需要排除掉数据流动的时间，pipe通信的时间，其余开销时间，才是context switch的时间。\n性能评估主要对CPU、memory、disk IO和network IO四个指标，从带宽和时延两个角度评估。\n所谓带宽，不仅仅是硬件上的读写速度，而是数据从源头到达CPU，然后CPU将其送往目的地的速度，考验的是传输能力；所谓时延，更多的评估传输的效率，读取一定量的数据，数据可以在多长时间内从内存读取到CPU。\n所谓时延，其实也是另一种意义的速度，比如context switch，并没有吞吐量这个概念，但是通过将多个进程切换N次，得到总体时间，平均后可以获得单次切换用时，用以评估context switch的latency。\n测试前的准备 很多很重要的选项，比如设计测试场景，从逻辑上说通一个测试中都包含哪些时间开销，如何测量\n  测量数据块大小\n 测量从内存经过CPU拷贝到另一块内存时，如果数据量过小，比如32KB，可能这个数据只在最次L2 Cache中流动，那么测量结果将会比真实数据大；如果数据量过大，又有可能被从内存换到磁盘上 为此的应对方法是，在循环中逐渐增大一倍数据量，列出不同数据量大小的测试数据，我们其实可以分辨出哪些是L1 Cache，哪些是Cache已经失效，因为他们之间的速度差异是巨大的；另外，当我们每次跨过一页访问该页内存，如果访问时间需要好几个us，那么说明这个内存页不在内存中    测量时间\n 不论用多么精确的时钟，由于benchmark时单次任务执行时间都非常短，因此用多次loop中求整体运行时间，取平均后能够得到误差较小的结果。 lmbench的时间机制写的很精妙，比如针对不同的任务，在每次bench的时候，都会预估执行当前任务执行比如500000us，需要执行多少次，这就是需要loop的次数；比较精妙，能够照顾到即使是相同计算机，在负载不一样的时候，对不同任务有一定的适应能力，在运行足够时间后使得系统表现稳定，得到稳定的结果；使用lmbench对相同任务做bench的时候，每次执行结果的误差都不大，低于2%，这个结果我想还是很稳定的。    考虑多进程，以及编译器可能导致的问题\n 测试时运行的benchmark也都比较小，其实可以视为和单处理器没有区别；或者我们可以将进程绑定到某个CPU上。 用gcc编译benchmark，优化级别为 -O，可以避免优化过度；注意一些load指令，如果load结果没有被用到的话，可能会被优化掉    带宽性能测试   Memory: 数据从内存到CPU的带宽\n rd  单次读取512Byte数据，即128个int整数，并做相加操作以防止编译器优化；循环展开，而非在for中挨个相加 use_int(sum)等指令也是防止编译器优化的 每次读取512Byte，直到读完，算是一次读取完成 如果rd的数据块太小，比如32MB，很快被读完，这时需要调整连续循环读取iterations次，然后求平均；iterations的取值取决于根据系统负载情况，实时计算需要执行的次数。 我的CPU是至强E5-2620，包含632KB的8路组相连数据L1缓存，6256KB的8路组相连L2,15MB的20路共享L3cache 根据读取的数据块大小，我们可以逻辑上推断该数据处于哪级缓存；本机的L1/L2/L3分别为192kb/1536kb/15360kb，  数据块16KB，带宽34999.46MB/s 数据块32KB，带宽19191.28MB/s 数据块320KB，带宽10669.75MB/s 数据块8MB，带宽6394.37MB/s 数据块16MB，带宽4993.96MB/s 数据块1024MB，带宽4979.76MB/s  纯内存带宽       wr  向内存块每一个4字节写入1   rdwr  wr与rd的结合，性能略差      pipe：系统提供的IPC机制的带宽\n 测量方式：父子进程阻塞读写pipe    mmap方式读取文件：从磁盘文件读取数据的带宽\n mmap方式读取文件，首先要打开文件，然后通过mmap将fd映射到匿名内存页，mmap的内存页在读取时才会真正分配 测量方式：  一、多次循环中，open、mmap，然后读取内容，最后close 二、在测量前open文件，并进行mmap；在多次循环中，每次读取目标大小的文件数据； 前者可以得到通过读取文件数据时，mmap的纯开销；后者更贴近实际情况   测量结果  读取1024m数据；测试采用-C标志，复制文件后再进行，可以以冷数据的方式避开文件缓存 结果一：3223.58 MB/s 结果二：7948.87 MB/s      read方式读取文件：从磁盘文件读取数据的带宽\n 测量方式  一、以及包含open、read和close的带宽 二、测试单纯读文件(read)的带宽，   测量结果  读取1024m数据；测试采用-C标志，复制文件后再进行，可以以冷数据的方式避开文件缓存 一、5526.29 MB/s 二、5442.29 MB/s      注：带宽测试中使用的计算机为 至强E5-2620；之后机器收回，我采用我的一台闲置笔记本Thinkpad X220进行时延性能测试，CPU为 i5-2520M\n  时延性能测试 计算机所有的时延几乎都跟memory时延有关，做context switch时首先要store当前进程状态，然后load下一个进程。可以说准确测量计算机内存时延是评估其他时延的前提，虽然内存时延的准确测量不太容易。\n计算机系统中存在的时延主要有内存访问时延、调用操作系统组件如读写文件和系统调用等、进程创建的时延，以及进程切换导致的上下文切换时延。\n内存延时   定义:\n 刨去硬件相关的内存芯片和系统总线时延外，从总线和内存空闲时读数据，和连续读数据这两种场景的差异值得探讨。 总线空闲时读取内存数据  处理器等待从内存中取数据的时间  这个时间通常是一种标称值，有些处理器取数据时并不等待和停顿，因此测量延时可能显得小于标称值 而压测时，由于突发读取导致cache miss，导致实际延时又比这个值大 因此采用这个值也不是那么合理     连续繁忙读内存数据  连续读内存时，每次load都会跟着一个load。 连续读可能会导致时延高于空闲读，有些系统会有\u0026quot;关键字优先\u0026quot;的机制，读取某个字时不等待整个cache line填充就把该line中的要读取数据喂给CPU，然而此时cache依然处于busy状态；如果此时再有第二次load，会因为当前cache busy而停顿等待。在UltraSPARC中空闲读和连续读的差异可达35%.   所以lmbench采用的是测试连续读时的内存时延，一来连续读的测量比空闲读容易些，二来连续读更贴近实际情况。由于处理器速度很快，即使发生cache miss，引起的load latency也和连续读更接近。    测量原理\n 采用不同内存大小、不同读取跨度stride来评估、探测系统的各级内存：L1、L2、L3以及主存的时延 读取内存，每跨读一步，如果跨度很小，比如64B/128B小于Cache line，那么时延会很小，如果跨度很大，比如1M，可能就需要从主存中将该地址内容读出，时延会相应增大 每次测量前先通过分配一块内存将cache内容全部替换掉 与bw_mem不同的是，并不读取数据块，而是通过地址链表跨stride去访问内存，以此测试内存时延，可见目的和测试bandwidth不一样  地址链表的实现方式是，比如512KB的内存块buf，编址从0开始，以512 Byte为一跳 buf = 0x77889900, buf + 512 = 0x77889b00 *buf = 0x77889b00，即用内存块存放下一跳地址，这个时候 buf = 0x77889900, *(long *)buf = 0x77889b00, buf[0] = 0x00, buf[1] = 0x9b等等 这个地址链表非常高效，因为我们不关心buf存放内容，所以就利用buf的空间来存储下一跳地址；类似于如下代码   对一块内存进行跨行访问，依次采用不同大小内存和不同跨度，比如size=32KB，stride=512B，根据访问次数64次，和用时time，可以得出该次访问用时为time/64. 对于L1 Cache 32KB，L2 Cache 256KB和L3 Cache 3072KB来说，通过内存大小可以限定访问主要集中在哪级Cache，控制stride降低各级Cache miss，可以推断出访问时间。    1 2 3 4 5 6 7 8  char *buf = (char *)malloc(sizeof(char ) * 1024); memset(buf, 0, 1024); *(char **)\u0026amp;(buf[0]) = (char *)\u0026amp;(buf[512]); printf(\u0026#34;%p\\t%p\\n\u0026#34;, buf, buf + 512); char **p = (char *)\u0026amp;buf[0]; printf(\u0026#34;%p\\t%p\\n\u0026#34;, p, *p);    测量结果  测量结果如下图所示，原图见，原始数据见文件    图例： 横轴读取的内存块大小，从4KB到8MB，横轴以0.5MB为单位 纵轴是load平均延迟，单位为ns，从1ns到50ns 不同颜色的线表示不同的Stride，即每次读内存时跨越的数据长度 系统L1 Cache 32KB、L2 Cache 256KB、L3 Cache 3072KB getconf命令可以获取系统的必要信息，包括各级Cache   intel CPU的各级缓存latency的官方数据和其他解释  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20  Core i7 Xeon 5500 Series Data Source Latency (approximate) L1 CACHE hit, ~4 cycles L2 CACHE hit, ~10 cycles L3 CACHE hit, line unshared ~40 cycles L3 CACHE hit, shared line in another core ~65 cycles L3 CACHE hit, modified in another core ~75 cycles remote L3 CACHE ~100-300 cycles Local Dram ~60 ns Remote Dram ~100 ns   对于测量各级Cache的latency来说，需要对每级进行特定分析。 L1 Cache Latency 对于intel i5-2520M来说，L1 Cache的32KB容量，cache line长64B，8路组相连，每路4KB大小，有64组cache line供选择；在不考虑其他因素的情况下，每32KB连续数据中一定会产生L1 miss，每4KB连续数据一定会有一次组内选择哪路cache line存储数据，可能产生cache miss；一旦产生cache miss，会进行L2乃至下一级的读取，造成时延加大，影响L1 cache的latency测量。因此为了避免cache miss带来的影响，在测量L1时尽量采用小步长，小内存块进行逼近，得到尽可能精确的L1 Cache的latency。\nL2 Cache Latency 对于L2 Cache来说，所用内存块和步长应该加大，尽量使L1 Cache失效，访问L2；也应该注意步长不能过大，造成L2 失效；所采用的内存块大小倒不是关键，因为即使再大的内存块也需要按照步长读取\nL3 Cache Latency L3 Cache的latency反而难以测量，原因一是L3可能是多核共享的，容易受干扰，二是相比L1/L2和DRAM的性能差异，L3与DRAM的访问差异显得不那么大。此外，TLB的因素也应该考虑在内。\n拜读完CacheMemory.pdf的第一章后，发现自己考虑的太少了，所以以上的测量方法其实只能说明一个大概，并不能极具说服力的得出时延数据就是L1 Latency。\nTODO   measure cache line measure tlb  tlb和cache区别   lwn IBM关于lmbench对mem latency的深度benchmark stackoverflow_1 stackoverflow_2 读内存过程  CPU读数据的一系列过程 What Your Computer Dos While You Wait 译文 intel: Cache相关的问题    调用系统组件（系统调用） 调用操作系统的入口，一般指系统调用，比如读写设备的read/write，比如getpid()或getimeofday()。\n对于前者的bench，选择操作/dev/null设备，每次写一个字，一般为4字节，经历了用户进程发起系统调用、转入内核态、查询文件描述符、VFS层等一整个过程，测量这个过程的时间；选择该设备的原因是所有操作系统都没有对该设备进行优化\n而后者，系统调用比如getpid、gettimeofday，各平台不同优化，甚至是实现在用户层，不过在Linux上仍然是实现在内核态的。所以可以通过getpid()了解基本开销，通过写/dev/null设备了解整体开销。\n 原理：  在循环中调用getpid()和write(fd, *buf, 1)   结果：  getpid  Simple syscall: 0.0540 microseconds   write to /dev/null  Simple write: 0.0897 microseconds      信号处理耗时   建立信号sigaction耗时\n 测量原理：  在本进程内部，调用sigaction建立信号 time = sig_installation   测量结果：  Signal handler installation: 0.1492 microseconds      发送信号耗时 kill(pid, sig)\n 测量原理：  设置不捕获信号，进程内向自己发送信号，kill(pid_self, SIGUSR1) time = sig_send   测量结果：  0.1240 microseconds      捕获并处理信号耗时\n 测量原理：  设置捕获信号，向自己发送信号，kill(pid_self, SIGUSR1) sig_handle = total_time - sig_installation - sig_send   测量结果：  Signal handler overhead: 0.9763 microseconds      捕获信号耗时：\n 测量原理：  在bench进程中以只读方式mmap一段内存，如果试图写这块内存，则会一直触发SIGBUS和SIGSEGV信号 在触发信号前设置SIGBUS和SIGSEGV的处理函数，在处理函数中不执行任务，只动态调整捕获次数；捕获次数达到一定数量时，评估单次处理用时 由于重复捕获信号，并且信号处理函数里基本没有任务，可以认为这段时间是捕获信号耗时，或者说是信号传递耗时   测量结果:  Protection fault: 0.4928 microseconds      补充：\n 由于第三项与第四项采用不相同的测试场景测量，前三项是一个场景，进程设置信号捕获，并向自身发送信号，然后捕获处理，第四项是产生page fault后重复触发信号，bench程序统计捕获次数和耗时计算得来。    创建进程开销 进程相关的benchmark主要是衡量几个进程原语：创建新进程、执行新的程序以及上下文切换。\n 创建进程的开销：fork process  测量原理：  通过fork创建子进程，子进程直接执行exit退出；所以最后测量的时间是：fork() + exit()  其实这里的开销还包括了父进程调用wait系统调用和父子进程上下文切换的开销，前者在本机50ns、后者在本机1.93us，相对进程创建来说非常小     测量结果：  Process fork+exit: 107.1001 microseconds     创建进程并执行新程序的开销：fork + execlp  测量原理：  不光测量fork的时间开销，还要通过execlp系统调用加载新程序/tmp/hello，打印一条hello world信息   测量结果：  Process fork+execve: 372.2349 microseconds      上下文切换的开销   上下文切换会受到多个因素影响：切换进程数量、每个进程自身的内存大小，以及Cache在其中的影响\n  测量结果：\n  intel的超线程技术 CPU的超线程机制通过复制、分区和共享 Intel NetBurst 微结构管道中的资源，使得一个物理处理器能包含两个逻辑处理器。逻辑处理器有自己的处理器状态、指令指针、重命名逻辑以及一些较小的资源，共享的资源有乱序执行引擎和高速缓存。超线程机制HT利用各资源的速度差异，在时间上并行模拟出两份计算资源，理论上讲提供多一倍的计算能力，而代价是，既然涉及到共享，那么在一些操作中会因为共享和竞争而有性能损耗。下表是超线程对Linux API的影响，采用lmbench测试结果，这份bench报表在运行有linux-2.4.19内核的Intel Xeon处理器上，主频1.60GHz。\n我认为上表中，ht没有影响的API是一些lmbench单进程可以测试的项，比如read、write等，而ht有影响的选项，比如管道延迟、进程fork+exit等，是lmbench需要发起多个进程进行bench，这些进程在逻辑处理器间有竞争，导致性能有些下降。从报表中可以看到，开启ht的bench结果在有些项目中耗时加长，但是比较小，但是整个系统获得了一倍的计算资源。\nintel的turbo技术 linux的睿频工具可以实时看到CPU的运行频率\n1  sudo i7z   以笔记本CPU i5-2520M 为例，在CPU空闲时主频低至\n1 2 3  Core [core-id] :Actual Freq (Mult.) C0% Halt(C1)% C3 % C6 % Temp VCore Core 1 [0]: 1366.96 (13.72x) 24.9 81.6 4.69 0 62 1.1008 Core 2 [2]: 1381.01 (13.86x) 23.3 79.2 7.89 0 62 1.1008   甚至更低\n运行lmbench时，CPU满载，主频可达\n1 2 3  Core [core-id] :Actual Freq (Mult.) C0% Halt(C1)% C3 % C6 % Temp VCore Core 1 [0]: 2989.17 (30.00x) 16.9 73.4 6.37 0 69 1.1409 Core 2 [2]: 3050.32 (30.61x) 99.1 0 0 0 73 1.1409   可见睿频的主要作用是动态调整CPU主频适应处理任务，当我们运行lmbench时，理所应当的CPU将会满载运行，所以计算CPU主频时完全不用担心睿频导致的频率变化，总是接近最高值的。\nlmbench测试框架  fork出child进程，作为bench执行体；parent作为控制体； parent和child通过pipe互相通信，child通知parent准备好，parent通知child开始，child执行完毕后通知parent完成；parent取完执行结果，通知child结束。 应用程序中child首先执行init程序，将准备工作做好；比如如果是bw_mem中的cp benchmark，需要首先分配好src内存块，然后分配dst内存块，准备好后即返回，init结束； init结束后，benchmp框架进行下一步，while循环中执行benchmark进入warmup状态 child执行benchmark是以状态机方式实现的  首先是warmup状态，在该状态中childparent发送ready信号，阻塞等待parent的start信号。child收到start后，状态设置为timing_interval，取iterations为1，执行benchmark程序；执行完后重入while循环； 然后切换到timing_interval状态。由于之前在warmup状态执行以iterations为1的benchmark，重入循环时满足state-\u0026gt;need_warmup == 0，所以开始统计上一次benchmark开始到现在的用时，并刨去t_overload和l_overload；进入状态机的timing_interval状态，该状态评估本次benchmark的用时是否符合期望，如果符合，将本次bench结果记录，并设置状态为cooldown；如果不符合期望，分情况，如果用时result小于150，说明单次benchmark用时太短，需要多次迭代得到总结果，因此将iterations扩大8倍去计算；如果用时大于150，说明单次benchmark执行时间是够的，但是需要微调迭代次数，按照 enough * 1.1 / (iterations / result)；所以说timing_interval状态是用来在bench过程中微调迭代次数以使执行时间符合预期的过程。 在cooldown状态，child阻塞等待取结果信号，并将结果写回response管道中；执行清理函数后，得到parent发出的exit信号，结束自己。   最终结果存储在全局变量iterations和stop_tv中，其实是通过get_time()和get_n()，以及set_time()/save_n()或者set_results()中，并在用户程序的结束时进行计算。  ","date":"2016-10-23T16:43:44+08:00","permalink":"https://bg2bkk.github.io/p/%E6%80%8E%E6%A0%B7%E5%B0%BD%E5%8F%AF%E8%83%BD%E5%85%A8%E9%9D%A2%E7%9A%84%E8%AF%84%E4%BC%B0%E4%B8%80%E5%8F%B0%E6%9C%8D%E5%8A%A1%E5%99%A8%E7%9A%84%E6%80%A7%E8%83%BD/","title":"怎样尽可能全面的评估一台服务器的性能"},{"content":"理解TCP/IP协议栈 实现网络应用  遇到好文章我就想给翻译下来，觉得写的很好，现在cubrid的一篇TCP/IP相关的文章详细介绍了TCP协议栈，以及收发数据包的流程，非常有启发意义，所以我就想翻译一下，做个记录。将TCP/IP协议栈在一篇文章内讲明白是不可能的，所以本文能够做到的是讲清楚TCP/IP协议栈收发数据包的流程，我们要做的是首先了解大致流程，然后尝试根据TCP/IP协议和拿着代码去理解。由于我本人能力十分有限，译文都是我个人理解，所以会有大量错误，希望您能帮我纠正。如果您想转载，我必须提醒一句我这个译文是自己学习之用，并未取得版权方同意，因此首先做免责声明。\n 我们不能想象没有TCP/IP，互联网服务将会是何种情况。所有我们开发和使用的Internet服务都基于一个坚实的基础：TCP/IP。理解数据如何在网络中传输可以帮助你通过优化和调试的方式来提升程序性能，引入和使用新的技术。\n本文将通过在linux操作系统和硬件层面的数据流和控制流来描述网络技术栈的整体执行流程。\nTCP/IP的关键字 我如何设计网络协议，能够在保持数据不丢失不乱序的情况下，快速传输数据？\nTCP/IP协议为这些考虑而设计，如下是理解TCP/IP协议栈需要了解的关键字\n确切的说，TCP和IP是两个不同的层，理应分开描述；不过惯例上一直将他俩合成一个概念来讲   CONNECTIONI-ORIENTED，面向连接的  首先通信双方需要建立一条连接，一条TCP连接的标识符是local IP address, local port number和remote IP address, remote port number组成的四元组   BIDIRECTIONAL BYTE STREAM, 双向数据流传输  使用字节流实现双向传输   IN-ORDER DELIVERY，顺序发送  接收方按照数据从发送方发送的顺序接收；采用32bit整型作为数据包的序号，以实现顺序传输   RELIABILITY THROUGH ACK，通过ACK实现可靠性  当发送方发送数据后，没有收到接收方传来的该包的ACK，发送方将重新发送该数据。因此，发送方的TCP需要将未被ACK的数据缓存起来。   FLOW CONTROL，流控  发送方都想尽可能的发送数据给接收方，但是对端也得能够有能力接收，因此接收方要发送自己能够接收的最大数据量给发送方知道，最终发送方发出数据量由接收方的接收窗口决定。   CONGESTION CONTROL，拥塞控制  拥塞窗口是除接收窗口之外的另一个通过限制在途数据流大小以防止网络拥塞的方法。发送方尽可能多的发出拥塞窗口允许的数据量，该窗口大小有诸多方法可以实现，Vegas、Westwood、BIC或者CUBIC。不同于流控中的接收窗口，拥塞窗口是由发送方单独确定的。    数据发送 如下图所示，一个网络栈有很多层，图中包含各层类型。\n图中虽然有多层，但可以简要分为3类：\n User area 用户区 Kernel area 内核区 Device area 设备区  在user area和kerne area处理的任务都是由CPU完成的，所以user area和kernel area统称为host来与device area加以区分。在这里的device是Network Interface Card(NIC)，也就是网卡，用于收发数据，NIC是一个比我们常用的\u0026quot;局域网网卡\u0026quot;更准确的术语。\n让我们大致看看user area，首先应用程序准备好数据(右上角的user data灰色框)，然后调用***write()***系统调用发送数据。假设所用的socket(图中write调用的参数fd)合法，那么当发起系统调用后，发送流程切换到kernel area。\nPOSIX系列操作系统例如Linux和Unix在应用程序通过一个file descriptor，即文件描述符fd来表示所用的socket。在POSIX系系统中，socket也是一种文件，应用程序使用的fd在进程中有其对应的file structure，与socket对应（file-\u0026gt;private_data指向对应的struct socket，此处不影响理解），图1中的文件层进行简单的检查(VFS对write()的权限检查)，然后通过调用socket的相关函数最终实现write()。\n内核中每个socket有两个buffer：\n 一个是send socket buffer，发送缓冲区，用于发送 一个是receive socket buffer，接收缓冲区，用于接收  当write系统调用被调用时，待发送数据从用户空间复制到内核内存中，然后添加进发送缓冲区的链表末尾。这样就可以按顺序发出数据。图一中的\u0026rsquo;Sockets\u0026rsquo;那层对应的右边灰色的小格子指向socket send buffer中的数据。然后，调用TCP/IP协议栈。\n每个tcp类型的socket都有一个***TCP Control Block(TCB)***tcp控制块的数据结构，TCB包括了一个TCP连接所需要的成员，比如connection state连接状态(LISTEN, ESTABLISHED, TIME_WAIT等)、receive window接收窗口，congestion window拥塞窗口、sequence number包序号和resending timer重传定时器等。可以认为一个TCB 代表一条TCP连接。\n如果当前TCP状态允许数据传输，会新建一个新的TCP segment(packet，报文)；否则系统调用结束并返回错误码。\n下图是一个TCP报文，包括两个TCP片段：TCP header和Payload，如图2所示\npayload部分是待发送的数据，处于socket的未确认(unACK)发送缓冲区，每个包的payload的最大长度由对方接收窗口大小、拥塞窗口大小和maximum segment size（MSS，最大报文长度）共同决定。\n然后计算packet的checksum校验码，实际上，checksum计算目前由NIC用硬件实现，放在这里只是为了逻辑通顺。\n然后TCP报文进入下一层IP层处理，IP层添加IP头部和checksum，并进行IP路由选择。IP路由选择是选择下一跳的过程。当IP层计算并添加IP头部校验checksum后，将数据包发送到下一层Ethernet层，即数据链路层。Ethernet层采用ARP协议搜索查询下一跳IP的MAC地址，然后向报文添加Ethernet头部。添加完Ethernet头部后，host部分的报文就处理完毕了。\n在IP路由选择执行完毕后，根据结果选择哪个NIC作为传输接口；在host处理完报文后，调用NIC驱动发送数据。（一定要注意，NIC和NIC驱动不是一体的，前者是NIC网卡硬件，后者是运行在host和内核的驱动程序，硬件是CPU）\n此时，如果一个抓包软件比如tcpdump或者wireshark正在运行，kernel将报文从内核态复制一份到这些软件内存中。同样的，如果是抓接收到的包，也同样是从NIC驱动这里抓取的。一般来说，流量整形工具也是在这一层实现的。\nNIC驱动程序通过厂商制定的网卡与主机的通信协议向NIC请求发送packet。\nNIC收到发送网络包请求后，将报文复制到自己的内存中然后发送到网络。发送前，为遵守以太网标准，还要修改一些标志，包括packet的CRC校验码，IFG（Inter-Frame Gap）包内间隔和报文头等标志；CRC校验码用于数据保真，其他二者用于区分其实包还是中间包（需要翻译调整）。数据包传输速度根据网络物理速度和以太网流控制条件来调整，一般取低值，并留有一定余量。\n当NIC发出一个数据包，NIC向CPU发出中断；每个中断有其自己的中断号，操作系统根据中断号调用对应的驱动程序处理中断，驱动的中断处理函数是NIC驱动在OS启动时注册中断回调函数；当中断发生时，OS调用中断服务程序，然后中断服务程序向OS返回发送完成的数据包（编号）。\n至此我们讨论了应用程序数据发送的流程，贯穿kernel和NIC设备。而且，即使没有应用程序的写请求，kernel可以调用TCP/IP协议栈直接发送数据包。例如，当收到一个ACK后并且得知对端接收窗口扩大，kernel将自动的把仍在发送缓存中的数据打包，直接发出。\n数据接收 现在我们看看数据的接收流程，当数据包到来的时候，网络栈是如何处理的，如图3所示。\n首先，NIC将数据包写入自身内存，检查该包是否CRC合法，然后将该包发送给host的内存，host的内存是NIC驱动事先向kernel申请的内存，用于接收数据包，当host分配成功，通过NIC驱动告诉NIC这块内存的地址和大小。如果NIC driver没有实现分配好内存，NIC收到数据包后会直接丢弃。\n当NIC将数据包写入到host的内存缓冲区后，NIC向host 操作系统发出中断信号。\n然后，NIC驱动来确认它是否可以处理这个新包，这个过程使用的是NIC和NIC驱动之间的通信协议。\n当驱动需要将数据包发送到上一层时，这个数据包必须被包装成OS可以理解的包格式。比如，linux上的sk_buff，BSD系列内核的mbuf结构，或者MS系统的NET_BUFFER_LIST结构。NIC驱动将封装后的数据包转给上层处理。\n链路层Ethernet层检查数据包是否合法，然后根据数据包头部的ethertype值选择上层网络协议。IPV4类型的值为0x0800。本层的工作就是去掉数据包的Ethernet头部，传送给上层IP层。\nIP层同样首先检查数据包合法性，采用检查IP头部的checksum字段的方式。在逻辑上进行IP路由选择，决定是否本机操作系统处理这个包，还是转发给另一个系统。如果本机处理数据包，那么IP层将根据IP头部的协议proto值选择上层传输层协议，比如TCP协议的proto值是6.本层的工作就是移除IP头部，发送给上层TCP层。\n同样的，TCP层检查数据包的checksum是否正确。之前说过，TCP的checksum也是由NIC计算得到的。（可以理解这些CRC校验的工作都是由NIC硬件实现的，如果硬件层没有校验通过，可以直接在网卡丢弃）\n然后开始采用IP:PORT四元组作为标志搜索这个数据包对应的TCP Control Block。找到TCP控制块后就找到了TCP连接，根据包协议处理数据包。如果是收到新数据，那么将其加入socket接收缓冲区中。根据TCP状态，协议栈发送TCP回复包（比如ACK包）。现在TCP/IP的接收数据流程完成了。\nsocket接收缓冲区的大小是TCP接收窗口大小。数据接收时，TCP接受窗口扩大时TCP的吞吐能力增大；在此之前，socket的缓冲区大小由应用程序或者操作系统配置来调整，而现在新的网络栈具有自动调整接受缓冲区大小的功能。\n当应用程序调用read系统调用时，从user area切换到kernel area，数据从socket的缓冲区复制到user area，然后从socket缓冲区中释放。然后调用TCP协议栈；因为socket缓冲区有了新的空间，所以TCP增大接受窗口；然后根据该连接的状态发送ACK包或者其他包比如RST。如果进行read系统调用时没有新数据包，那么read()就终止返回。\n网络栈进化方向 以上描述的网络栈各层的功能都是一些基本的功能。90年代早期的网络栈功能比以上描述的还少。不过，目前最新的网络栈的功能更加丰富，复杂度更高，这些新功能根据用途有如下分类：\n Packet Processing Procedure Manipulation, 控制修改包处理流程  类似于Netfilter(firewall, NAT)和流量控制。通过在数据包基本处理流程中插入用户代码可以实现不同功能。\n Protocol Performance, 协议性能提升  目的是在同样的网络质量情况下，提升吞吐量、降低时延，提高稳定性。多种拥塞控制算法和附加TCP功能比如SACK（选择确认）就是这类功能。通过协议提升性能在本文中不作重点讨论。\n Packet Processing Efficiency, 数据包处理效率  包处理效率相关的功能旨在提升每秒能够处理最大量的数据包，通过降低单机处理数据包的CPU时间、内存占用和内存访问次数。目前有多种降低系统时延的尝试，包括并行处理、头部预测、零拷贝、单一副本、免校验、TSO、LRO和RSS等。\n网络栈的控制流 现在我们可以从更细节的角度观察linux网络栈的内部流程。就像其他非网络栈的子系统，linux的网络站以事件驱动的方式，当网络事件发生时进行相应处理，也就是说网络栈内只有一个进程或者控制流处理运行（其实就是kernel）。上文的图1和图3表示数据包的简化版控制流，图4将显示更多细节。\n图4的控制流(1)中，应用程序通过系统调用比如read()和write()调用TCP/IP协议栈，在这里没有数据包的发送，需要经过协议栈传输。控制流(2)与控制流(1)的不同之处在于，它要求调用TCP后直接发送数据包，参考raw socket的用法。它创造一个数据包然后将该包发送到NIC驱动前的一个队列中，然后队列的实现方式决定何时将该包发送给NIC驱动。这其实就是linux中的队列方式(queue discipline, qdisc)，linux的流量控制功能就是操作这个队列实现的，默认的操作方式是FIFO，先进先出。通过使用其他队列控制方式，linux可以实现多种效果，比如人工控制丢包、包延迟和流量限制等等功能待查：同一个队列的不同控制方式discipline，还是多个队列。在控制流(1)和(2)中，应用程序的处理流程最终将调用NIC驱动。\n控制流(3)表示TCP用到的一些定时器，比如当TIME_WAIT定时器超时后，TCP协议栈将响应并删除超时的连接。\n与控制流(3)类似，(4)表示超时后TCP将处理一系列待处理的数据包。比如，当重传定时器超时后，未得ACK确认的包将被重传。\n控制流(3)和(4)显示定时器软中断的处理流程。\n当NIC驱动收到NIC中断，它将释放已传输完成的数据包。大部分情况下，NIC驱动的处理流程在这里就终止了。控制流(5)表示数据包在传输队列中累积，NIC驱动请求软中断，然后软中断处理函数从发送队列中将累积的数据包发送给NIC驱动（请结合(5)左边的黑线）。\n当NIC驱动收到中断并且发现一个新的数据包到来，它将请求软中断。处理接收数据包的软中断调用NIC驱动接收并将收到的数据包传给上层处理。在LInux中，如上描述的处理接受数据包的处理方式称为New API(NAPI)。NAPI与轮询类似，因为NIC驱动并不直接向上层发送数据，而是上层从NIC驱动中主动拿数据包，这段代码称为NAPI poll(NAPI轮询)。这里的实现方式有很多，比如NIC收到大量包时，就不会采用中断方式接收，而是改为轮询方式，总之实现比较精妙，值得看看\n控制流(6)显示TCP协议栈接受数据包的完整处理流程，控制流(7)表示请求额外数据包发送的过程比如ACK包？。控制流(5)、(6)、(7)都是由NIC发起中断，软中断服务程序处理NIC中断实现的。\n怎样处理中断然后接收数据包 中断处理是复杂的，毕竟你需要理解与接受数据包有关的各个环节。图5显示了中断处理流程图。\n想象下CPU 0正在执行应用程序，这个时候NIC收到一个数据包，向CPU 0产生一个中断。然后CPU执行内核中断处理程序。内核通过中断号调用中断处理程序，响应NIC驱动，然后NIC驱动释放已发送完成的数据包，然后调用napi_schedule()函数去准备接受数据包，该函数请求软中断并返回，此时NIC驱动的中断处理程序结束，控制权交回内核软中断处理程序。硬中断上下文执行完成后，软中断开始执行（这里是内核的tasklet或者work_queue了吧，处于中断上下文的话，只能是tasklet，涉及到阻塞方法，比如与NIC设备的通信，就需要wait_queue了），软硬中断上下文都是由同一个进程执行的（linux kernel）。不过，软硬中断的执行栈不一样，硬中断将会屏蔽硬件中断，软中断执行期间是不屏蔽的（老生常谈）。\n内核软中断程序处理napi_schedule()产生的softirq，调用net_rx_action()处理收到的数据包，这个函数调用驱动的poll()方法。poll()方法调用netif_receive_skb()方法收取所有数据包，然后将其逐层向上层传送。处理完以上软中断后，应用程序从断点继续执行，此时可以开始调用系统调用比如read读取数据。\n这就是CPU从收到硬中断到完成接收数据包的完整过程，Linux、BSD和MS Windows系统都是大同小异的。\n当你查看服务器CPU使用率时，有时你看到只有一个CPU在辛苦的执行软中断，这个现象我们上文描述的可以解释，只有CPU 0在响应网卡中断，使用多队列网卡、RSS和RPS（在软件层面模拟实现硬件的多队列网卡功能）可以解决这个问题，将软中断绑定到多个CPU上。\n相关数据结构 下文列出一些关键性的数据结构\nsk_buff structure 首先，sk_buff结构体或者skb结构体表示一个数据包，图6表示sk_buff结构体的主要部分。虽然sk_buff的功能越来越丰富，也越来越复杂，但图6足以说明sk_buff相关的通用方法。\nsk_buff包括数据包的Data部分和元数据部分 sk_buff直接包括数据包的数据部分，或者用指针指向它。在图6中，sk_buff结构体中的data成员指向一个skb_shared_info结构体的Ethernet到buffer成员之间的内存，而额外的数据由skb_shared_info的frags成员指向具体的内存页。\nsk_buff的一些基本信息，比如头部信息和包数据长度存在元数据区域(元数据待理解，初步认定是skb_shared_info)。如图6所示，链路层头部mac_header、网络层头部network_header和传输控制层头部transport_header都有对应的指针依次指向从元数据开始的地方。这种方式使得TCP协议处理更容易一些。\n如何添加或删除头部 当在网络协议栈各层处理时，数据包的头部将会被添加或者删除，此时采用指针来移动到不同层的header位置最为方便高效，比如如果要删除Ethernet头部，只需要将头部指针，即sk_buff的head成员指向上一层IP头部位置即可。\n怎样合并或者分解数据包 在向socket缓冲区高效添加或者删除数据包的数据量时，采用链表的方式最为方便。sk_buffer的next和prev指针成员的目的就是在此。\n快速分配或者释放内存 当创建一个packet时，一个sk_buff结构体要被分配，这里需要快速分配器。比如，如果数据在万兆网卡传输，那么每秒最多将超过百万包被分配和释放。\nTCP Control Block 第二，需要有一个结构体来表示一条TCP连接。此前，它被笼统的称为TCP控制块。Linux使用tcp_sock结构体表示TCP Control Block，如图7所示，你可以看到socket、tcp_socket和struct file之间的关系。\n当一个系统调用执行时，首先搜索进程的fd对应的struct file结构，对于类Uinx操作系统来说，一个socket、一个文件或者一个设备对于普通文件系统来说都抽象成struct file结构。因此，文件系统包括了基本信息，对于一个socket结构体来说，struct socket包含了与socket相关的信息，以及一个file指针，该socket结构体同样有一个struct sock类型的指针sk成员，struct sock可强制类型转换到struct tcp_sock（参考tcp_sk函数）。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28  // socket 结构体, include/linux/net.h  /** * struct socket - general BSD socket * @state: socket state (%SS_CONNECTED, etc) * @type: socket type (%SOCK_STREAM, etc) * @flags: socket flags (%SOCK_NOSPACE, etc) * @ops: protocol specific socket operations * @file: File back pointer for gc * @sk: internal networking protocol agnostic socket representation * @wq: wait queue for several uses */ struct socket { socket_state\tstate; kmemcheck_bitfield_begin(type); short\ttype; kmemcheck_bitfield_end(type); unsigned long\tflags; struct socket_wq __rcu\t*wq; struct file\t*file; struct sock\t*sk; const struct proto_ops\t*ops; };   socket结构体指向的tcp_sock结构体除了支持TCP协议类型外还有别的比如sock，inet_sock等类型，这点可以视为某种意义上的多态。\n所有TCP协议的状态信息保存在tcp_sock结构体中，比如TCP的序号sequence number、接受窗口receive window、拥塞窗口和重传定时器等。\nsocket发送缓冲区和socket接收缓冲区都是tcp_sock的sk_buff链表；tcp_sock的dst_entry成员，存储IP路由选择结果，避免再次进行路由选择，dst_entry可以进行ARP结果的快速检索，比如对端MAC地址。dst_entry是路由表的一部分，由于路由表的复杂结构，所以不在本文讨论，总之记住dst_entry可用来选择传输本数据包的网络设备NIC，NIC就是dst_entry指向的net_device成员。\n因此，通过struct file我们可以很容易的找到与TCP连接的所有信息，只占用少量内存，几KB而已（有很多功能添加进来，所以这部分内存从过去到现在不断增长）。\n最后，我们看下TCP连接的查找表，这是一个用于查找所收到数据包对应的TCP连接的哈希表，索引通过数据包的IP:PORT四元组进行Jenkins哈希算法计算得到。选择这个算法的原因是考虑到防范对此哈希表的攻击（待查）。\n追踪代码：如何发送数据 我们通过追踪阅读Linux kernel源码来学习TCP/IP协议栈如何执行，通过常用的读数据和写数据来观察。\n首先，应用程序调用write()来实现数据发送\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21  SYSCALL_DEFINE3(write, unsigned int, fd, const char __user *, buf, ...) { struct file *file; [...] file = fget_light(fd, \u0026amp;fput_needed); [...] ===\u0026gt; ret = filp-\u0026gt;f_op-\u0026gt;aio_write(\u0026amp;kiocb, \u0026amp;iov, 1, kiocb.ki_pos); struct file_operations { [...] ssize_t (*aio_read) (struct kiocb *, const struct iovec *, ...) ssize_t (*aio_write) (struct kiocb *, const struct iovec *, ...) [...] }; static const struct file_operations socket_file_ops = { [...] .aio_read = sock_aio_read, .aio_write = sock_aio_write, [...] };   当应用程序调用write()系统调用，内核在文件层执行write()，首先找到fd对应的struct file，然后调用file_operations中的aio_write()，它是一个函数指针，最终调用的是socket_file_ops的sock_aio_write()方法。kenerl中通过函数表的方式实现接口，这点十分常见，但是对于TCP的具体指向方式，可以以后详查(TODO LIST)。\n接下来是sock_aio_write()的具体调用过程。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34  static ssize_t sock_aio_write(struct kiocb *iocb, const struct iovec *iov, ..) { [...] struct socket *sock = file-\u0026gt;private_data; [...] ===\u0026gt; return sock-\u0026gt;ops-\u0026gt;sendmsg(iocb, sock, msg, size); struct socket { [...] struct file *file; struct sock *sk; const struct proto_ops *ops; }; const struct proto_ops inet_stream_ops = { .family = PF_INET, [...] .connect = inet_stream_connect, .accept = inet_accept, .listen = inet_listen, .sendmsg = tcp_sendmsg, .recvmsg = inet_recvmsg, [...] }; struct proto_ops { [...] int (*connect) (struct socket *sock, ...) int (*accept) (struct socket *sock, ...) int (*listen) (struct socket *sock, int len); int (*sendmsg) (struct kiocb *iocb, struct socket *sock, ...) int (*recvmsg) (struct kiocb *iocb, struct socket *sock, ...) [...] };   sock_aio_write()函数从struct file中获取struct socket，然后调用socket的sendmsg方法，sendmsg依然是个函数指针，指向的是struct socket中的proto_ops函数表的sendmsg，IPv4协议族的TCP类型的proto_ops操作表是inet_stream_ops，将sendmsg实现为tcp_sendmsg。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56  int tcp_sendmsg(struct kiocb *iocb, struct socket *sock, struct msghdr *msg, size_t size) { struct sock *sk = sock-\u0026gt;sk; struct iovec *iov; struct tcp_sock *tp = tcp_sk(sk); struct sk_buff *skb; [...] mss_now = tcp_send_mss(sk, \u0026amp;size_goal, flags); /* Ok commence sending. */ iovlen = msg-\u0026gt;msg_iovlen; iov = msg-\u0026gt;msg_iov; copied = 0; [...] while (--iovlen \u0026gt;= 0) { int seglen = iov-\u0026gt;iov_len; unsigned char __user *from = iov-\u0026gt;iov_base; iov++; while (seglen \u0026gt; 0) { int copy = 0; int max = size_goal; [...] skb = sk_stream_alloc_skb(sk, select_size(sk, sg), sk-\u0026gt;sk_allocation); if (!skb) goto wait_for_memory; /* * Check whether we can use HW checksum. */ if (sk-\u0026gt;sk_route_caps \u0026amp; NETIF_F_ALL_CSUM) skb-\u0026gt;ip_summed = CHECKSUM_PARTIAL; [...] skb_entail(sk, skb); [...] /* Where to copy to? */ if (skb_tailroom(skb) \u0026gt; 0) { /* We have some space in skb head. Superb! */ if (copy \u0026gt; skb_tailroom(skb)) copy = skb_tailroom(skb); if ((err = skb_add_data(skb, from, copy)) != 0) goto do_fault; [...] if (copied) tcp_push(sk, flags, mss_now, tp-\u0026gt;nonagle); [...] }   tcp_sendmsg()首先从参数struct socket *sock获取tcp_sock，即TCP Control Blcok，然后将应用程序请求发送的数据复制到socket的发送缓冲区中。当复制数据到sk_buff中前，首先获取socket的Maximum Segment Size(MSS，最大消息长度)，MSS代表一个TCP包可携带的最大数据量（当然如果支持TSO或者GSO的话可以大于MSS），然后创建数据包，即sk_stream_alloc_skb()函数创建一个新的sk_buff，返回skb，skb_entail()函数将新建的skb添加到socket的发送缓冲区中（前文提到该缓冲区是一个链表）。skb_add_data函数将应用程序的数据复制到skb的buffer中。所有的数据将在重复调用这一过程中复制完成。此时，socket的发送缓冲区将以链表形式组织起MSS大小的若干个sk_buff。最后，调用tcp_push()函数将可发送的数据以数据包的形式发送出去，实现write()掉用的完整流程。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26  static inline void tcp_push(struct sock *sk, int flags, int mss_now, ...) [...] ===\u0026gt; static int tcp_write_xmit(struct sock *sk, unsigned int mss_now, ...) int nonagle, { struct tcp_sock *tp = tcp_sk(sk); struct sk_buff *skb; [...] while ((skb = tcp_send_head(sk))) { [...] cwnd_quota = tcp_cwnd_test(tp, skb); if (!cwnd_quota) break; if (unlikely(!tcp_snd_wnd_test(tp, skb, mss_now))) break; [...] if (unlikely(tcp_transmit_skb(sk, skb, 1, gfp))) break; /* Advance the send_head. This one is sent out. * This call will increment packets_out. */ tcp_event_new_data_sent(sk, skb); [...]   tcp_push()函数尽可能的将TCP允许发送的sk_buff按序号发送出去。首先调用tcp_send_head()函数获取发送缓冲区队列头的sk_buff，然后tcp_cwnd_test()和tcp_snd_wnd_test()函数用来检查拥塞窗口和接收窗口是否允许新的数据包发送，如果可以，调用tcp_transmit_skb()函数新建网络数据包，用于发送。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44  static int tcp_transmit_skb(struct sock *sk, struct sk_buff *skb,int clone_it, gfp_t gfp_mask) { const struct inet_connection_sock *icsk = inet_csk(sk); struct inet_sock *inet; struct tcp_sock *tp; [...] if (likely(clone_it)) { if (unlikely(skb_cloned(skb))) skb = pskb_copy(skb, gfp_mask); else skb = skb_clone(skb, gfp_mask); if (unlikely(!skb)) return -ENOBUFS; } [...] skb_push(skb, tcp_header_size); skb_reset_transport_header(skb); skb_set_owner_w(skb, sk); /* Build TCP header and checksum it. */ th = tcp_hdr(skb); th-\u0026gt;source = inet-\u0026gt;inet_sport; th-\u0026gt;dest = inet-\u0026gt;inet_dport; th-\u0026gt;seq = htonl(tcb-\u0026gt;seq); th-\u0026gt;ack_seq = htonl(tp-\u0026gt;rcv_nxt); [...] icsk-\u0026gt;icsk_af_ops-\u0026gt;send_check(sk, skb); [...] err = icsk-\u0026gt;icsk_af_ops-\u0026gt;queue_xmit(skb); if (likely(err \u0026lt;= 0)) return err; tcp_enter_cwr(sk, 1); return net_xmit_eval(err); }   tcp_transmit_skb()首先调用pskb_copy()创建待发送sk_buff的副本，仅复制sk_buff的元数据；然后调用skb_push()锁定tcp头部区域，然后填充头部字段，send_check()计算TCP头部的checksum。最后，queue_xmit()将数据包skb转移到下一层IP层，IPv4的queue_xmit指针指向函数ip_queue_xmit()。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46  int ip_queue_xmit(struct sk_buff *skb) { [...] rt = (struct rtable *)__sk_dst_check(sk, 0); [...] /* OK, we know where to send it, allocate and build IP header. */ skb_push(skb, sizeof(struct iphdr) + (opt ? opt-\u0026gt;optlen : 0)); skb_reset_network_header(skb); iph = ip_hdr(skb); *((__be16 *)iph) = htons((4 \u0026lt;\u0026lt; 12) | (5 \u0026lt;\u0026lt; 8) | (inet-\u0026gt;tos \u0026amp; 0xff)); if (ip_dont_fragment(sk, \u0026amp;rt-\u0026gt;dst) \u0026amp;\u0026amp; !skb-\u0026gt;local_df) iph-\u0026gt;frag_off = htons(IP_DF); else iph-\u0026gt;frag_off = 0; iph-\u0026gt;ttl = ip_select_ttl(inet, \u0026amp;rt-\u0026gt;dst); iph-\u0026gt;protocol = sk-\u0026gt;sk_protocol; iph-\u0026gt;saddr = rt-\u0026gt;rt_src; iph-\u0026gt;daddr = rt-\u0026gt;rt_dst; [...] res = ip_local_out(skb); [...] ===\u0026gt; int __ip_local_out(struct sk_buff *skb) [...] ip_send_check(iph); return nf_hook(NFPROTO_IPV4, NF_INET_LOCAL_OUT, skb, NULL,\tskb_dst(skb)-\u0026gt;dev, dst_output); [...] ===\u0026gt; int ip_output(struct sk_buff *skb) { struct net_device *dev = skb_dst(skb)-\u0026gt;dev; [...] skb-\u0026gt;dev = dev; skb-\u0026gt;protocol = htons(ETH_P_IP); return NF_HOOK_COND(NFPROTO_IPV4, NF_INET_POST_ROUTING, skb, NULL, dev, ip_finish_output, [...] ===\u0026gt; static int ip_finish_output(struct sk_buff *skb) { [...] if (skb-\u0026gt;len \u0026gt; ip_skb_dst_mtu(skb) \u0026amp;\u0026amp; !skb_is_gso(skb)) return ip_fragment(skb, ip_finish_output2); else return ip_finish_output2(skb);   ip_queue_xmit()方法负责执行IP层的任务，__sk_dst_check()检查缓存的路由结果是否合法。如果此时没有缓存的路由，或者缓存路由结果过期，就会进行IP路由查找。然后调用skb_push()锁定IP包头，填充IP包头部字段。接着调用ip_send_check()计算IP头的checksum，然后使用nf_hook调用netfilter模块，nf_hook()方法设置回调函数为dst_output，该函数被调用时，作为函数指针指向的是ip_output()函数。在ip_output()函数中，设置ip_finish_output()为回调函数，当发送数据包需要被分片发送时，进行分片，否则调用ip_finish_output2()，添加Ethernet Header，进入链路层Ethernet层。这样，一个数据包最终生成。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31  int dev_queue_xmit(struct sk_buff *skb) [...] ===\u0026gt; static inline int __dev_xmit_skb(struct sk_buff *skb, struct Qdisc *q, ...) [...] if (...) { .... } else if ((q-\u0026gt;flags \u0026amp; TCQ_F_CAN_BYPASS) \u0026amp;\u0026amp; !qdisc_qlen(q) \u0026amp;\u0026amp; qdisc_run_begin(q)) { [...] if (sch_direct_xmit(skb, q, dev, txq, root_lock)) { [...] ===\u0026gt; int sch_direct_xmit(struct sk_buff *skb, struct Qdisc *q, ...) [...] HARD_TX_LOCK(dev, txq, smp_processor_id()); if (!netif_tx_queue_frozen_or_stopped(txq)) ret = dev_hard_start_xmit(skb, dev, txq); HARD_TX_UNLOCK(dev, txq); [...] } int dev_hard_start_xmit(struct sk_buff *skb, struct net_device *dev, ...) [...] if (!list_empty(\u0026amp;ptype_all)) dev_queue_xmit_nit(skb, dev); [...] rc = ops-\u0026gt;ndo_start_xmit(skb, dev); [...] }   上层最终生成数据包后，函数dev_queue_xmit()将数据包发送出去。首先，数据包以qdisc方式传递过去；如果采用默认数据包入队规则（FIFO）并且队列为空，sch_direct_xmit()函数将直接把数据包发送给网卡驱动，越过缓冲队列；该函数调用dev_hard_start_xmit()函数选择对应的驱动并发送。在调用网卡驱动前，设备的TX队列将被加锁，防止在多任务同时访问网络设备。由于kernel已经向设备的TX队列加锁，所以设备驱动的发送代码不需要额外加锁。这和接下来要讨论的并行处理紧密相关。\nndo_start_xmit()函数负责调用NIC驱动代码。在调用之前你可以看到ptype_all和dev_queue_xmit_nit()语句，ptype_all是一个包括处理模块的链表，比如抓包模块，如果一个抓包程序在运行中，这个数据包将被ptype_all复制给这个程序。所以，类似于tcpdump这类软件显示的数据包，其实是发送给网卡驱动的；响应的tcpdump显示收到的数据包也是从这层拿到的。此时数据包未携带checksum，或者如果此时TSO使能的话，NIC将会操作编辑这个包。所以tcpdump抓到的包和实际发到网络的包还是有一定区别的。当完成发送数据包后，网卡驱动的终端处理程序返回发送的sk_buff。\n追踪代码：如何接收数据 姑且认为接收代码的流程与发送代码区别不大，所以我们先进行下一部分。\nNIC和NIC驱动是怎样通信的 NIC和NIC驱动之间的通信属于网络栈的最底层，往往被人忽视。然而NIC目前在网络性能方面承担着越来越多的任务，了解基本的操作模式将帮你学习到更多。\n驱动和NIC之间是异步通信的。首先，NIC驱动请求发送一个数据包后，CPU转向执行其他任务，并不阻塞等待响应；然后NIC发送数据包，通知CPU；最后NIC驱动将发送完成的数据包返回给上层。与发送一样，数据接收也一样是异步的，首先，NIC驱动请求接收一个包，然后CPU转而执行其他任务，然后NIC收到包后，通知CPU，NIC驱动处理收到的包处理并返回（由之前图4所示，NIC驱动注册时会请求kernel提前分配好缓存收到数据包的内存，NIC接受指令将数据包写入这部分内存）。\n所以，有一个空间，用于存放请求和响应是很必要的。大部分NIC使用环状结构(ring structure)，环状结构与普通队列结构类似，有固定的容量，一个单位存储一个请求或者响应。使用时也是按序处理，区别在于到达队列末尾后重头开始，形成一个环。\n如图8所示的包发送流程图，我们可以看到ring是怎样工作的。\nNIC驱动收到上层发来的数据包后，创建NIC可以理解的发送描述符(send descriptor)，发送描述符包括包大小、物理地址等信息；NIC要求通过物理地址访问NIC驱动的内存，所以NIC驱动需要将包的虚拟地址转换为物理地址。然后NIC驱动将send descriptor加入到发送环状缓冲(Send TX Ring Buffer)，如图8的流程(1)所示。\n然后，NIC驱动通知NIC有新的发送请求，见流程(2)，NIC驱动直接将这个请求写入到NIC的内存地址中，在这里，CPU采用Programmed I/O(PIO)的方法，直接将数据写到设备（其实这里如果开发过Linux 设备驱动，比如以前开发过的 PCIE驱动就知道，将PCIE设备的配置寄存器映射到Host的内存空间中，kernel可以像访问自身内存一样读写这些地址，进而将控制指令写入设备中）。被通知的NIC从host的内存（发送环状内存缓冲区）以DMA方式获取发送描述符，见流程(3)。拿到发送描述符后，获得数据包在host内存的的物理地址和大小，然后将数据包以DMA方式读出。\nNIC取得发送数据包后，计算包的checksum然后加到数据包里，然后发送，见流程(5)。发送完成后，NIC将发送的数据包数量写回host内存(流程6)；然后向CPU发起中断(流程7)。NIC驱动独处发送了哪些数据包后，将数据包返回。(The NIC sends packets (5) and then writes the number of packets that are sent to the host memory (6). Then, it sends an interrupt (7). The driver reads the number of packets that are sent and then returns the packets that have been sent so far.)\n如图9所示，我们可以看到读取数据包流程图.\n首先，NIC驱动在host分配用于存储接收数据包和接收描述符的内存。接收描述符包括缓冲区大小和物理地址，与发送描述符一样都是物理地址，用于DMA传输。然后，NIC驱动将接收描述符添加到RX ring中(流程1)。通过PIO，NIC驱动将新的接收描述符地址写入NIC中(流程2)，NIC从Rx ring中以DMA方式获取接收描述符，获得用于接收数据包的缓冲区的大小和物理地址并存储(流程3)。\nNIC收到数据包后(流程4)，NIC将数据包写入实现分配好的host内存中(流程5),如果网卡有计算数据包checksum的功能，那么NIC此时计算数据包的checksum。接收数据包的大小、checksum和其他信息存储在另一个环状buffer(the receive return ring，接收返回环 )中(流程6)。接收返回环也存储NIC处理接收到数据包的结果，比如返回包。然后NIC发出中断(流程7)，NIC驱动从接收返回包中获取包的信息，然后处理数据包。如果必要的话，NIC驱动还会继续分配内存并重复流程(1)和(2).\n在调优网络栈的时候，大家都认为环状缓存大小和中断设置要互相匹配。当发送环状缓存Tx ring比较大时，可以一次发出较多请求；当Rx ring比较大时，可以一次收到较多数据包。大Ring buffer可以并发大量发送操作，提高工作能力；实际实现中，NIC使用一个定时器定期收集处理中断，减少CPU中断的次数，以免CPU为处理中断而分心。\n缓存和流控制 流控制是网络栈各层通力合作实现的。图10显示发送数据时网络栈的各级缓存。首先，应用程序创建数据，添加到socket发送缓存中，如果缓存没有内存可用，则send/write系统调用返回失败或者堵塞。因此，应用程序流向kernel的数据流速由socket缓冲区大小来限制。\nTCP协议栈创建和发送数据包，通过发送队列transmit queue(qdisc)向NIC驱动发送。这是个典型的FIFO队列，队列长度可以由ifconfig工具配置，执行ifconfig工具结果中的txqueuelen的值，一般为1000，意味着缓存1000个数据包。\n环状发送队列(TX ring)处于NIC驱动和NIC之间，正如上一章提到的，Tx ring可被认为是发送请求队列。如果Tx ring满，此时NIC驱动不能发出发送请求，那么待发送的数据包将会累积在TCP/IP协议栈和NIC驱动之间的qdisc中，如果累积数据包超过qdisc大小，那么再想发送新包，会被直接丢弃。\nNIC将待发送数据包存储在自身缓存中，包速率主要由NIC的物理速度决定。而且由于链路层Ethernet layer的流控制，如果NIC的接收缓冲区没有空间，那么发送数据包也将停止（可以猜测原因是自身停止发送后，对端将不会再发送数据包过来，有助于NIC和NIC驱动将拥塞在接受缓冲区的数据包处理完）。\n当发送自kernel的数据包速度大于发送自NIC的数据包速度时，包将拥堵在NIC的缓存中。如果NIC自身缓存没有多余空间，NIC将不会从Tx ring中去取发送请求request；这样的话，越来越多的发送请求累积在Tx ring中，最终Tx ring也堵满；此时NIC驱动再也不能发起新的发送请求，并且要发送的新包将堵塞在qdisc中；就这样，性能衰退从底向上传递。（感觉这里我们可以通过检测各级buffer的堵塞情况，判断程序堵塞在哪一步）\n图11显示接受数据包的传递过程。首先，收到的数据包将缓存在NIC自身缓存中。从流控制的角度来看，NIC和NIC驱动之间的Rx ring队列作为缓存，NIC驱动从Rx ring中将已接受数据包的请求取出，发给上层，在这里NIC驱动和协议栈之前没有缓冲区，因为这里是通过kernel调用NAPI去poll已收到的数据包的。(需要想想怎么翻译).这里可以认为上层直接从Rx ring中获取数据包。网络包的数据部分将上传缓存在socket的接收缓冲区中，应用程序随后从socket的接受缓冲区中读取数据。\nTo Be Continued ldd\n","date":"2016-10-20T22:21:55+08:00","permalink":"https://bg2bkk.github.io/p/understanding-tcpip-network-stack-writing-network-apps/","title":"Understanding TCPIP Network Stack \u0026 Writing Network Apps"},{"content":" futex初体验 阿里基础架构事业群的博客关于futex的文章 linux线程同步机制  Linux中的线程同步机制(二)–In Glibc  大部分的glibc的同步方式，mutex或者semaphore，大多基于futex的方式，首先进行用户态检查，未果的话进行futex系统调用。这是我疑惑为什么futex这么常用却在代码层面上看不到它，原因是我们使用的都是基于futex的机制   Linux中的线程同步机制(三)–Practice  pthread库中的pthread_join也是基于futex的哦，当父进程执行pthread_join它的某一个子线程时，如果子线程已经执行完毕，则父进程不会调用futex系统调用，如果子线程仍然执行中，那么父进程调用futex系统调用进行FUTEX_WAIT休眠，等待子线程的唤醒   好文章，值得深挖和思考   man page  先通过__sync_bool_compare_and_swap等原子操作比对futex的值是否有变化，如果没有，说明没有进程竞争。这里都是用户态执行的 如果有变化，说明有进程竞争了，所以这时系统调用futex进行FUTEX_WAIT，使得本进程休眠或休眠一段时间，直到有别的进程FUTEX_WAKE它 说白了，futex针对有些同步场景中，尽管没有竞争发生，但是还要陷入内核态去获得锁或者标志位然后同步的情况，futex可以仅通过原子性的内核态即可实现线程安全 提供futex_demo.c  如果将futex1和futex2互换下，画面太美不敢看，CPU暴涨100%      1 2 3 4 5 6 7  futex(0x7f99372d3004, FUTEX_WAIT, 0, NULL) = -1 EAGAIN (Resource temporarily unavailable) futex(0x7f99372d3004, FUTEX_WAIT, 0, NULL) = -1 EAGAIN (Resource temporarily unavailable) futex(0x7f99372d3004, FUTEX_WAIT, 0, NULL) = -1 EAGAIN (Resource temporarily unavailable) futex(0x7f99372d3004, FUTEX_WAIT, 0, NULL) = -1 EAGAIN (Resource temporarily unavailable) futex(0x7f99372d3004, FUTEX_WAIT, 0, NULL) = -1 EAGAIN (Resource temporarily unavailable) futex(0x7f99372d3004, FUTEX_WAIT, 0, NULL) = -1 EAGAIN (Resource temporarily unavailable) futex(0x7f99372d3004, FUTEX_WAIT, 0, NULL) = -1 EAGAIN (Resource temporarily unavailable)   ","date":"2016-10-13T13:57:31+08:00","permalink":"https://bg2bkk.github.io/p/futex%E5%92%8Clinux%E7%9A%84%E7%BA%BF%E7%A8%8B%E5%90%8C%E6%AD%A5%E6%9C%BA%E5%88%B6/","title":"futex和linux的线程同步机制"},{"content":"我们都知道实验新药的时候首先要在实验大鼠身上实行，有些时候我们发现，明明在大鼠身上具有良好功效的药品，在人身上的临床效果并不明显。我们此时可能会从鼠和人类的基因的区别、体质等方面找原因，这是受过基本科研训练的人会有的正常想法：控制变量法。其实我们可能忽略了一个点，那就是，实验大鼠从小生活在无菌环境下，而人并不是。实验大鼠在无菌环境下出生和长大，并不会遇到某些细菌，自然不会建立起免疫机制，可与之类比的大概是人类婴儿吧。科学家们通过对比实验大鼠和农场中生活的野鼠，确认野鼠体质与成年人类更为接近，因此在实验新药的时候会更倾向于使用农场大鼠。这个案例告诉我们，遇到问题时，我们一般会从受到过的训练中获取方法，快速反应去比对变量，但同时眼睛将固定于自己更想看到的部分，而没有跳出来看问题。在分析系统性能问题时，对某个瓶颈点百思不得其解时，这个时候是否可以把自己跳出来，多想想其他变量，甚至是其他更显得“弱智”的因素。\n有时候阅读一些技术文章时，会有这样一种感觉：这和理论不太一样啊。读林佩满的wireshark的书时，他分析过这样一个问题：某厂某个文件传输的产品在竞标时，发现大文件通过网络传输，从上海发往北京时很慢，需要很长时间才能将文件发送完成，而同时测试网速发现完全不是瓶颈，带宽利用率低到个位数；林在通过wireshark抓包时，发现有非常多的数据包被重传了多次，并且是毫无意义的重传；配管人员配置数据传送采用UDP方式，理论上讲不需要三次握手，且不需要发送确认的UDP传输更适合发送大文件，比如FTP协议就是基于UDP实现的，课本上都这么说。然而实际情况是，发送数据从上海到北京，会经过多多少少的中间结点，客户端发送较大的数据包，中途转发时，一些设备可能将大包拆成小包发送，对于UDP协议而言，如果最终小包里有一个包传输失败，那么需要将整个大包重传，而对于TCP协议而言，哪个小包没有收到，由于TCP头部的序号，客户端将只需要重传丢失的这个小包即可，避免了大量的无意义的重传；将传输方式改为TCP传输后，带宽利用率果然一下就上去了，这里我们的结论是远途传输大文件或大数据包时，TCP的性能由于UDP，这和课本上讲的很不一样。很多知识都不是一成不变的，会根据落地情况而调整、而改变，更有可能进化成不同形态。我想起前几天在火车上看到的一篇文章，城市里的灯光将会吸引虫子，趋光的虫子绕着灯泡一圈圈转，最后力竭饿死，这是个老知识了；而科学家经过实验，从城市和郊区抓取同样数量的飞虫，放在一个房间里观察，发现郊区的飞虫全部围绕着灯泡，而城市中的飞虫则有一小部分对灯光不感兴趣。科学家做出的结论是，城市中的飞虫有一部分进化出了不那么趋光的特性，基因得以保留。所以，我们看到，知识不是一成不变的，尽量做到不先入为主，从实际情况出发来分析问题。\n说到实际情况出发，与一个朋友聊epoll在实际使用中会遇到的一些问题，对于多进程（线程）程序而言，具有父子关系的两个进程共享一个epoll fd，A进程通过epoll_ctl将fd添加进epfd的监听fd列表中，如果有fd有事件发生，发生事件的fd有的是A添加的，有的是B添加的，如果此时唤醒的是B进程，B进程将该epfd监听的所有有事件发生的fd取出，进行处理时，遇到有些不是自己添加的fd时，这个fd对B进程而言是无效的，如果不做处理，去read或者write它，可能会导致core dump，当然，我还没有自己做实验验证，但是理论上讲，这是会发生的。\n在技术书之外，其实还有非常多的有趣的书，使人明事里、断情势，提升人的气质，这是一个大的范畴，我愿接下来一点点的分享我的读书感受。\n","date":"2016-09-18T19:43:59+08:00","permalink":"https://bg2bkk.github.io/p/%E4%B8%BA%E4%BB%80%E4%B9%88%E6%88%91%E5%96%9C%E6%AC%A2%E8%AF%BB%E4%B9%A6/","title":"为什么我喜欢读书"},{"content":"redis过期清除和淘汰机制   过期时间设置\n expire key seconds 该命令设置指定key超时的秒数，超过该时间后，可以将被删除 在超时之前，如果该key被修改，与之关联的超时将被移除  persist key 持久化该key，超时时间移除 set key newvalue 设置新值，会清除过期时间 del key\t显然会清除过期时间 例外情况：  lpush, zset, incr等操作，在高版本（2.1.3++）之后不会清除过期时间，毕竟修改的不是key本身 rename 也不会清除过期时间，只是改key名字        过期处理\n redis对过期key采用lazy expiration方式，在访问key的时候才判定该key是否过期 此外，每秒还会抽取volatile keys进行抽样，处理删除过期键    过期键删除策略种类\n  事件删除\n 每个键都有一个定时器，到期时触发处理事件，在事件中删除 缺点是需要为每个key维护定时器，key的量大时，cpu消耗较大    惰性删除\n 每次访问时才检查，如果没过期，正常返回，否则删除该键并返回空    定期删除\n 每隔一段时间，检查所有设置了过期时间的key，删除已过期的键    redis采用后两种结合的方式\n 读写一个key时，触发惰性删除策略 惰性删除策略不能及时处理冷数据，因此redis会定期主动淘汰一批已过期的key 内存超过maxmemory时，触发主动清理      http://blueswind8306.iteye.com/blog/2240088\n  http://www.cnblogs.com/chenpingzhao/p/5022467.html\n  ","date":"2016-08-01T13:26:07+08:00","permalink":"https://bg2bkk.github.io/p/redis%E8%BF%87%E6%9C%9F%E6%B8%85%E9%99%A4%E6%9C%BA%E5%88%B6%E5%8F%8A%E5%BA%94%E7%94%A8%E6%96%B9%E6%B3%95/","title":"redis过期清除机制及应用方法"},{"content":"免密登录配置脚本 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20  #!/bin/bash passwd=\u0026#39;xxxxxxx\u0026#39; pub_key=`cat ~/.ssh/id_rsa.pub` for i in `cat $1`;do /usr/bin/expect \u0026lt;\u0026lt;-EOF set time 30 spawn ssh zhendong.hzd@$i \u0026#34;mkdir -p .ssh/;touch .ssh/authorized_keys;chmod 700 .ssh/authorized_keys;echo \\\u0026#34;$pub_key\\\u0026#34; \u0026gt;\u0026gt; .ssh/authorized_keys\u0026#34; expect { \u0026#34;*yes/no\u0026#34; { send \u0026#34;yes\\r\u0026#34;; exp_continue } \u0026#34;*password:\u0026#34; { send \u0026#34;$passwd\\r\u0026#34; } } interact expect eof EOF done   将待登录IP列表写入文件中，更改相应密码配置和用户名，然后执行\nubuntu命令行连接wifi wifi设置\n1 2 3 4 5 6 7 8 9 10  cat /etc/network/interfaces # interfaces(5) file used by ifup(8) and ifdown(8) auto lo iface lo inet loopback # 添加如下，启动时自动启动wlp2s0网卡，dhcp模式，采用wpa_conf配置 auto wlp2s0 iface wlp2s0 inet dhcp wpa_conf /etc/wpa_supplicant/wpa_supplicant.conf   wpa_supplicant.conf内容\n1 2 3 4 5  network={ ssid=\u0026#34;wills\u0026#34; psk=\u0026#34;1234567890\u0026#34; priority=1 }   连接wifi和密码，priority为优先级，数字越大级别越高\nvim和shell的配置方式 作为一个懒人，现在自然期待有一键脚本能够将我的shell和vim配置好。github上有很多repo帮助我们实现。\n知乎帮助一 知乎帮助二 vim一键配置 shell \u0026amp;\u0026amp; zsh vim一键配置二\nlinux下进程的有效用户ID和实际用户ID 见博客，getuid()\nhexdump查看文件指定位置 方法为所示，\n1  hexdump -s OFFSET -l LENGTH FILENAME   mac os下virtual box与虚拟机进行网络互通 方法见链接，需要在virtual box添加host only网卡，并在虚拟机中添加网卡。\n普通用户获取linux的root权限  方法一：sudo sh -c \u0026ldquo;su\u0026rdquo; 方法二：sudo strace su  改变默认登录的shell 使用不同linux发行版或者上不同机器时，发现shell不是我喜欢的bash，而是sh等，这个时候需要修改\n 修改登录shell  方法一：chsh -s /bin/bash 方法二：修改/etc/passwd中的shell设置    解决shell不显示路径的方法\nsar在ubuntu中的配置 1 2 3 4 5 6 7 8 9 10 11 12 13  sudo apt-get install sysstat sudo vim /etc/default/sysstat \u0026#34; change ENABLED=\u0026#34;false\u0026#34; to \u0026#34;true\u0026#34; sudo vim /etc/cron.d/sysstat \u0026#34; change \u0026#34; 5-55/10 * * * * root command -v debian-sa1 \u0026gt; /dev/null \u0026amp;\u0026amp; debian-sa1 1 1 \u0026#34; to \u0026#34; */2 * * * * root command -v debian-sa1 \u0026gt; /dev/null \u0026amp;\u0026amp; debian-sa1 1 1 \u0026#34; change the collection interval from every 10 minutes to every 2 minutes. sudo /etc/init.d/sysstat restart   全世界最愚蠢的事情就是，重复做相同的事情，却期待有不同的结果发生 Insanity: doing the same thing over and over again and expecting different results.  说实话，在写代码、调代码的时候，一但出现非预期结果，首先检查之前自己的编码和输入，确定后不应该再二次重试，毕竟相同条件下不可能产生不同结果，此时应该将思维跳脱出来，另辟蹊径为好。\nubuntu下解压zip文件出现乱码的解决办法 参考链接\n由于zip格式中并没有指定编码格式，Windows下生成的zip文件中的编码是GBK/GB2312等，因此，导致这些zip文件在Linux下解压时出现乱码问题，因为Linux下的默认编码是UTF8。\n目前网上流传一种unzip -O cp936的方法，但一些unzip是没有-O这个选项的。\n亲测好用，不好用的看链接\nredis设置和清除密码 正规项目终于要对redis设置密码了，如何加密码呢\n  配置文件中添加密码\n 配置文件中的requirepass配置指令用于配置密码 配置文件中的masterauth用于配置从机登陆主机的密码    运行中添加密码\n config set requirepass PASSWORD    运行中删除密码\n config set requirepass \u0026quot;\u0026quot;    获取密码\n config get requirepass    配置从机\n config set masterauth MASTER_PASSWORD config set requirepass SLAVE_AUTH    epoll是同步非阻塞的 epoll、select等多路服用IO，将fd加入等待时间的队列中，每隔一段时间去轮询一次，因此是同步的；优点是能够在等待任务的时间里去做别的任务；缺点是任务完成的响应延迟增大，因为每隔一段时间去轮询他们，在时间间隔内任务可能已经完成而等待处理等待了一段时间了。\n参考链接\n同步/异步指的是被调用方的通知方式，被调用方完成后，主动通知调用方，还是等待调用方发现。前者是异步，后者是同步。从这里也可以看出，异步IO通知调用方时，数据已经就绪，对于网络IO来说，异步IO已经将数据从内核复制到用户空间了。\n阻塞/非阻塞是调用方的等待方式，是一直等待在做的事件完成，还是去做别的事情，等到在做的事件完成后再接着进行处理。前者是阻塞，后者是非阻塞\n因此epoll是同步和非阻塞的。\nsed合并相邻两行 从redis中取出一个键的所有内容时，比如hgetall，得到的结果并不是排序好的，类似于这样\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23  11485) \u0026#34;1470347460\u0026#34; 11486) \u0026#34;69\u0026#34; 11487) \u0026#34;1470262350\u0026#34; 11488) \u0026#34;34\u0026#34; 11489) \u0026#34;1470262170\u0026#34; 11490) \u0026#34;68\u0026#34; 11491) \u0026#34;1470242400\u0026#34; 11492) \u0026#34;21\u0026#34; 11493) \u0026#34;1470288030\u0026#34; 11494) \u0026#34;65\u0026#34; 11495) \u0026#34;1470303390\u0026#34; 11496) \u0026#34;54\u0026#34; 11497) \u0026#34;1470205320\u0026#34; 11498) \u0026#34;85\u0026#34; 11499) \u0026#34;1470318330\u0026#34; 11500) \u0026#34;92\u0026#34; 11501) \u0026#34;1470167040\u0026#34; 11502) \u0026#34;1\u0026#34; 11503) \u0026#34;1470281880\u0026#34; 11504) \u0026#34;14\u0026#34; 11505) \u0026#34;1470298140\u0026#34; 11506) \u0026#34;113\u0026#34;   该hash的key为unix时间戳，val为数值，如果想手动看分布的话，需要将相邻两行合并然后排序，在此我们借助sed\n1  sed \u0026#39;$!N;s/\\n/\\t/\u0026#39; filename   redis分析实例中所有key和单个key的内存占用情况 采用rdb工具\n1  rdb -c memory /path/to/ab-dump.rdb \u0026gt; memory.csv   sort进行多重排序 sort和uniq在文本处理，尤其是日志处理中用的较多的工具，记得当年校招时候准备面试，用到这两个命令，惊为天人，非常shock。在日常工作中，用的也非常多。\n目前有这样的需求，拿到两列数据，第一列是ip，第二列是访问计数，想看一下分布，要求ip要按文本排序，访问计数按数值排序。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24  221.179.175.109\t97 221.179.175.164\t101 221.179.175.46\t8 221.179.175.164\t102 221.179.175.178\t78 221.179.175.46\t9 221.179.175.109\t98 221.179.175.178\t79 120.239.141.197\t96 221.179.175.70\t114 218.202.7.121\t70 221.179.175.178\t80 218.202.7.121\t71 221.179.175.134\t83 120.239.141.197\t97 221.179.175.46\t10 221.179.175.178\t81 221.179.175.47\t24 120.239.141.197\t98 221.179.175.70\t115 221.179.175.70\t116 120.239.141.197\t99 221.179.175.134\t84   解决方法\n1 2 3 4 5 6  sort -t \u0026#39; \u0026#39; -k1,1 -k2n,2 data 其中 -t \u0026#39; \u0026#39; 指定使用空格分列 -k1,1 指定以第一列为关键字排序 -k2n,2 指定以第二列为关键字做数据排序   也可以\n1  sort -k1 -k2n out   shell脚本批量处理文本 善用awk、grep、xargs、bash、sed等工具，可以提高生产力\n1  grep \u0026#39;config.config\u0026#39; . -r | awk -F\u0026#39;:\u0026#39; \u0026#39;{print $1}\u0026#39; | grep \u0026#39;lua$\u0026#39; | xargs sed \u0026#34;s/config.config\u0026#39;)/config.config\u0026#39;).ab/g\u0026#34; -i   shell循环\n1  for j in {a..z}; do echo $j; done   redis批量删除key 手动清理redis中的key时，很想通过 del keys* 的方式实现批量删除，而redis却没有提供这样的选项，因此需要借助外部工具\n  1、sehll 命令行\n redis-cli keys ip:* | xargs redis-cli del    2、lua脚本\n redis-cli eval \u0026ldquo;redis.call(\u0026lsquo;del\u0026rsquo;, unpack(redis.call(\u0026lsquo;keys\u0026rsquo;,\u0026lsquo;ip*')))\u0026rdquo; 0 这种方式受限于lua的unpack函数，一次删除的key不能太多    3、借助客户端\n php jedis    unix获取时间戳 1 2 3 4 5 6 7 8 9 10 11 12 13 14  date \u0026#39;+%s\u0026#39; export timestamp=`date \u0026#39;+%s\u0026#39;`; echo $timestamp # 标准时间格式转unix时间戳 date -d \u0026#34;2011-03-02 15:00\u0026#34; +%s # unix时间戳转为标准格式 date -d \u0026#39;1970-01-01 UTC 1299049200 seconds\u0026#39; # 或者 date -d \u0026#34;@1279592730\u0026#34;   链接中提到了各种各样的格式，以后写shell脚本就不担心时间戳问题了\ndocker 设置代理下载镜像 在systemd中设置\n mkdir /etc/systemd/system/docker.service.d touch /etc/systemd/system/docker.service.d/http-proxy.conf 在文件中添加：[Service]Environment=\u0026ldquo;HTTP_PROXY=http://proxy.example.com:80/\u0026quot; 重启daemon：sudo systemctl daemon-reload 查看设置状态：sudo systemctl show docker \u0026ndash;property Environment 重启docker：sudo systemctl restart docker  tested on Ubuntu 16.04\n用ps查看进程的执行时间 1  ps -eo pid,tty,user,comm,stime,etime | grep main   可以打印进程的开始时间和执行时间\nvim下以16进制查看文本文件 vim的功能实在是太强大了，可以以16进制查看文本信息：\n十六进制显示 :%!xxd 正常显示 :%!xxd -r  Linux获取系统调度时间片长度 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19  #define _GNU_SOURCE #include \u0026lt;sched.h\u0026gt;#include \u0026lt;stdlib.h\u0026gt;#include \u0026lt;unistd.h\u0026gt;#include \u0026lt;stdio.h\u0026gt;#include \u0026lt;assert.h\u0026gt; int main(int argc, char *argv[]) { int ret, i; struct timespec tp; ret = sched_rr_get_interval(0, \u0026amp;tp); if(ret == -1) printf(\u0026#34;sched_rr_get_interval error.\\n\u0026#34;); printf(\u0026#34;The time is %ds:%ldns.\\n\u0026#34;, (int)tp.tv_sec, tp.tv_nsec); return 0; }   1 2  $ ./cpu_time_slice.o The time is 0s:16000000ns.   可见Ubuntu-16.04 64bit的系统进程时间片是16ms\nC语言中short、int、long内存占用 随着工作年限的增加，很多基本功反而落了下来，甚至开始怀疑short等类型的内存占用问题了呵呵。印象里一直记得int和long类型都是4字节大小啊\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19  #include \u0026lt;stdio.h\u0026gt;#include \u0026lt;stdlib.h\u0026gt; struct test{ char *ch1; int i; long ch2; } aa; int main() { printf(\u0026#34;Linux-64Bit-platform\\n\u0026#34;); printf(\u0026#34;\t%lu\\n\u0026#34;, (unsigned long)sizeof(aa)); printf(\u0026#34;\tsizeof short : %d\\n\u0026#34;, sizeof(short)); printf(\u0026#34;\tsizeof int : %d\\n\u0026#34;, sizeof(int)); printf(\u0026#34;\tsizeof long : %d\\n\u0026#34;, sizeof(long)); }   1 2 3 4 5 6 7  Linux-64Bit-platform 24 sizeof short : 2 sizeof int : 4 sizeof long : 8   1 2 3 4 5 6 7  Linux-32Bit-platform 24 sizeof short : 2 sizeof int : 4 sizeof long : 4   看来，long和int大小一样已经是32位机器的老黄历了，基础知识还是应该常用常新啊\n文件操作的线程安全相关（待续） http://stackoverflow.com/questions/29981050/concurrent-writing-to-a-file\nubuntu关闭键盘和触摸板的方法 家里的猫就是喜欢趴在笔记本键盘上看你干活，我只能再买一个键盘，然后笔记本键盘留给猫大爷了。\n然而它还喜欢在键盘上跳舞，这样太影响输入了，只能想办法把笔记本键盘关掉。\n在ubuntu下，键盘鼠标触控板都属于xinput设备，可以通过以下命令查看：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20  $ xinput --list ⎡ Virtual core pointer id=2\t[master pointer (3)] ⎜ ↳ Virtual core XTEST pointer id=4\t[slave pointer (2)] ⎜ ↳ SynPS/2 Synaptics TouchPad id=16\t[slave pointer (2)] ⎜ ↳ Rapoo Rapoo Gaming Keyboard id=11\t[slave pointer (2)] ⎜ ↳ RAPOO Rapoo 2.4G Wireless Device id=12\t[slave pointer (2)] ⎜ ↳ Wacom ISDv4 E6 Pen stylus id=13\t[slave pointer (2)] ⎜ ↳ Wacom ISDv4 E6 Finger touch id=14\t[slave pointer (2)] ⎜ ↳ Wacom ISDv4 E6 Pen eraser id=18\t[slave pointer (2)] ⎜ ↳ TPPS/2 IBM TrackPoint id=19\t[slave pointer (2)] ⎣ Virtual core keyboard id=3\t[master keyboard (2)] ↳ Virtual core XTEST keyboard id=5\t[slave keyboard (3)] ↳ Power Button id=6\t[slave keyboard (3)] ↳ Video Bus id=7\t[slave keyboard (3)] ↳ Sleep Button id=8\t[slave keyboard (3)] ↳ Integrated Camera id=9\t[slave keyboard (3)] ↳ Rapoo Rapoo Gaming Keyboard id=10\t[slave keyboard (3)] ↳ AT Translated Set 2 keyboard id=15\t[slave keyboard (3)] ↳ ThinkPad Extra Buttons id=17\t[slave keyboard (3)]   可以看到笔记本键盘是\n1  ↳ AT Translated Set 2 keyboard id=15\t[slave keyboard (3)]   而触控板是\n1  ⎜ ↳ SynPS/2 Synaptics TouchPad id=16\t[slave pointer (2)]   他们的id分别是 15和 16，所以采用以下命令关掉就可以\n1 2  sudo sudo xinput set-prop 15 \u0026#34;Device Enabled\u0026#34; 0 sudo sudo xinput set-prop 16 \u0026#34;Device Enabled\u0026#34; 0   附送shell脚本\n1 2 3 4 5 6 7 8 9 10 11 12 13  #!/bin/bash  keyboard=`xinput --list | grep AT | awk -F\u0026#39;=\u0026#39; \u0026#39;{print $2}\u0026#39; | awk \u0026#39;{print $1}\u0026#39;` touchpad=`xinput --list | grep Synaptics | awk -F\u0026#39;=\u0026#39; \u0026#39;{print $2}\u0026#39; | awk \u0026#39;{print $1}\u0026#39;` function doit() { echo \u0026#39;关闭 笔记本键盘\u0026#39; `sudo xinput set-prop $keyboard \u0026#34;Device Enabled\u0026#34; 0`\techo \u0026#39;关闭 笔记本触摸板\u0026#39; `sudo xinput set-prop $touchpad \u0026#34;Device Enabled\u0026#34; 0` } doit   小于1024的保留端口都有哪些 我们会遇到如下情况：\n1 2 3 4  $ sudo tcpdump -i any port 1080 tcpdump: verbose output suppressed, use -v or -vv for full protocol decode listening on any, link-type LINUX_SLL (Linux cooked), capture size 262144 bytes 15:08:42.421693 IP localhost.55092 \u0026gt; localhost.socks: Flags [.], ack 1960200857, win 342, options [nop,nop,TS val 4687328 ecr 4676064], length 0   我想监听1080端口，tcpdump为什么不乖乖显示1080，而是出现个socks呢？（可以通过***-n***参数解决）为什么1080是socks，而不是别的呢？\n这是因为低于1024的保留端口大多有自己的名字，他们由IANA分配，通常用于系统进程，而我们可以在***/etc/services***文件中找到：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39  # # From ``Assigned Numbers\u0026#39;\u0026#39;: # #\u0026gt; The Registered Ports are not controlled by the IANA and on most systems #\u0026gt; can be used by ordinary user processes or programs executed by ordinary #\u0026gt; users. # #\u0026gt; Ports are used in the TCP [45,106] to name the ends of logical #\u0026gt; connections which carry long term conversations. For the purpose of #\u0026gt; providing services to unknown callers, a service contact port is #\u0026gt; defined. This list specifies the port used by the server process as its #\u0026gt; contact port. While the IANA can not control uses of these ports it #\u0026gt; does register or list uses of these ports as a convienence to the #\u0026gt; community. # socks\t1080/tcp\t# socks proxy server socks\t1080/udp proofd\t1093/tcp proofd\t1093/udp rootd\t1094/tcp rootd\t1094/udp openvpn\t1194/tcp openvpn\t1194/udp rmiregistry\t1099/tcp\t# Java RMI Registry rmiregistry\t1099/udp kazaa\t1214/tcp kazaa\t1214/udp nessus\t1241/tcp\t# Nessus vulnerability nessus\t1241/udp\t# assessment scanner lotusnote\t1352/tcp\tlotusnotes\t# Lotus Note lotusnote\t1352/udp\tlotusnotes ms-sql-s\t1433/tcp\t# Microsoft SQL Server ms-sql-s\t1433/udp ms-sql-m\t1434/tcp\t# Microsoft SQL Monitor ms-sql-m\t1434/udp ingreslock\t1524/tcp ingreslock\t1524/udp prospero-np\t1525/tcp\t# Prospero non-privileged   git修改默认分支名 在develop分支改动太大了，导致merge 到master分支时非常被动，这个时候我想，干脆将develop分支作为分支好了。还好碰到stackoverflow的一个帖子\n git branch -m master oldmaster git branch -m develop master git push -f origin master  另一个方法是从github的项目主页上更改\n编译openssl 1.0.2g 1  ./config shared -fPIC zlib-dynamic \u0026amp;\u0026amp; make depend -j \u0026amp;\u0026amp; make -j   编译nginx/tengine: CPP模块 1  ./configure --add-module=../cpp_module --with-ld-opt=\u0026#34;-lstdc++\u0026#34;   curl -i 和 -I的区别 man page:\n1 2 3 4 5  -i, --include (HTTP) Include the HTTP-header in the output. The HTTP-header includes things like server-name, date of the document, HTTP-version and more... -I, --head (HTTP/FTP/FILE) Fetch the HTTP-header only! HTTP-servers feature the command HEAD which this uses to get nothing but the header of a document. When used on an FTP or FILE file, curl displays the file size and last modification time only.   -i选项会打印出HTTP头部的一些信息，这个选项是curl软件的选项，这些信息本来就是存在的\n-I选项会发送HEAD请求，获取信息\nlinux系统如何将父子进程一起kill掉 对于普通进程而言，kill掉父进程将会连带着把子进程kill掉；而对于daemon等类型进程而言，kill掉父进程，子进程会被daemon接管，所以如果想父子一起kill掉的话，不能直接kill父进程。\n有两种方法\n  kill \u0026ndash; -PPID\n PPID前面有***-***号，可以将父子进程kill掉    使用exec或者xargs来kill掉他们\n  dns查询中，域名是否可以有多个cname呢？ 不可以 * http://serverfault.com/questions/574072/can-we-have-multiple-cnames-for-a-single-name\ngit代理访问 git config \u0026ndash;global http.proxy 10.8.0.1:8118\nubuntu操作、挂载、格式化SD卡 玩树莓派等板子的时候，需要从host机器将os镜像烧进sd卡，然后启动。那么ubuntu如何操作呢？\nfdisk -l命令可以用来查看系统中的存储硬件\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38  Disk /dev/sda: 111.8 GiB, 120034123776 bytes, 234441648 sectors Units: sectors of 1 * 512 = 512 bytes Sector size (logical/physical): 512 bytes / 512 bytes I/O size (minimum/optimal): 512 bytes / 512 bytes Disklabel type: gpt Disk identifier: C27256BB-CE04-48C2-96F4-8F79FAE2AE87 Device Start End Sectors Size Type /dev/sda1 2048 234440703 234438656 111.8G Linux filesystem Disk /dev/sdb: 167.7 GiB, 180045766656 bytes, 351651888 sectors Units: sectors of 1 * 512 = 512 bytes Sector size (logical/physical): 512 bytes / 512 bytes I/O size (minimum/optimal): 512 bytes / 512 bytes Disklabel type: dos Disk identifier: 0x42b438a2 Device Boot Start End Sectors Size Id Type /dev/sdb1 * 2048 105887743 105885696 50.5G 7 HPFS/NTFS/exFAT /dev/sdb2 105887744 187807665 81919922 39.1G 83 Linux /dev/sdb3 187807744 228767743 40960000 19.5G 7 HPFS/NTFS/exFAT /dev/sdb4 228769790 351649791 122880002 58.6G f W95 Ext\u0026#39;d (LBA) /dev/sdb5 228769792 351649791 122880000 58.6G 7 HPFS/NTFS/exFAT Disk /dev/sdc: 14.9 GiB, 16021192704 bytes, 31291392 sectors Units: sectors of 1 * 512 = 512 bytes Sector size (logical/physical): 512 bytes / 512 bytes I/O size (minimum/optimal): 512 bytes / 512 bytes Disklabel type: dos Disk identifier: 0x00000000 Device Boot Start End Sectors Size Id Type /dev/sdc1 8192 31291391 31283200 14.9G c W95 FAT32 (LBA)   如果sd卡（tf卡）通过usb 读卡器接入电脑，则会显示为 /dev/sdc\n如果是标准sd卡（大卡），则会显示为 /dev/mmblck0\n1 2 3 4 5 6 7 8 9 10 11  Disk /dev/mmcblk0: 14.9 GiB, 16021192704 bytes, 31291392 sectors Units: sectors of 1 * 512 = 512 bytes Sector size (logical/physical): 512 bytes / 512 bytes I/O size (minimum/optimal): 512 bytes / 512 bytes Disklabel type: dos Disk identifier: 0x00000000 Device Boot Start End Sectors Size Id Type /dev/mmcblk0p1 8192 31291391 31283200 14.9G c W95 FAT32 (LBA)   推荐使用USB读卡器，速度较为快一些。\nLua库文件的加载路径 Lua 提供一个名为 require 的函数来加载模块，使用也很简单，它只有一个参数，这个参数就是要指定加载的模块名，例如：\n1 2 3  require(\u0026#34;\u0026lt;模块名\u0026gt;\u0026#34;) -- 或者是 -- require \u0026#34;\u0026lt;模块名\u0026gt;\u0026#34;   然后会返回一个由模块常量或函数组成的 table，并且还会定义一个包含该 table 的全局变量。\n或者给加载的模块定义一个别名变量，方便调用：\n1 2 3  local m = require(\u0026#34;module\u0026#34;) print(m.constant) m.func3()   对于自定义的模块，模块文件不是放在哪个文件目录都行，函数 require 有它自己的文件路径加载策略，它会尝试从 Lua 文件或 C 程序库中加载模块。\nrequire 用于搜索 Lua 文件的路径是存放在全局变量 package.path 中，当 Lua 启动后，会以环境变量 LUA_PATH 的值来初始这个环境变量。如果没有找到该环境变量，则使用一个编译时定义的默认路径来初始化。\n1 2 3 4 5  Lua 5.1.5 Copyright (C) 1994-2012 Lua.org, PUC-Rio \u0026gt; print(package.path) ~/lua/?.lua;/usr/local/share/lua/5.1/?.lua;/home/huang/workspace/luactor/?.lua;./?.lua;/usr/local/share/lua/5.1/?.lua;/usr/local/share/lua/5.1/?/init.lua;/usr/local/lib/lua/5.1/?.lua;/usr/local/lib/lua/5.1/?/init.lua;   如果没有 LUA_PATH 这个环境变量，也可以自定义设置\n1 2 3 4 5 6  huang@ThinkPad-X220:~/workspace/luapkg/luasocket-2.0.2$ export LUA_PATH=\u0026#34;4;;\u0026#34; huang@ThinkPad-X220:~/workspace/luapkg/luasocket-2.0.2$ lua Lua 5.1.5 Copyright (C) 1994-2012 Lua.org, PUC-Rio \u0026gt; print(package.path) 4;./?.lua;/usr/local/share/lua/5.1/?.lua;/usr/local/share/lua/5.1/?/init.lua;/usr/local/lib/lua/5.1/?.lua;/usr/local/lib/lua/5.1/?/init.lua; \u0026gt;   可以看到，随便加的环境变量\u0026quot;4;\u0026ldquo;写在了package.path中。\n而为什么4需要两个\u0026rsquo;；\u0026lsquo;号呢：文件路径以 \u0026ldquo;;\u0026rdquo; 号分隔，最后的 2 个 \u0026ldquo;;;\u0026rdquo; 表示新加的路径后面加上原来的默认路径。\n1 2 3 4 5 6  huang@ThinkPad-X220:~/workspace/luapkg/luasocket-2.0.2$ export LUA_PATH=\u0026#34;4;\u0026#34; huang@ThinkPad-X220:~/workspace/luapkg/luasocket-2.0.2$ lua Lua 5.1.5 Copyright (C) 1994-2012 Lua.org, PUC-Rio \u0026gt; print(package.path) 4; \u0026gt;   可见如果只有一个；号，将只采用这个分号。\n如果找过目标文件，则会调用 package.loadfile 来加载模块。否则，就会去找 C 程序库。搜索的文件路径是从全局变量 package.cpath 获取，而这个变量则是通过环境变量 LUA_CPATH 来初始。搜索的策略跟上面的一样，只不过现在换成搜索的是 so 或 dll 类型的文件。如果找得到，那么 require 就会通过 package.loadlib 来加载它。\n我们也可以在lua代码中动态修改package.path变量，\n1 2 3  package.path = \u0026#34;../?.lua;\u0026#34;..package.path require \u0026#34;fun\u0026#34;   这点对于我们自己的lua project的设置来说无疑是很方便的。 参考链接\ncpp调用c函数 由于CPP在链接时与C不太一样，因此在调用C函数时，需要做一定处理。\n将C函数的声明房子 #ifdef __cplusplus 块中\n1 2 3 4 5 6 7 8 9 10 11  #ifdef __cplusplus extern \u0026#34;C\u0026#34; { #endif /*. * c functions declarations ..*/ #ifdef __cplusplus } #endif   多少人在猜你机器的密码呢 VPS在公网就是个待宰的肥肉，都想去登陆，那都谁猜我的IP了呢？\n1  sudo grep \u0026#34;Failed password for root\u0026#34; /var/log/auth.log | awk \u0026#39;{print $11}\u0026#39; | sort | uniq -c | sort -nr | more   grep的简单使用，与 或 非  或操作  1 2 3  grep -E \u0026#39;123|abc\u0026#39; filename // 找出文件（filename）中包含123或者包含abc的行 egrep \u0026#39;123|abc\u0026#39; filename // 用egrep同样可以实现 awk \u0026#39;/123|abc/\u0026#39; filename // awk 的实现方式    与操作  1  grep pattern1 files | grep pattern2 ：显示既匹配 pattern1 又匹配 pattern2 的行。    其他操作  1 2 3 4 5 6 7  grep -i pattern files ：不区分大小写地搜索。默认情况区分大小写， grep -l pattern files ：只列出匹配的文件名， grep -L pattern files ：列出不匹配的文件名， grep -w pattern files ：只匹配整个单词，而不是字符串的一部分（如匹配‘magic’，而不是‘magical’）， grep -v pattern files ：不匹配pattern grep -C number pattern files ：匹配的上下文分别显示[number]行，   iptables的简单使用 其实并不想写iptables相关的内容，因为用的不熟，但是一些常用的命令还是记一下吧\niptables的详细解释\nLinux系统中,防火墙(Firewall),网址转换(NAT),数据包(package)记录,流量统计,这些功能是由Netfilter子系统所提供的，而iptables是控制Netfilter的工具。iptables将许多复杂的规则组织成成容易控制的方式，以便管理员可以进行分组测试，或关闭、启动某组规则。  1 2 3 4  https://blog.phpgao.com/vps_iptables.html http://www.tabyouto.com/bandwagon-vps-for-shadowsocks-was-hacked.html http://my.oschina.net/yqc/blog/82111?fromerr=VxVIazGW http://www.vpser.net/security/linux-iptables.html   1 2 3 4 5 6 7 8 9  # 列出所有规则 iptables -L -n # 更新iptables规则，规则写在/etc/iptables.rules iptables-restore \u0026lt; /etc/iptables.rules # 保存iptables规则，规则写在/etc/iptables.rules iptables-save \u0026gt; /etc/iptables.rules   需要注意的是Debian/Ubuntu上iptables是不会保存规则的。\n需要按如下步骤进行，让网卡关闭是保存iptables规则，启动时加载iptables规则：\n创建/etc/network/if-post-down.d/iptables 文件，添加如下内容：\n1 2  #!/bin/bash iptables-save \u0026gt; /etc/iptables.rules   执行：chmod +x /etc/network/if-post-down.d/iptables 添加执行权限。\n创建/etc/network/if-pre-up.d/iptables 文件，添加如下内容：\n1 2  #!/bin/bash iptables-restore \u0026lt; /etc/iptables.rules   执行：chmod +x /etc/network/if-pre-up.d/iptables 添加执行权限。\niptables的一些常用规则：\n1 2  #允许ping iptables -A INPUT -p icmp -m icmp --icmp-type 8 -j ACCEPT   如果想清空的话，先执行\n/sbin/iptables -P INPUT ACCEPT  然后执行\n/sbin/iptables -F  VPS简单的ssh登陆设置 初次使用VPS，不懂得安全的重要性，直到扣款时候才心疼，这个时候，弱口令，密码登陆什么的，还是都放弃吧，只用ssh登陆，并且换一个自己的端口。参考链接\n简单来说，任何一台主机想登陆VPS的主机都需要有本身的ssh公钥私钥\n1 2  cd ~/.ssh/ ssh-keygen -t rsa -C \u0026#34;username@gmail.com\u0026#34;   然后复制~/.ssh/id_rsa.pub中的内容，就是本机的公钥。\n将公钥添加到VPS服务器的/home/username/.ssh/authorized_keys中，本机就能以username用户名登陆VPS了\n然后在/etc/ssh/sshd_config中禁用禁用 VPS 的密码登录和 root 帐号登录，将以下两项改为no\n1 2 3 4 5  PasswordAuthentication no PermitRootLogin no Port 11111   随后重启SSH服务\n1  sudo service ssh restart   vim删除空行  从网页上copy下代码后，发现很多情况下有不想要的空行，非常影响阅读，通过vim的正则可以解决  Delete all blank lines (^ is start of line; \\s* is zero or more whitespace characters; $ is end of line) 删除所有空白行(^是行的开始，\\s*是零个或者多个空白字符；$是行尾)    1  :g/^\\s*$/d   ubuntu通过命令设置系统时间 在嵌入式开发中，在pcduino或者rpi板子上安装好linux后，系统时间是UTC时间1970年，对于有些软件来说可能影响安装，所以需要命令行修改date\n1  sudo date -s \u0026#34;13 DEC 2015 20:43\u0026#34;   ubuntu终端下中文设置 在安装完ubuntu系统后，我们发现中文支持的不好，主要体现在locale的错误，解决方法：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17  perl: warning: Setting locale failed. perl: warning: Please check that your locale settings: LANGUAGE = (unset), LC_ALL = (unset), LC_PAPER = \u0026#34;zh_CN.UTF-8\u0026#34;, LC_ADDRESS = \u0026#34;zh_CN.UTF-8\u0026#34;, LC_MONETARY = \u0026#34;zh_CN.UTF-8\u0026#34;, LC_NUMERIC = \u0026#34;zh_CN.UTF-8\u0026#34;, LC_TELEPHONE = \u0026#34;zh_CN.UTF-8\u0026#34;, LC_IDENTIFICATION = \u0026#34;zh_CN.UTF-8\u0026#34;, LC_MEASUREMENT = \u0026#34;zh_CN.UTF-8\u0026#34;, LC_TIME = \u0026#34;zh_CN.UTF-8\u0026#34;, LC_NAME = \u0026#34;zh_CN.UTF-8\u0026#34;, LANG = \u0026#34;en_US.UTF-8\u0026#34; are supported and installed on your system. perl: warning: Falling back to the standard locale (\u0026#34;C\u0026#34;).   这是因为中文包没有安装好的缘故，如下命令就可以解决：\n1 2 3 4 5 6  添加简体中文支持 sudo apt-get -y install language-pack-zh-hans language-pack-zh-hans-base 添加繁体中文支持 sudo apt-get -y install language-pack-zh-hant language-pack-zh-hant-base   如果还不行，先观察下locale的配置\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19  huang@localhost:~$ locale locale: Cannot set LC_CTYPE to default locale: No such file or directory locale: Cannot set LC_MESSAGES to default locale: No such file or directory locale: Cannot set LC_ALL to default locale: No such file or directory LANG=en_US.UTF-8 LANGUAGE= LC_CTYPE=\u0026#34;en_US.UTF-8\u0026#34; LC_NUMERIC=zh_CN.UTF-8 LC_TIME=zh_CN.UTF-8 LC_COLLATE=\u0026#34;en_US.UTF-8\u0026#34; LC_MONETARY=zh_CN.UTF-8 LC_MESSAGES=\u0026#34;en_US.UTF-8\u0026#34; LC_PAPER=zh_CN.UTF-8 LC_NAME=zh_CN.UTF-8 LC_ADDRESS=zh_CN.UTF-8 LC_TELEPHONE=zh_CN.UTF-8 LC_MEASUREMENT=zh_CN.UTF-8 LC_IDENTIFICATION=zh_CN.UTF-8 LC_ALL=   再重新配置下语言包\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17  huang@localhost:~$ sudo locale-gen \u0026#34;en_US.UTF-8\u0026#34; Generating locales... en_US.UTF-8... done Generation complete. huang@localhost:~$ sudo pip install shadowsocks^C huang@localhost:~$ sudo locale-gen \u0026#34;zh_CN.UTF-8\u0026#34; Generating locales... zh_CN.UTF-8... done Generation complete. huang@localhost:~$ sudo dpkg-reconfigure locales Generating locales... en_US.UTF-8... done zh_CN.UTF-8... up-to-date zh_HK.UTF-8... done zh_SG.UTF-8... done zh_TW.UTF-8... done Generation complete.   一般就都能解决\nLinux终端下的颜色设置输出 Linux终端下，如果有一个彩色的终端，可以明显提升人的阅读兴趣，通过printf的简单设置即可实现彩色输出\n1 2 3 4  \\033[显示方式;前景色;背景色m 显示方式、前景色、背景色至少一个存在即可。 格式：\\033[显示方式;前景色;背景色m   1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19  前景色 背景色 颜色 30 40 黑色 31 41 红色 32 42 绿色 33 43 黃色 34 44 蓝色 35 45 紫红色 36 46 青蓝色 37 47 白色 显示方式 意义 0 终端默认设置 1 高亮显示 4 使用下划线 5 闪烁 7 反白显示 8 不可见   1 2 3 4 5  \\033[1;31;40m \u0026lt;!--1-高亮显示 31-前景色红色 40-背景色黑色--\u0026gt; \\033[0m \u0026lt;!--采用终端默认设置，即取消颜色设置--\u0026gt; printf(\u0026#34;\\033[1;31;40m\u0026#34;); printf(\u0026#34;\\033[0m\u0026#34;);   tsar监控系统负载和nginx运行情况 tsar是阿里巴巴发布的一款能够实时监控系统状态的命令行工具，并且支持第三方模块扩展，其中比较注明的是nginx模块。使用tsar时，可以将系统负载和nginx运行情况同步同时打出，可以用来定位系统瓶颈，所以广受好评。\ntsar -li1 是其最经典的用法，可以将一般我们感兴趣的监控项每秒更新一次并输出\n1 2 3 4  Time ---cpu-- ---mem-- ---tcp-- -----traffic---- --sda--- ---load- Time util util retran bytin bytout util load1 25/03/16-19:03:30 0.08 10.22 0.00 1.4K 1.2K 0.00 0.33 25/03/16-19:03:31 0.08 10.21 0.00 424.00 468.00 0.00 0.33   如果想使能nginx模块，需要对其进行配置\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28  1. mkdir /etc/tsar/conf.d 2. touch /etc/tsar/conf.d/nginx.conf 3. 写入如下内容并保存 mod_nginx on ####add it to tsar default output output_stdio_mod mod_nginx ####add it to center db #output_db_mod mod_nginx ####add it to nagios send ####set nagios threshold for alert #output_nagios_mod mod_nginx #threshold nginx.value1;N;N;N;N; #threshold nginx.value2;N;N;N;N; #threshold nginx.value3;N;N;N;N; 表示使能nginx模块，并使用stdio输出 4. tsar -li1 Time ---cpu-- ---mem-- ---tcp-- -----traffic---- --sda--- ---load- ------------------nginx----------------- Time util util retran bytin bytout util load1 qps rt sslqps spdyps sslhst 25/03/16-19:06:19 0.08 11.40 7.14 302.00 546.00 0.00 0.02 1.00 0.00 0.00 0.00 0.00   wrk在CentOS系统上的编译方法 wrk作为一款可以内嵌lua脚本的，支持多线程的压测工具，受到了广泛欢迎。在高版本CentOS 7上，直接在wrk目录下执行make，可以首先编译deps/luajit，得到deps/luajit/libluajit.a，然而在低版本上，CentOS 6.5系统中，会报一些莫名奇妙的错误。\n解决方法是，查看wrk的Makefile，发现wrk依赖于luajit，那么首先进入deps/luajit编译它，并且是静态编译\n1 2 3 4 5 6 7  cd wrk cd deps/luajit make -j24 BUILDMODE=static cd ../.. make -j24   rpmbuild环境的快速初始化 需要将代码打包为CentOS的RPM包时，可以先自己在本地新建一个环境\n1 2 3 4  1. mkdir -p ~/rpmbuild/{SOURCES,BUILD,BUILDROOT,RPMS,SRPMS,SPECS} 2. 将代打包的代码压缩包 software.tar.gz 放入SOURCES文件夹 3. 将 software.spec 放入SPECS文件夹 4. rpmbuild -ba path/to/software.spec 即可   git记住密码，不用每次都输密码才登入 git有两种方式，一种是ssh方式，配置公钥私钥，对于新手而言还是比较麻烦的；另一种是http方式，这里有一个办法可以让git记住密码，避免每次都需要输入密码\n1 2 3 4 5 6 7  1. touch ~/.git-credentials 2. 将 https://{username}:{password}@github.com 写入该文件 3. git config --global credential.helper store 就可以使得git记住密码了 4. 此时查看 ~/.gitconfig，发现多了一项 [credential] helper = store   centos系统上某些软件，比如gcc、python等版本过低的解决方案 在CentOS Server上，经常会遇到某些软件依赖版本过低的问题，比如CentOS 6.5的python是2.7版本的，gcc是4.2版本的，那么我们如何获得一个干净的、与原版本无冲突的运行环境呢。CentOS系提供了一个叫SCL的工具，可以帮我们实现目的\n1 2 3 4 5 6  $ sudo wget http://people.centos.org/tru/devtools-1.1/devtools-1.1.repo -P /etc/yum.repos.d $ sudo sh -c \u0026#39;echo \u0026#34;enabled=1\u0026#34; \u0026gt;\u0026gt; /etc/yum.repos.d/devtools-1.1.repo\u0026#39; $ sudo yum install devtoolset-1.1 $ scl enable devtoolset-1.1 bash $ gcc --version # 通过devtoolset工具可以暂时提高gcc版本，而不更改之前服务器的配置，这个很有效果，高版本的gcc会智能保留symbol。   1 2 3 4 5  # CentOS 6.5 sudo yum install centos-release-SCL sudo yum install python27 scl enable python27 bash python --version   ubuntu系统上某些软件，比如gcc等版本过高的解决方案 与CentOS相反，debian系发行版的软件版本都很高，Ubuntu 16.04的gcc 版本已经到了5.2，然而编译一些早期linux内核的话，需要gcc-4.7左右的版本，这时候我们怎么办呢，有两个方法：\n 通过apt安装低版本gcc  sudo apt-get install gcc-4.7 在编译linux 内核时， make CC=gcc-4.7 即可   update-alternatives可以帮忙更改符号链接，指向不同版本的gcc  参考链接1 参考链接2 附赠    python的matplotlib库实现绘制图标  sudo apt-get install python-matplotlib  参考链接 example\npython使用requests库发送http请求 参考链接\npython解析命令行参数：argparse 参考链接\ngit比较两次commit的差异 通过比较两次commit的代码差异，能够快速理解此次commit的目的，理解作者意图\n git log  查看commit历史    1 2 3 4 5 6 7 8 9 10 11  commit 2279c3f4a8a42e696a0f34e6e9b6289487da92c1 Author: bg2bkk \u0026lt;bg2bkk@gmail.com\u0026gt; Date: Sun Mar 13 09:12:26 2016 +0800 add SO_REUSEADDR和SO_REUSEPORT.md commit 2b9d85f8427c5ca9e4f9c128c22acd280eb94405 Author: bg2bkk \u0026lt;bg2bkk@gmail.com\u0026gt; Date: Sat Mar 12 01:16:00 2016 +0800 add 采用二级指针实现单链表操作 单链表翻转 删除单链表结点    git diff commit 2279c3f4a8a42e696a0f34e6e9b6289487da92c1 2b9d85f8427c5ca9e4f9c128c22acd280eb94405  git返回强制返回某次提交  git log git reset 5f4769a98985b5acfea45462df27830e51a75145 \u0026ndash;hard  可见commit号很重要    iptables允许端口被外网访问 防火墙设置，配置1985端口可以被外网访问\n sudo iptables -A INPUT -m state \u0026ndash;state NEW -m tcp -p tcp \u0026ndash;dport 1985 -j ACCEPT  tcpdump过滤指定标志的packet 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15  # tcp包里有个flags字段表示包的类型，tcpdump可以根据该字段抓取相应类型的包： # tcp[13] 就是 TCP flags (URG,ACK,PSH,RST,SYN,FIN) # Unskilled 32 # Attackers 16 # Pester 8 # Real 4 # Security 2 # Folks 1 #抓取fin包： tcpdump -ni any port 9001 and \u0026#39;tcp[13] \u0026amp; 1 != 0 \u0026#39; -s0 -w fin.cap -vvv #抓取syn+fin包： tcpdump -ni any port 9001 and \u0026#39;tcp[13] \u0026amp; 3 != 0 \u0026#39; -s0 -w syn_fin.cap -vvv #抓取rst包： tcpdump -ni any port 9001 and \u0026#39;tcp[13] \u0026amp; 4 != 0 \u0026#39; -s0 -w rst.cap -vvv   参考链接\n查看进程的内存占用情况 用Ternary Search Tree代替Trie Tree后，我想知道我的进程内存占用有多大区别。\n  ps -e -o \u0026lsquo;pid,comm,args,pcpu,rsz,vsz,stime,user,uid\u0026rsquo; | grep MyDict\n rsz是实际占用内存，单位是KB    pmap -d pid\n  ","date":"2016-07-24T16:36:33+08:00","permalink":"https://bg2bkk.github.io/p/effective-tips-in-daily-work/","title":"effective tips in daily work"},{"content":"  eclipse下载\n neon http://ftp.jaist.ac.jp/pub/eclipse/technology/epp/downloads/release/neon/R/eclipse-jee-neon-R-linux-gtk-x86_64.tar.gz system workbench:    stlink 驱动\n http://erika.tuxfamily.org/wiki/index.php?title=Tutorial:_STM32_-_Integrated_Debugging_in_Eclipse_using_GNU_toolchain\u0026oldid=5474 http://www.st.com/content/st_com/en/products/embedded-software/development-tool-software/stsw-link004.html# Linux: https://github.com/texane/stlink  sudo apt-get install autoreconf sudo apt-get install libusb-1.0-0 libusb-1.0-0-dev make \u0026amp;\u0026amp; make -j \u0026amp;\u0026amp; make install sudo ./st-util  之前还需要做udev.rules，现在发现不需要     用法：http://erika.tuxfamily.org/wiki/index.php?title=Tutorial:STM32-_Integrated_Debugging_in_Eclipse_using_GNU_toolchain\u0026amp;oldid=5474    arm gcc compiler\n sudo apt-get install gcc-arm-none-eabi    插件和eclipse环境配置：\n  eclipse cdt 插件\n https://eclipse.org/cdt/downloads.php    http://gnuarmeclipse.sourceforge.net/updates\n gnu arm eclipse plugins的几种安装方法 http://gnuarmeclipse.github.io/plugins/install/ http://gnuarmeclipse.github.io/eclipse/workspace/preferences/ http://gnuarmeclipse.github.io/plugins/packs-manager/    ac6 system workbench: http://www.ac6-tools.com/Eclipse-updates/org.openstm32.system-workbench.site/\n 有了它就可以ac6 debugger了，但是没办法，neon不支持 http://www.emcu.it/STM32/What_should_I_use_to_develop_on_STM32/stm32f0_linux_dvlpt.pdf      如何使用eclipse新建工程\n 可以安装以上两个插件后，从eclipse新建ac6工程，下载相应库即可，ac6保证这个好使； 可以从cube新建工程sw4stm32类型的工程，然后引入SW4STM32工程    如何调试工程\n  debugger: AC6\t普通的\n 使用ac6调试, http://www.xlgps.com/article/387805.html    debugger: hardware debugger configuration\n http://stm32discovery.nano-age.co.uk/open-source-development-with-the-stm32-discovery/getting-hardware-debuging-working-with-eclipse-and-code-sourcey neon还不支持ac6 debugger，所以只能用后者 http://www.openstm32.org/forumthread3023    create debugging configuration\n http://www.openstm32.org/Creating+debug+configuration 开始debug      st-flash 烧录工具 https://www.youtube.com/watch?v=HKX12hJApZM\n  openocd https://www.youtube.com/watch?v=ZeUQXjTg-8c\n ./configure \u0026ndash;enable-verbose \u0026ndash;enable-verbose-jtag-io \u0026ndash;enable-parport \u0026ndash;enable-jlink \u0026ndash;enable-ulink \u0026ndash;enable-stlink \u0026ndash;enable-ti-icdi make -j \u0026amp;\u0026amp; sudo make install openocd -f tcl/board/stm32f4discovery.cfg    openocd是debug server，3333端口\n  eclipse需要debug configuration\n  eclipse的设置\n 代码自动提示：http://blog.csdn.net/u012750578/article/details/16811227    elua\n  1 2 3  openocd -f ../../openocd-0.9.0/tcl/board/stm32f429discovery.cfg -c \u0026#34;init\u0026#34; -c \u0026#34;reset halt\u0026#34; -c \u0026#34;sleep 100\u0026#34; -c \u0026#34;wait_halt 2\u0026#34; -c \u0026#34;echo \\\u0026#34;--- Writing elua_lua_stm32f4discovery.bin\\\u0026#34;\u0026#34; -c \u0026#34;flash write_image erase elua_lua_stm32f4discovery.bin 0x08000000\u0026#34; -c \u0026#34;sleep 100\u0026#34; -c \u0026#34;echo \\\u0026#34;--- Verifying\\\u0026#34;\u0026#34; -c \u0026#34;verify_image elua_lua_stm32f4discovery.bin 0x08000000\u0026#34; -c \u0026#34;sleep 100\u0026#34; -c \u0026#34;echo \\\u0026#34;--- Done\\\u0026#34;\u0026#34; -c \u0026#34;resume\u0026#34; -c \u0026#34;shutdown\u0026#34; st-flash --reset write elua_lua_stm32f4discovery.bin 0x8000000   ","date":"2016-07-06T12:51:05+08:00","permalink":"https://bg2bkk.github.io/p/stm32_eclipse_openocd_stlink/","title":"stm32_eclipse_openocd_stlink"},{"content":"最近突然看协程和并发编程比较多，遇到这样一道题\n 题目：子线程循环 10 次，接着主线程循环 100 次，接着又回到子线程循环 10 次，接着再回到主线程又循环 100 次，如此循环50次，试写出代码。\n 参考文档里的代码采用C++11编写，而我，很不幸的，看不懂。\n我想我的cpp已经退化到看不见了吧，然后c++11我更加看不懂了，甚至连cpp较为官方的文档都开始采用c++11了。\n虽然我不会c++11，但是我会lua、c、golang、python、shell，我要报复性的把这个题做了。\nC++11 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27  #include\u0026lt;iostream\u0026gt;#include\u0026lt;thread\u0026gt;#include\u0026lt;mutex\u0026gt;#include\u0026lt;condition_variable\u0026gt;using namespace std; mutex m; condition_variable cond; int flag=10; void fun(int num){ for(int i=0;i\u0026lt;2;i++){ unique_lock\u0026lt;mutex\u0026gt; lk(m);//A unique lock is an object that manages a mutex object with unique ownership in both states: locked and unlocked.  while(flag!=num) cond.wait(lk);//在调用wait时会执行lk.unlock()  for(int j=0;j\u0026lt;num;j++) cout\u0026lt;\u0026lt;j\u0026lt;\u0026lt;\u0026#34; \u0026#34;; cout\u0026lt;\u0026lt;endl; flag=(num==10)?100:10; cond.notify_one();//被阻塞的线程唤醒后lk.lock()恢复在调用wait前的状态  } } int main(){ thread child(fun,10); fun(100); child.join(); return 0; }   Lua lua的协程使得主从两个thread之间并没有竞争关系，所以很顺畅的就可以把代码写出来，逻辑也十分简单\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40  local thread = coroutine.create(function() for cnt = 1, 5 do local tmp = {} for i = 1, 10 do table.insert(tmp, i) end print(\u0026#39;child: \u0026#39;, table.concat(tmp, \u0026#39; \u0026#39;)) coroutine.yield() local tmp = {} for i = 1, 10 do table.insert(tmp, i) end print(\u0026#39;child: \u0026#39;, table.concat(tmp, \u0026#39; \u0026#39;)) coroutine.yield() end end) for i=1, 5 do coroutine.resume(thread) local tmp = {} for i = 1, 100 do table.insert(tmp, i) end print(\u0026#39;main: \u0026#39;, table.concat(tmp, \u0026#39; \u0026#39;)) print(\u0026#39;------------------------------------\u0026#39;) coroutine.resume(thread) local tmp = {} for i = 1, 100 do table.insert(tmp, i) end print(\u0026#39;main: \u0026#39;, table.concat(tmp, \u0026#39; \u0026#39;)) print(\u0026#39;====================================\u0026#39;) end   1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32  child: 1 2 3 4 5 6 7 8 9 10 main: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 ------------------------------------ child: 1 2 3 4 5 6 7 8 9 10 main: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 ==================================== child: 1 2 3 4 5 6 7 8 9 10 main: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 ------------------------------------ child: 1 2 3 4 5 6 7 8 9 10 main: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 ==================================== child: 1 2 3 4 5 6 7 8 9 10 main: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 ------------------------------------ child: 1 2 3 4 5 6 7 8 9 10 main: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 ==================================== child: 1 2 3 4 5 6 7 8 9 10 main: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 ------------------------------------ child: 1 2 3 4 5 6 7 8 9 10 main: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 ==================================== child: 1 2 3 4 5 6 7 8 9 10 main: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 ------------------------------------ child: 1 2 3 4 5 6 7 8 9 10 main: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 ====================================   c  volatile: 使用volatile类型的全局变量和sleep函数实现阻塞和互斥  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53  #include \u0026lt;pthread.h\u0026gt;#include \u0026lt;string.h\u0026gt;#include \u0026lt;stdio.h\u0026gt;#include \u0026lt;stdlib.h\u0026gt;#include \u0026lt;unistd.h\u0026gt;#include \u0026lt;errno.h\u0026gt;#include \u0026lt;ctype.h\u0026gt; volatile int cond = 0; void func(int n) { int i = 0, j = 0; for(j=0; j\u0026lt;5; j++){ while(cond != 0) sleep(1); printf(\u0026#34;\\n----------------------------------\\n\u0026#34;); printf(\u0026#34;child: \u0026#34;); for(i=0; i \u0026lt; 10; i++ ) printf(\u0026#34;%d\\t\u0026#34;, i); printf(\u0026#34;\\n\u0026#34;); cond = 1; } } int main() { pthread_t tid; int s = pthread_create(\u0026amp;tid, NULL, func, 10); if(s != 0){ printf(\u0026#34;pthread_create error for %s\u0026#34;, strerror(errno)); exit(1); } int i = 0, j = 0; for(j=0; j \u0026lt; 5; j++){ while(cond != 1) sleep(1); printf(\u0026#34;master: \u0026#34;); for(i=0; i \u0026lt; 100; i++ ) printf(\u0026#34;%d\\t\u0026#34;, i); printf(\u0026#34;\\n\u0026#34;); printf(\u0026#34;==================================\\n\u0026#34;); cond = 0; } s = pthread_join(tid, NULL); if(s != 0){ printf(\u0026#34;pthread_join error for %s\u0026#34;, strerror(errno)); exit(1); } }     signal: 使用信号，用于进程互相通知对方\n  semop: 使用System V　进行线程同步，控制并发访问\n  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89  #include \u0026lt;pthread.h\u0026gt;#include \u0026lt;string.h\u0026gt;#include \u0026lt;stdio.h\u0026gt;#include \u0026lt;stdlib.h\u0026gt;#include \u0026lt;unistd.h\u0026gt;#include \u0026lt;errno.h\u0026gt;#include \u0026lt;ctype.h\u0026gt;#include \u0026lt;signal.h\u0026gt; #include \u0026lt;sys/types.h\u0026gt;#include \u0026lt;sys/ipc.h\u0026gt;#include \u0026lt;sys/sem.h\u0026gt; volatile int cond = 0; int semid; struct sembuf sem[2]; void func(int n) { //\tprintf(\u0026#34;ppid = %d, getpid = %d\\n\u0026#34;, getppid(), getpid());  int i = 0, j = 0; for(j=0; j\u0026lt;5; j++){ sem[1].sem_num = 1; sem[1].sem_op = -1; sem[1].sem_flg = 0; semop(semid, \u0026amp;sem[1], 1); printf(\u0026#34;\\n----------------------------------\\n\u0026#34;); printf(\u0026#34;child: \u0026#34;); for(i=0; i \u0026lt; 10; i++ ) printf(\u0026#34;%d\\t\u0026#34;, i); printf(\u0026#34;\\n\u0026#34;); sem[0].sem_num = 0; sem[0].sem_op = 1; sem[0].sem_flg = 0; semop(semid, \u0026amp;sem[0], 1); } } int main() { semid = semget(IPC_PRIVATE, 2, 0666| IPC_CREAT); if(semid \u0026lt; 0){ printf(\u0026#34;semget error for %s\u0026#34;, strerror(errno)); exit(1); } pthread_t tid; int s = pthread_create(\u0026amp;tid, NULL, func, getpid()); if(s != 0){ printf(\u0026#34;pthread_create error for %s\u0026#34;, strerror(errno)); exit(1); } int i = 0, j = 0; for(j=0; j \u0026lt; 5; j++){ sem[0].sem_num = 0; sem[0].sem_op = -1; sem[0].sem_flg = 0; sem[1].sem_num = 1; sem[1].sem_op = 1; sem[1].sem_flg = 0; semop(semid, \u0026amp;sem[1], 1); semop(semid, \u0026amp;sem[0], 1); printf(\u0026#34;master: \u0026#34;); for(i=0; i \u0026lt; 100; i++ ) printf(\u0026#34;%d\\t\u0026#34;, i); printf(\u0026#34;\\n\u0026#34;); printf(\u0026#34;==================================\\n\u0026#34;); sem[1].sem_num = 1; sem[1].sem_op = 1; sem[1].sem_flg = 0; } s = pthread_join(tid, NULL); if(s != 0){ printf(\u0026#34;pthread_join error for %s\u0026#34;, strerror(errno)); exit(1); } }   python  python yield  golang  goroutine之间可以通过channel进行多任务同步  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42  package main import \u0026#34;fmt\u0026#34; var c1 chan int var c2 chan int func task(loop int, times int) { for i := 0; i \u0026lt; loop; i++ { \u0026lt;-c1 fmt.Println(\u0026#34;------------------------------\u0026#34;) fmt.Print(\u0026#34;child: \u0026#34;) for j := 0; j \u0026lt; times; j++ { fmt.Printf(\u0026#34;%d\\t\u0026#34;, j) } fmt.Println() c2 \u0026lt;- 1 } } func main() { times := 100 loop := 5 c1 = make(chan int, 1024) c2 = make(chan int, 1024) go task(loop, 10) for i := 0; i \u0026lt; loop; i++ { c1 \u0026lt;- 1 \u0026lt;-c2 fmt.Print(\u0026#34;master: \u0026#34;) for j := 0; j \u0026lt; times; j++ { fmt.Printf(\u0026#34;%d\\t\u0026#34;, j) } fmt.Println() fmt.Println(\u0026#34;==============================\u0026#34;) } }    其实golang的chan作为阻塞读取的协程通信组件，有一个也就能实现谁先谁后的同步了；毕竟，不光 val \u0026lt;- chan 这种读操作会堵塞，chan \u0026lt;- val这种写操作也会被堵塞  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39  package main import \u0026#34;fmt\u0026#34; var c1 chan int func task(loop int, times int) { for i := 0; i \u0026lt; loop; i++ { fmt.Println(\u0026#34;------------------------------\u0026#34;) fmt.Print(\u0026#34;child: \u0026#34;) for j := 0; j \u0026lt; times; j++ { fmt.Printf(\u0026#34;%d\\t\u0026#34;, j) } fmt.Println() c1 \u0026lt;- 1 } } func main() { times := 100 loop := 5 c1 = make(chan int) go task(loop, 10) for i := 0; i \u0026lt; loop; i++ { \u0026lt;-c1 fmt.Print(\u0026#34;master: \u0026#34;) for j := 0; j \u0026lt; times; j++ { fmt.Printf(\u0026#34;%d\\t\u0026#34;, j) } fmt.Println() fmt.Println(\u0026#34;==============================\u0026#34;) } }   shell  token bucket  ","date":"2016-05-26T01:39:59+08:00","permalink":"https://bg2bkk.github.io/p/%E4%B8%80%E9%81%93%E5%A4%9A%E7%BA%BF%E7%A8%8B%E9%9D%A2%E8%AF%95%E9%A2%98/","title":"一道多线程面试题"},{"content":"结论 1. SO_REUSEPORT用于多个socket监听同一个TCP链接 2. SO_REUSEADDR可用于多个进程bind同一端口，但需要TCP连接的四元组不一样。 3. SO_REUSEPORT比SO_REUSEADDR更加扩展，但是也带来了隐患，需要额外注意 4. SO_REUSEADDR的最大作用是，当服务因故障重启时，不用等待需要绑定的端口从TIME_WAIT状态变更到CLOSED状态，就可以直接绑定该端口  引言 nginx 1.9.1引入了 SO_REUSEPORT选项，在高版本（linux kernel 3.9以上）系统上可用。该选项允许多个socket监听同一个IP:PORT组合，\n SO_REUSEPORT可以简化服务器编程 prefork模式：master预先分配进程池，每一个client连接用一个进程处理  省资源，不用每次都fork，然后再回收 可控制，预先分配的进程池大小是固定的   SO_REUSEPORT使得多进程时不用使master再做管理工作，比如管理子进程，设置信号等等，设置不需要一个master进程，只需要子进程监听同一个端口就行。操作系统做了大部分工作。  这里还有个好处是，C写的server模块，python写的server模块，它们可以共存监听同一个端口，灵活性更好    听听linux kernle维护者怎么说  允许多个进程绑定host上的同一端口 只需要第一个绑定端口的进程指定SO_REUSEPORT选项，后继者都可以绑定该端口，所以需要担心的是端口劫持，不希望恶意程序能accept该端口的连接。 方法是后继者要与第一次绑定端口的进程的USER ID一样，比如用root和普通用户启动程序绑定同一个端口，会报address already in use SO_REUSEPORT的负载均衡性能更好   TCP和UDP都可以用  UDP场景中，在DNS server的应用比较有意义，可以负载均衡的处理dns请求 作者指出，SO_REUSEADDR虽然也能让UDP连接绑定同一端口，但是SO_REUSEPORT可以防止劫持，并能将请求均衡的分配给监听的线程    传统多线程的工作模式的缺点    传统的多线程server都是有一个listener线程绑定端口并接受所有的请求，然后传递给其他线程，而这个listener往往会成为瓶颈    master绑定端口，每个slave轮流accept从该端口获取连接（nginx）   缺点是有可能导致每个slave不能平均的处理连接，unblanced；有的slave处理的过多，有的slave处理的过少，导致cpu资源不能充分利用 SO_REUSEPORT的实现可以使请求平均的分配给堵塞在accept上的各个进程    SO_REUSEPORT的应用举例  server.py  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15  import socket import os SO_REUSEPORT = 15 s = socket.socket(socket.AF_INET, socket.SOCK_STREAM) s.setsockopt(socket.SOL_SOCKET, SO_REUSEPORT, 1) s.bind((\u0026#39;\u0026#39;, 10000)) s.listen(1) while True: conn, addr = s.accept() print(\u0026#39;Connected to {}\u0026#39;.format(os.getpid())) data = conn.recv(1024) conn.send(data) conn.close()   启动两个进程，都绑定10000端口；使用nc作为client\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16  $ python server.py\u0026amp; [1] 12649 $ python server.py\u0026amp; [2] 12650 $ echo data | nc localhost 10000 Connected to 12649 data $ echo data | nc localhost 10000 Connected to 12650 data $ echo data | nc localhost 10000 Connected to 12649 data $ echo data | nc localhost 10000 Connected to 12650 data   再启动一个新的进程显然也是可以的\n1 2 3 4 5 6 7 8  $ python server.py\u0026amp; [3] 14021 $ echo data | nc localhost 10000 Connected to 12650 data $ echo data | nc localhost 10000 Connected to 14021 data     SO_REUSEPORT在golang中的实践\n  SO_REUSEADDR的使用场景\n 在某tcp连接处于 TIME_WAIT状态时，它所占用的port不能被立刻使用 例如server端服务挂掉后需要重启，重启时发现需要bind的端口处于TIME_WAIT状态，不能立刻使用，错误码为EADDRINUSE，glibc将这个错误码渲染为 \u0026ldquo;Address already in use\u0026rdquo; TIME_WAIT状态持续时间为2MSL，一个MSL通常是30s到2min，所以该状态时长为1min到4min；这是不可忍受的 SO_REUSEADDR可以使得进程能够绑定处于TIME_WAIT状态的端口，在服务重启的时候很有用 SO_REUSEADDR同样可以使得进程能够绑定处于ESTABLISHED状态的连接 无论如何，SO_REUSEADDR不允许相同ip和port的四元组存在    SO_REUSEPORT 和 SO_REUSEADDR 对比（待续）  前者可以防止端口被恶意进程劫持 前者可以使请求平均分配给各个进程  参考链接  lwn: the SO_REUSEPORT socket option topic on so_reuseaddr and so_reuseport on stackoverflow  ","date":"2016-05-09T16:55:42+08:00","permalink":"https://bg2bkk.github.io/p/so_reuseaddr%E5%92%8Cso_reuseport/","title":"SO_REUSEADDR和SO_REUSEPORT"},{"content":"引言  三次握手的过程中，当用户首次访问server时，发送syn包，server根据用户IP生成cookie，并与syn+ack一同发回client；client再次访问server时，在syn包携带TCP cookie；如果server校验合法，则在用户回复ack前就可以直接发送数据；否则按照正常三次握手进行。\n  TFO提高性能的关键是省去了热请求的三次握手，这在充斥着小对象的移动应用场景中能够极大提升性能。\n Google研究发现TCP 二次握手是页面延迟时间的重要部分，所以提出TFO\nTFO的fast open标志体现在TCP报文的头部的OPTION字段\nTCP Fast Open的标准文档是rfc7413\nTFO与2.6.34内核合并到主线，lwn通告地址\nTFO的使用目前还是有些复杂的，从linux的network文档来看：\nTFO的配置说明：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27  tcp_fastopen - INTEGER Enable TCP Fast Open feature (draft-ietf-tcpm-fastopen) to send data in the opening SYN packet. To use this feature, the client application must use sendmsg() or sendto() with MSG_FASTOPEN flag rather than connect() to perform a TCP handshake automatically. The values (bitmap) are 1: Enables sending data in the opening SYN on the client w/ MSG_FASTOPEN. 2: Enables TCP Fast Open on the server side, i.e., allowing data in a SYN packet to be accepted and passed to the application before 3-way hand shake finishes. 4: Send data in the opening SYN regardless of cookie availability and without a cookie option. 0x100: Accept SYN data w/o validating the cookie. 0x200: Accept data-in-SYN w/o any cookie option present. 0x400/0x800: Enable Fast Open on all listeners regardless of the TCP_FASTOPEN socket option. The two different flags designate two different ways of setting max_qlen without the TCP_FASTOPEN socket option. Default: 1 Note that the client \u0026amp; server side Fast Open flags (1 and 2 respectively) must be also enabled before the rest of flags can take effect. See include/net/tcp.h and the code for more details.   为了启用 tcp fast open功能\n- client需要使用sendmsg或者sento系统调用，加上MSG_FASTOPEN flag，来连接server端，代替connect系统调用。 - 对server端不做要求。  linux系统（高版本内核）默认tcp_fastopen为1：\n1 2 3  $ sysctl -a | grep fastopen net.ipv4.tcp_fastopen = 1   测试代码： server.c\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43  // reference: http://blog.csdn.net/hanhuili/article/details/8540227  #include \u0026lt;string.h\u0026gt; #include \u0026lt;sys/types.h\u0026gt; /* See NOTES */#include \u0026lt;sys/socket.h\u0026gt;#include \u0026lt;netinet/in.h\u0026gt;int main(){ int portno = 6666; socklen_t clilen; char buffer[256]; struct sockaddr_in serv_addr, cli_addr; int cfd; int sfd = socket(AF_INET, SOCK_STREAM, 0); // Create socket  bzero((char *) \u0026amp;serv_addr, sizeof(serv_addr)); serv_addr.sin_family = AF_INET; serv_addr.sin_addr.s_addr = INADDR_ANY; serv_addr.sin_port = htons(portno); bind(sfd, \u0026amp;serv_addr,sizeof(serv_addr)); // Bind to well known address  int qlen = 5; // Value to be chosen by application  int err = setsockopt(sfd, IPPROTO_TCP/*SOL_TCP*/, 23/*TCP_FASTOPEN*/, \u0026amp;qlen, sizeof(qlen)); listen(sfd,1); // Mark socket to receive connections  while(1){ cfd = accept(sfd, NULL, 0); // Accept connection on new socket  while(1){ int len = read(cfd,buffer,256); if(len) printf(\u0026#34;tcp fast open: %s\\n\u0026#34;,buffer); else break; // read and write data on connected socket cfd \t} memset(buffer, 0, 256); close(cfd); } }   测试代码：client.c\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34  #include \u0026lt;sys/socket.h\u0026gt;#include \u0026lt;netinet/in.h\u0026gt;#include \u0026lt;netdb.h\u0026gt; #include \u0026lt;errno.h\u0026gt;int main(){ struct sockaddr_in serv_addr; struct hostent *server; char *data = \u0026#34;Hello, tcp fast open\u0026#34;; int data_len = strlen(data); int sfd = socket(AF_INET, SOCK_STREAM, 0); server = gethostbyname(\u0026#34;localhost\u0026#34;); bzero((char *) \u0026amp;serv_addr, sizeof(serv_addr)); serv_addr.sin_family = AF_INET; bcopy((char *)server-\u0026gt;h_addr, (char *)\u0026amp;serv_addr.sin_addr.s_addr, server-\u0026gt;h_length); serv_addr.sin_port = htons(6666); // /usr/src/linux-headers-4.4.0-22/include/linux/socket.h:#define MSG_FASTOPEN\t0x20000000\t/* Send data in TCP SYN */  // int len = sendto(sfd, data, data_len, 0x20000000/*MSG_FASTOPEN*/,  int len = sendto(sfd, data, data_len, MSG_FASTOPEN/*MSG_FASTOPEN*/, (struct sockaddr *) \u0026amp;serv_addr, sizeof(serv_addr)); if(errno != 0){ printf(\u0026#34;error: %s\\n\u0026#34;, strerror(errno)); } close(sfd); }   通信过程：tcpdump\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61  $ sudo tcpdump -i any port 6666 -X # 第一次 ./client.o 00:29:34.820187 IP localhost.51388 \u0026gt; localhost.6666: Flags [S], seq 755101042, win 43690, options [mss 65495,sackOK,TS val 17053 ecr 0,nop,wscale 7,unknown-34,nop,nop], length 0 0x0000: 4500 0040 afef 4000 4006 8cc6 7f00 0001 E..@..@.@....... 0x0010: 7f00 0001 c8bc 1a0a 2d01 ed72 0000 0000 ........-..r.... 0x0020: b002 aaaa fe34 0000 0204 ffd7 0402 080a .....4.......... 0x0030: 0000 429d 0000 0000 0103 0307 2202 0101 ..B.........\u0026#34;... 00:29:34.820284 IP localhost.6666 \u0026gt; localhost.51388: Flags [S.], seq 3725111481, ack 755101043, win 43690, options [mss 65495,sackOK,TS val 17053 ecr 17053,nop,wscale 7], length 0 0x0000: 4500 003c 0000 4000 4006 3cba 7f00 0001 E..\u0026lt;..@.@.\u0026lt;..... 0x0010: 7f00 0001 1a0a c8bc de08 b0b9 2d01 ed73 ............-..s 0x0020: a012 aaaa fe30 0000 0204 ffd7 0402 080a .....0.......... 0x0030: 0000 429d 0000 429d 0103 0307 ..B...B..... 00:29:34.820372 IP localhost.51388 \u0026gt; localhost.6666: Flags [P.], seq 1:21, ack 1, win 342, options [nop,nop,TS val 17053 ecr 17053], length 20 0x0000: 4500 0048 aff0 4000 4006 8cbd 7f00 0001 E..H..@.@....... 0x0010: 7f00 0001 c8bc 1a0a 2d01 ed73 de08 b0ba ........-..s.... 0x0020: 8018 0156 fe3c 0000 0101 080a 0000 429d ...V.\u0026lt;........B. 0x0030: 0000 429d 4865 6c6c 6f2c 2074 6370 2066 ..B.Hello,.tcp.f 0x0040: 6173 7420 6f70 656e ast.open 00:29:34.820433 IP localhost.6666 \u0026gt; localhost.51388: Flags [.], ack 21, win 342, options [nop,nop,TS val 17053 ecr 17053], length 0 0x0000: 4500 0034 f227 4000 4006 4a9a 7f00 0001 E..4.\u0026#39;@.@.J..... 0x0010: 7f00 0001 1a0a c8bc de08 b0ba 2d01 ed87 ............-... 0x0020: 8010 0156 fe28 0000 0101 080a 0000 429d ...V.(........B. 0x0030: 0000 429d ..B. 00:29:34.859246 IP localhost.6666 \u0026gt; localhost.51388: Flags [.], ack 22, win 342, options [nop,nop,TS val 17063 ecr 17053], length 0 0x0000: 4500 0034 f228 4000 4006 4a99 7f00 0001 E..4.(@.@.J..... 0x0010: 7f00 0001 1a0a c8bc de08 b0ba 2d01 ed88 ............-... 0x0020: 8010 0156 fe28 0000 0101 080a 0000 42a7 ...V.(........B. 0x0030: 0000 429d ..B. # 第二次 ./client.o 00:29:39.271936 IP localhost.51398 \u0026gt; localhost.6666: Flags [S], seq 2362540136, win 43690, options [mss 65495,sackOK,TS val 18166 ecr 0,nop,wscale 7,exp-tfo cookiereq], length 0 0x0000: 4500 0040 c69e 4000 4006 7617 7f00 0001 E..@..@.@.v..... 0x0010: 7f00 0001 c8c6 1a0a 8cd1 8068 0000 0000 ...........h.... 0x0020: b002 aaaa fe34 0000 0204 ffd7 0402 080a .....4.......... 0x0030: 0000 46f6 0000 0000 0103 0307 fe04 f989 ..F............. 00:29:39.271986 IP localhost.6666 \u0026gt; localhost.51398: Flags [S.], seq 3703577773, ack 2362540137, win 43690, options [mss 65495,sackOK,TS val 18166 ecr 18166,nop,wscale 7], length 0 0x0000: 4500 003c 0000 4000 4006 3cba 7f00 0001 E..\u0026lt;..@.@.\u0026lt;..... 0x0010: 7f00 0001 1a0a c8c6 dcc0 1cad 8cd1 8069 ...............i 0x0020: a012 aaaa fe30 0000 0204 ffd7 0402 080a .....0.......... 0x0030: 0000 46f6 0000 46f6 0103 0307 ..F...F..... 00:29:39.272038 IP localhost.51398 \u0026gt; localhost.6666: Flags [P.], seq 1:21, ack 1, win 342, options [nop,nop,TS val 18166 ecr 18166], length 20 0x0000: 4500 0048 c69f 4000 4006 760e 7f00 0001 E..H..@.@.v..... 0x0010: 7f00 0001 c8c6 1a0a 8cd1 8069 dcc0 1cae ...........i.... 0x0020: 8018 0156 fe3c 0000 0101 080a 0000 46f6 ...V.\u0026lt;........F. 0x0030: 0000 46f6 4865 6c6c 6f2c 2074 6370 2066 ..F.Hello,.tcp.f 0x0040: 6173 7420 6f70 656e ast.open 00:29:39.272072 IP localhost.6666 \u0026gt; localhost.51398: Flags [.], ack 21, win 342, options [nop,nop,TS val 18166 ecr 18166], length 0 0x0000: 4500 0034 5a58 4000 4006 e269 7f00 0001 E..4ZX@.@..i.... 0x0010: 7f00 0001 1a0a c8c6 dcc0 1cae 8cd1 807d ...............} 0x0020: 8010 0156 fe28 0000 0101 080a 0000 46f6 ...V.(........F. 0x0030: 0000 46f6 ..F. 00:29:39.311280 IP localhost.6666 \u0026gt; localhost.51398: Flags [.], ack 22, win 342, options [nop,nop,TS val 18176 ecr 18166], length 0 0x0000: 4500 0034 5a59 4000 4006 e268 7f00 0001 E..4ZY@.@..h.... 0x0010: 7f00 0001 1a0a c8c6 dcc0 1cae 8cd1 807e ...............~ 0x0020: 8010 0156 fe28 0000 0101 080a 0000 4700 ...V.(........G. 0x0030: 0000 46f6 ..F.   奇怪的是，在代码中启用tcp_fastopen的结果和不启用，并没有区别。那这是什么原因呢？\n通过搜索，发现在介绍tcp fast open优化shadowsocks时，设置net.ipv4.tcp_fastopen为3，虽然奇怪，但是可以试试：\n1 2 3  $ sysctl -a | grep fastopen net.ipv4.tcp_fastopen = 3   1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62  # 第一次，server返回cookie unknown-34 0x38af51c10bf41ca4 00:36:36.667932 IP localhost.52220 \u0026gt; localhost.6666: Flags [S], seq 3662514892, win 43690, options [mss 65495,sackOK,TS val 122515 ecr 0,nop,wscale 7,unknown-34,nop,nop], length 0 0x0000: 4500 0040 545f 4000 4006 e856 7f00 0001 E..@T_@.@..V.... 0x0010: 7f00 0001 cbfc 1a0a da4d 8acc 0000 0000 .........M...... 0x0020: b002 aaaa fe34 0000 0204 ffd7 0402 080a .....4.......... 0x0030: 0001 de93 0000 0000 0103 0307 2202 0101 ............\u0026#34;... 00:36:36.667990 IP localhost.6666 \u0026gt; localhost.52220: Flags [S.], seq 3186866007, ack 3662514893, win 43690, options [mss 65495,sackOK,TS val 122515 ecr 122515,nop,wscale 7,unknown-34 0x38af51c10bf41ca4,nop,nop], length 0 0x0000: 4500 0048 0000 4000 4006 3cae 7f00 0001 E..H..@.@.\u0026lt;..... 0x0010: 7f00 0001 1a0a cbfc bdf3 b757 da4d 8acd ...........W.M.. 0x0020: d012 aaaa fe3c 0000 0204 ffd7 0402 080a .....\u0026lt;.......... 0x0030: 0001 de93 0001 de93 0103 0307 220a 38af ............\u0026#34;.8. 0x0040: 51c1 0bf4 1ca4 0101 Q....... 00:36:36.668050 IP localhost.52220 \u0026gt; localhost.6666: Flags [P.], seq 1:21, ack 1, win 342, options [nop,nop,TS val 122515 ecr 122515], length 20 0x0000: 4500 0048 5460 4000 4006 e84d 7f00 0001 E..HT`@.@..M.... 0x0010: 7f00 0001 cbfc 1a0a da4d 8acd bdf3 b758 .........M.....X 0x0020: 8018 0156 fe3c 0000 0101 080a 0001 de93 ...V.\u0026lt;.......... 0x0030: 0001 de93 4865 6c6c 6f2c 2074 6370 2066 ....Hello,.tcp.f 0x0040: 6173 7420 6f70 656e ast.open 00:36:36.668109 IP localhost.6666 \u0026gt; localhost.52220: Flags [.], ack 21, win 342, options [nop,nop,TS val 122515 ecr 122515], length 0 0x0000: 4500 0034 69cb 4000 4006 d2f6 7f00 0001 E..4i.@.@....... 0x0010: 7f00 0001 1a0a cbfc bdf3 b758 da4d 8ae1 ...........X.M.. 0x0020: 8010 0156 fe28 0000 0101 080a 0001 de93 ...V.(.......... 0x0030: 0001 de93 .... 00:36:36.707264 IP localhost.6666 \u0026gt; localhost.52220: Flags [.], ack 22, win 342, options [nop,nop,TS val 122525 ecr 122515], length 0 0x0000: 4500 0034 69cc 4000 4006 d2f5 7f00 0001 E..4i.@.@....... 0x0010: 7f00 0001 1a0a cbfc bdf3 b758 da4d 8ae2 ...........X.M.. 0x0020: 8010 0156 fe28 0000 0101 080a 0001 de9d ...V.(.......... 0x0030: 0001 de93 .... # 第二次，client发送请求时，将cookie写在syn包中，同时带上发送的数据；server端校验后(kernel和tcp/ip协议栈做校验)后返回成功，如此在3次握手中节省了一次rtt时间 00:36:38.744954 IP localhost.52226 \u0026gt; localhost.6666: Flags [S], seq 1820632025:1820632045, win 43690, options [mss 65495,sackOK,TS val 123034 ecr 0,nop,wscale 7,unknown-34 0x38af51c10bf41ca4,nop,nop], length 20 0x0000: 4500 005c 4343 4000 4006 f956 7f00 0001 E..\\CC@.@..V.... 0x0010: 7f00 0001 cc02 1a0a 6c84 a3d9 0000 0000 ........l....... 0x0020: d002 aaaa fe50 0000 0204 ffd7 0402 080a .....P.......... 0x0030: 0001 e09a 0000 0000 0103 0307 220a 38af ............\u0026#34;.8. 0x0040: 51c1 0bf4 1ca4 0101 4865 6c6c 6f2c 2074 Q.......Hello,.t 0x0050: 6370 2066 6173 7420 6f70 656e cp.fast.open 00:36:38.745022 IP localhost.6666 \u0026gt; localhost.52226: Flags [S.], seq 3848342665, ack 1820632046, win 43690, options [mss 65495,sackOK,TS val 123034 ecr 123034,nop,wscale 7], length 0 0x0000: 4500 003c 0000 4000 4006 3cba 7f00 0001 E..\u0026lt;..@.@.\u0026lt;..... 0x0010: 7f00 0001 1a0a cc02 e561 0c89 6c84 a3ee .........a..l... 0x0020: a012 aaaa fe30 0000 0204 ffd7 0402 080a .....0.......... 0x0030: 0001 e09a 0001 e09a 0103 0307 ............ 00:36:38.745072 IP localhost.52226 \u0026gt; localhost.6666: Flags [.], ack 1, win 342, options [nop,nop,TS val 123034 ecr 123034], length 0 0x0000: 4500 0034 4344 4000 4006 f97d 7f00 0001 E..4CD@.@..}.... 0x0010: 7f00 0001 cc02 1a0a 6c84 a3ee e561 0c8a ........l....a.. 0x0020: 8010 0156 fe28 0000 0101 080a 0001 e09a ...V.(.......... 0x0030: 0001 e09a .... 00:36:38.745127 IP localhost.52226 \u0026gt; localhost.6666: Flags [F.], seq 1, ack 1, win 342, options [nop,nop,TS val 123034 ecr 123034], length 0 0x0000: 4500 0034 4345 4000 4006 f97c 7f00 0001 E..4CE@.@..|.... 0x0010: 7f00 0001 cc02 1a0a 6c84 a3ee e561 0c8a ........l....a.. 0x0020: 8011 0156 fe28 0000 0101 080a 0001 e09a ...V.(.......... 0x0030: 0001 e09a .... 00:36:38.747232 IP localhost.6666 \u0026gt; localhost.52226: Flags [.], ack 2, win 342, options [nop,nop,TS val 123035 ecr 123034], length 0 0x0000: 4500 0034 ec10 4000 4006 50b1 7f00 0001 E..4..@.@.P..... 0x0010: 7f00 0001 1a0a cc02 e561 0c8a 6c84 a3ef .........a..l... 0x0020: 8010 0156 fe28 0000 0101 080a 0001 e09b ...V.(.......... 0x0030: 0001 e09a ....    上述通信过程中  第一次，server返回cookie unknown-34 0x38af51c10bf41ca4 第二次，client发送请求时，将cookie写在syn包中，同时带上发送的数据；server端校验后(kernel和tcp/ip协议栈做校验)后返回成功，如此在3次握手中节省了一次rtt时间   也就是说，在net.ipv4.tcp_fastopen设置为3时，tcp fastopen特性使能  关于如何使能TFO，在前文中的TFO的配置说明中，我们可以看到，\n1 2 3 4 5 6 7 8 9 10  The values (bitmap) are 1: Enables sending data in the opening SYN on the client w/ MSG_FASTOPEN. 使能client端的TFO特性 2: Enables TCP Fast Open on the server side, i.e., allowing data in a SYN packet to be accepted and passed to the application before 3-way hand shake finishes. 使能server端的TFO特性 4: Send data in the opening SYN regardless of cookie availability and without a cookie option.   并且这个标志是位操作，如果我在本机做实验，将本机作为sever端和client端的话，需要两个位都使能，所以应该将该值设置为3.\n同时我们可以看到，tcp fast open是非常向后兼容的，升级成本不高，需要高于3.7+版本内核，但总体来说值得采用。\nnginx 1.5.18（2013年）开始支持tcp fast open\nTODO LIST  TFO在移动端场景中的性能体现：android+nginx tcp fast open 在内核中的实现  参考链接  TFO\u0026mdash;google tcp fast open protocol wikipedia TFO简介 tfo的golang实现(github) 上一行项目的作者bradley falzon google关于tfo的论文  ","date":"2016-05-09T15:53:34+08:00","permalink":"https://bg2bkk.github.io/p/tcp_fast_open%E7%9A%84%E6%A6%82%E5%BF%B5-%E4%BD%9C%E7%94%A8%E4%BB%A5%E5%8F%8A%E5%AE%9E%E7%8E%B0/","title":"tcp_fast_open的概念 作用以及实现"},{"content":"C语言中的宏 在看代码的过程中我看到了 ## 和 # 两个符号，前一个我知道，后一个就不清楚了。在文档中我才知道C语言有这么多的宏，##表示连接符concat，而#EXP表示将代码中的表达式EXP转为字符串，这样写在错误日志中我们就知道是那句代码导致错误的了。\nC语言中的宏以及用法还有很多，上述文档总结的非常好，除了##和#外，还有\u0026hellip;、宏展开时多次求值问题都有讲解，值得一读一试。\n这篇文章也不错，他们都源自于gnu的官方文档\nOmniGraffle  OmniGraffle简书 UX基础 - OmniGraffle新手指南  markdown * [markdown边角料](http://blog.csdn.net/phunxm/article/details/49565427)  kernel  kernel工程导论  内存管理  http://blog.jobbole.com/103993/ http://blog.jobbole.com/88673/  前端开发   如何使textarea的大小随着其内容增加而变化呢？\n  解决方法\n 1、http://audi.tw/Blog/Javascript/javascript.textarea.autogrow.asp 2、http://www.cnblogs.com/xmmcn/archive/2012/12/18/2822968.html 3、jquery 插件: https://bobscript.com/archives/419/    awk  http://blog.sina.com.cn/s/blog_4a033b090100xo2b.html http://blog.csdn.net/junjieguo/article/details/7525794  postgre  修改表结构，postgre_wiki 插入数据、postgre_wiki 清楚pg_xlog  python  python魔术方法  指南 入门    [http协议] http协议301和302的区别\n http://www.cnblogs.com/caiyuanzai/archive/2012/04/24/2469013.html http://blog.csdn.net/qmhball/article/details/7838989  linux socket编程样例   http://alas.matf.bg.ac.rs/manuals/lspe/snode=106.html\n  进程间传递fd\n  linux apue编程  epoll是同步非阻塞的  epoll、select等多路服用IO，将fd加入等待时间的队列中，每隔一段时间去轮询一次，因此是同步的；优点是能够在等待任务的时间里去做别的任务；缺点是任务完成的响应延迟增大，因为每隔一段时间去轮询他们，在时间间隔内任务可能已经完成而等待处理等待了一段时间了。\n参考链接\n同步/异步指的是被调用方的通知方式，被调用方完成后，主动通知调用方，还是等待调用方发现。前者是异步，后者是同步。从这里也可以看出，异步IO通知调用方时，数据已经就绪，对于网络IO来说，异步IO已经将数据从内核复制到用户空间了。\n阻塞/非阻塞是调用方的等待方式，是一直等待在做的事件完成，还是去做别的事情，等到在做的事件完成后再接着进行处理。前者是阻塞，后者是非阻塞\n因此epoll是同步和非阻塞的。\n性能分析  有用的systemtap脚本  分布式存储  知识体系  B-Tree    分布式ID生成  ID  方案一： 常规数据库的auto increment服务  改进：可以将ID划均分给若干数据库，每个数据库自增的起点不一样，可以保证各库生成ID不同  缺点是非强一致性     方案二： 单点批量生成；ID生成服务每次从数据库预定一定容量的ID，然后派发；可以成倍降低数据库压力；  缺点：单点服务、可能造成空洞； 改进：找备胎，一旦主ID服务挂掉，备胎立刻备上；通过vip+keepalived实现   方案三：uuid 方案四：当前毫秒数；缺点是每个毫秒容量有限，也可能重复 方案五：将64bit数字作为ID，分别包含字段：毫秒数、业务线、机房、机器、以及毫秒内序列号；根据业务来规划容量；毫秒数可以保证ID是趋势自增的   ID  分布式锁  http://www.cnblogs.com/zhengyun_ustc/archive/2012/11/17/topic2.html http://www.jeffkit.info/2011/07/1000/  一致性哈希 consistent hashing  http://www.codeproject.com/Articles/56138/Consistent-hashing http://blog.huanghao.me/?p=14 http://blog.csdn.net/sparkliang/article/details/5279393  2PC、3PC和Paxos算法  coolshell  并发编程   volatile\n  并发编程\n  聊聊并发\n  nginx配置文件   rewrite:\n http://www.xiehaichao.com/articles/428.html http://seanlook.com/2015/05/17/nginx-location-rewrite/    nginx配置使用用户自定义错误页面\n  Lua的学习、使用和源码精读 lua的元表   http://lua-users.org/wiki/MetamethodsTutorial\n 元表用来扩展lua对象的功能，元表的含义在于生来就有，lua对象本身会带有这个表，所以称为元表 metatable也是普通的lua table，包含一系列元方法metamethods，每个元方法有对应的events触发；比如算术运算符、__index 等操作 __index    http://lua-users.org/wiki/MetatableEvents\n  控制类型继承；当访问myTable[key]，而table中没有key域时，如果元表有__index项：\n 如果__index是函数，则调用该函数 如果__index是table，返回这个table中key域的值；如果__index是table并且该table没有key域，但是该table有__index，则继续查找(calls fallback function or fallback table) 使用rawget(myTable, key)可以跳过metatable    __index是一个应用广泛并用处很大的元方法metamethod\n 如果想获取table中的元素key，而key在table中没有找到，该方法可以定义为函数或者一个table，来寻找key。  如果__index是函数，该函数的第一个参数是没有找到key的这个table，第二个参数是key； 如果__index是table，那么将在该table中寻找key，如果没有找到，可以继续从这个table的__index寻找，因此你可以通过__index进行一整个链条的查找。      __metatable\n 用于隐藏metatable，当调用getmetatable(myTable)时，如果该域不为空，则返回这个域的值，而不是metatable    sample code\n    Lua面向对象  http://dabing1022.github.io/2014/03/18/multiple-inheritance-understand-lua/  Lua源码精读  Lua的全局和状态，以及初始化  Lua的全局和状态  在调用lua_newstate 初始化Lua虚拟机时，会创建一个全局状态和一个线程（或称为调用栈），这个全局状态在整个虚拟机中是唯一的，供其他线程共享。一个Lua虚拟机中可以包括多个线程，这些线程共享一个全局状态，线程之间也可以调用lua_xmove函数来交换数据。   LuaVM 初始化 lua_State    Lua与C的交互 * [深入理解Lua与C用于数据交互的栈](http://blog.csdn.net/maximuszhou/article/details/21331819) * 为啥要通过 栈 来通信呢？ * Lua是动态类型语言，在Lua语言中没有类型定义的语法，每个值都携带了它自身的类型信息，而C语言是静态类型语言 * Lua使用垃圾收集，可以自动管理内存，而C语言要求程序自己释放分配的内存，需应用程序自身管理内存 * 压栈的影响 * C将值压入栈中后，Lua将会生成相应类型的结构，存储和管理这个值 * Lua不会持有指向VM外部的指针，指向的都是自己的结构和栈上的结构 * 比如压入字符串，Lua生成Lua_TTSTRING类型的对象，C可以随意释放这个字符串   http://if-yu.info/lua-notes.html  list和nil * https://techsingular.org/2012/12/22/programming-in-lua%EF%BC%88%E5%9B%9B%EF%BC%89%EF%BC%8D-nil-%E5%92%8C-list/ * nil不但不是无的意思，反而在list中起到占位和有的意思。  lua的first class  可以被赋值给变量； 可以作为参数； 可以作为返回值； 可以作为数据结构的构成部分。( 注意 nil 并不完全符合这个要求，但是可以通过某个 field 的缺失来表示 nil。)  Lua的GC * lua的[gc](https://techsingular.org/2013/10/27/lua-%E7%9A%84%E5%9E%83%E5%9C%BE%E5%9B%9E%E6%94%B6/)  lua的table * https://facepunch.com/showthread.php?t=1306348   I don\u0026rsquo;t believe this is correct. It was my understanding that Lua tables are implemented as a two-part data structure whereby dense non-negative integer keys are stored as a simple array and are therefore indexed by a simple pointer addition and dereference, which is O(1). All other keys (non-integer, negative and sparse integer) are stored in a hashmap as a chained scatter table which stores key-values pairs as a flat array indexed by the hash of the key. Collisions are resolved by storing pointers to the next element with the same hash alongside this (essentially a linked list of colliding elements). At worst case, where all elements collide, the complexity of this implementation is O(n), however it is expected that on average the number of collisions per element is 1 and the maximum is 2; this means that the average complexity is O(1).\n  The implementation of Lua tables is such that, even with a huge number of elements, lookup is as quick as possible.\n lua函数使用   setmetatable\n https://www.lua.org/manual/5.2/manual.html setmetatable(table, metatable)  将metatable设置为table的元表，（在Lua中只能设置table的元表，其他类型的对象不行，除非使用C）。如果metatable为nil，则参数cable的元表被清除；如果该table的__metatable不为空，则抛出异常   该函数返回table    collectgarbage(\u0026ldquo;count\u0026rdquo;)\n 垃圾回收函数    lua yield 和 resume\n http://www.lua.org/manual/5.2/manual.html#pdf-coroutine.resume    1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20  -- http://my.oschina.net/wangxuanyihaha/blog/186401 function foo(a) print(\u0026#34;foo\u0026#34;, a) return coroutine.yield(2 * a) end co = coroutine.create(function ( a, b ) print(\u0026#34;co-body\u0026#34;, a, b) local r = foo(a + 1) print(\u0026#34;co-body\u0026#34;, r) local r, s = coroutine.yield(a + b, a - b) print(\u0026#34;co-body\u0026#34;, r, s) return b, \u0026#34;end\u0026#34; end) print(\u0026#34;main\u0026#34;, coroutine.resume(co, 1, 10)) print(\u0026#34;main\u0026#34;, coroutine.resume(co, \u0026#34;m\u0026#34;))\t-- resume的参数 \u0026#39;m\u0026#39; 是在调用yield传入的，所以本次是在第5行 return m print(\u0026#34;main\u0026#34;, coroutine.resume(co, \u0026#34;x\u0026#34;, \u0026#34;y\u0026#34;)) print(\u0026#34;main\u0026#34;, coroutine.resume(co, \u0026#34;x\u0026#34;, \u0026#34;y\u0026#34;))   1 2 3 4 5 6 7 8  co-body\t1\t10 foo\t2 main\ttrue\t4 co-body\tm main\ttrue\t11\t-9 co-body\tx\ty main\ttrue\t10\tend main\tfalse\tcannot resume dead coroutine   lua编码的陷阱 * [字符串拼接导致垃圾产生](http://tech.uc.cn/?p=1131)  golang golang的并发、协程和channel  golang并发编程初探  1 2 3 4 5 6  for i := 0; i \u0026lt; 10; i++ { go func() { arr[i] = i + i*i chs[i] \u0026lt;- arr[i] }() }   类似于如上形式的for循环中启动go协程，在for循环结束时协程才会开执行，所以每个协程用到的i都是最大值10\n可以将i加入到协程的参数中，如下：\n1 2 3 4 5 6 7 8  for i := 0; i \u0026lt; 10; i++ { chs[i] = make(chan int) go func(i) { arr[i] = i + i*i chs[i] \u0026lt;- arr[i] }(i) }   可以用局部变量保存i值\n1 2 3 4 5 6 7 8  for i := 0; i \u0026lt; 10; i++ { i := i chs[i] = make(chan int) go func() { arr[i] = i + i*i chs[i] \u0026lt;- arr[i] }() }     golang的底层数据结构\n  golang多协程channel同步\n sync.WaitGroup defer recover    golang闭包与协程使用\n  golang设计模式 * [单例模式](http://marcio.io/2015/07/singleton-pattern-in-go/) * [golang map reduce](https://gist.github.com/mcastilho)  golang接口 * https://github.com/astaxie/build-web-application-with-golang/blob/master/zh/02.6.md * http://xiaorui.cc/2016/03/11/%E5%85%B3%E4%BA%8Egolang-struct-interface%E7%9A%84%E7%90%86%E8%A7%A3%E4%BD%BF%E7%94%A8/ * http://blog.csdn.net/zhangzhebjut/article/details/24974315    UML图\n 实现关系、泛化关系、关联关系、聚合关系 http://design-patterns.readthedocs.io/zh_CN/latest/read_uml.html    设计模式\n http://tengj.top/2016/04/04/sjms3abstractfactory/    ","date":"2016-05-06T11:39:48+08:00","permalink":"https://bg2bkk.github.io/p/effective-tips-in-daily-learning/","title":"effective tips in daily learning"},{"content":" DNS消息格式 EDNS详解 rfc6891 edns_client_subnet draft 小米的ends实践  1  6. respond时也需要增加一个Additional RRs区域，直接把请求的Additional内容发过去就可以(如果支持source netmask，将请求中的source netmask复制到scope netmask中，OpenDNS要求必须支持scope netmask)   * 意思是非OpenDNS就可以不支持scope netmask吗？目前新浪的仍然不支持    miekg/dns: golang lib\n  dig with edns\n https://www.gsic.uva.es/~jnisigl/dig-edns-client-subnet.html http://xmodulo.com/geographic-location-ip-address-command-line.html 在线查询：curl ipinfo.io/23.66.166.151    DNS报文格式\n IP packet  IP Header 20 bytes IP Data: UDP  UDP Header 8 bytes UDP Data: DNS  DNS Header 12 bytes DNS Data: RR  RR: Question[] RR: Answer[] RR: Authority[] RR: Additional[] RR  QName QType QClass RDLENGTH RDATA   RR OPT  QName null QType OPT=41 QClass = UDP payload 2bytes TTL = Extended-RCODE 1byte: extend + VERSION 1byte: 0 + Z 2bytes: 0 RDLen\tlen of data(OPT) OPT  Option-Code 2bytes: EDNS0_SUBNET Option-Length 2bytes Option-Data  Family 1byte: IPV4(1) Source Netmask 1byte: 32 Scope Netmask 1byte: 0 Client Subnet 4bytes: 65.135.152.203                  我们总是在追一些时髦的技术，而不顾基础还不牢靠\n  我们总是看见新的框架，然而框架本质上仍然是那些东西，mvc，cs\n  ","date":"2016-04-13T11:37:24+08:00","permalink":"https://bg2bkk.github.io/p/dns-with-golang/","title":"DNS with golang"},{"content":"  What Your Computer Does While You Wait 关于现代CPU和OS有详解\n  读薄csapp\n  深入探索并发编程\n  刘浩mit 6.824 代码\n  packagecloud: guide to linux system call ltrace strace\n  linux 网络栈 IO栈\n  一位很棒的小朋友\n  赖明星数据库\n  netfilter\n  项仲的博客，cgroup和linux调度器等\n  intel 什么是代码现代化\n  https://github.com/martinezjavier/ldd3\n ldd3在kernel-2.6.32、kernel-2.6.35和kernel-2.6.37调试通过，可供学习。 我使用ubuntu-10.04，kernle-2.6.32测试    https://github.com/duxing2007/ldd3-examples-3.x\n ldd3在kernel-4.2.0-27、ubuntu-14.04测试，调试通过，可供学习    https://github.com/cmus/cmus\n 命令行式的音乐播放器，依赖的解码库较多    wangle\n facebook的RPC框架 所依赖的一些异步库，比如folly github主页看起来非常清爽，代码质量十分之高，而且不是那种为了kpi而造轮子的，每个项目都有用处，高层项目构建于基础项目 facebook主页可以多看看    https://github.com/cyfdecyf/spinlock\n 自旋锁专业研究    vdsotest\n vdso研究学习    google\n googletest gperftool    c gui lib\n https://github.com/vurtun/nuklear https://github.com/andlabs/libui    redis-lua bloom filter\n https://github.com/erikdubbelboer/redis-lua-scaling-bloom-filter redis实现的布隆过滤器    ","date":"2016-04-12T10:19:38+08:00","permalink":"https://bg2bkk.github.io/p/github-%E4%B8%8A%E9%82%A3%E4%BA%9B%E4%BB%A4%E6%88%91%E6%84%9F%E5%88%B0%E6%83%8A%E8%89%B3%E5%8F%88%E5%AE%9E%E7%94%A8%E7%9A%84%E9%A1%B9%E7%9B%AE/","title":"github 上那些令我感到惊艳又实用的项目"},{"content":" 1、想用nginx-gdb-utils来监控ngx_lua的内存使用情况 2、在CentOS 6.5上，gdb为7.2，python为2.6，没有一个符合的，想强上，没上了，只能在7上搞 3、CentSO 7的gdb是7.6，版本也很老，对于nginx-gdb-utils来说。python倒是2.7，可以搞 4、gdb需要编译安装，首先下载gdb-7.11  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23  cd gdb-7.11 ./configure --with-python=python2.7 报错，报python2.7找不到，换成 ./configure --with-python=/usr/bin/python2.7 依然报错 很奇怪，难道不是要python2.7吗，怎么报找不到。 后来才知道，需要python，要的不是python2.7的可执行文件，而是python的库文件等待 sudo yum install python2.7-devel make -j24 --with-python 注意我这里不用写--with-python=blahblah了 终于不报错了 make -C gdb install 报错，报没有makeinfo的错误，经查，makeinfo是texinfo的一部分，用来生成说明文档的，因为它而不能安装，蛋疼 sudo yum install texinfo make -C gdb install 安装在/usr/local/bin/gdb    5、将nginx-gdb-utils写入gdb初始化文件中，这样以后就不用每次加载py文件了  1 2 3 4 5 6 7 8 9 10 11 12  vim ~/.gdbinit directory /path/to/nginx-gdb-utils py import sys py sys.path.append(\u0026#34;/path/to/nginx-gdb-utils\u0026#34;) source luajit20.gdb source ngx-lua.gdb source luajit21.py source ngx-raw-req.py set python print-stack full   但其实一般而言我们都是用root用户的，所以在sudo或者直接是root用户下时，需要重新写~/.gdbinit，这时应该是在/root/.gdbinit了\n  6、/usr/local/bin/gdb -p 12345\n  7、lgcstat\n  发现报一些函数或者变量找不到，比如Lgref找不到，这个原因是相关软件没有把用 -g 选项把符号编译进去\n 8、对于LuaJit而言，make CCDEBUG=-g -B -j8 9、对于lua-cjson而言，make CCDEBUG=-g -B -j8 10、对于tengine、nginx或者openresty而言，CFLAGS=\u0026quot;-g -O2\u0026quot; ./configure  11、自此就可以愉快的玩耍了。这些工具还是很有意思的。  ","date":"2016-03-25T20:16:51+08:00","permalink":"https://bg2bkk.github.io/p/nginx-gdb-utils%E7%9A%84%E7%BC%96%E8%AF%91-%E5%AE%89%E8%A3%85%E5%92%8C%E4%BD%BF%E7%94%A8/","title":"nginx gdb utils的编译 安装和使用"},{"content":"EPOLL在linux 内核中的新发展 Epoll是linux专有的系统调用，用于快速地高效轮询大规模文件描述符fd。这个API在kernel-2.5版本时就已经合并，并使用至今。即使如此，epoll和其他接口一样，仍然有提升空间。现在有两个patch为epoll系列系统调用添加了新的功能。\nepoll概述 epoll的功能与select或者poll类似，但是epoll在应对轮询处理大规模文件描述符时拥有更灵活的选项和更高的性能。每次调用select和poll，都会将被轮询的fd集合复制，生成新的fd集合，所以内核需要检查每一个描述符是否合法，是否IO就绪，然后将执行监听的进程添加到相应的唤醒等待队列。但实际上，一般情况下，在两次select或者poll调用之间，有事件产生的fd并不多，所以对每个fd都进行前述流程实际上有很多不必要的重复性操作。Epoll将设置被监听fd和轮询fd是否就绪这两个任务分开，从而解决这一问题。\n使用epoll的话，必须首先新建epoll fd用于轮询，新建epfd通过如下调用：\n1 2 3 4  #include \u0026lt;sys/poll.h\u0026gt; int epoll_create(int size); int epoll_create1(int flags);   两者都返回epoll fd，而epoll_create()的size参数已经不再有意义，epoll_create1()的flag参数可以设置epfd的CLOSE_ON_EXEC标志。\n第二步是添加所有被监听的fd，通过调用：\n1  int epoll_ctl(int epfd, int op, int fd, struct epoll_event *event);   参数op是EPOLL_CTL_ADD时，fd将被添加进epfd轮询的fd集合中，event参数用于指定哪个类型的事件被轮询，读事件、写事件或者其他事件，详情参考man page。\n最后，等待集合中fd是否就绪的工作由以下函数实现：\n1 2 3  int epoll_wait(int epfd, struct epoll_event *events, int maxevents, int timeout); int epoll_pwait(int epfd, struct epoll_event *events, int maxevents, int timeout, const sigset_t *sigmask);   有事件发生时，epoll_wait将返回，产生的时间存在参数events中，最多maxevents个事件。如果timeout时间内没有事件发生，epoll_wait也将返回，timeout的单位是ms。epoll_pwait可以使用信号集sigmask来屏蔽特定信号，可以使应用程序安全的等待fd就绪或者捕获信号。二者的关系和select与pselect关系一样。\npatch 1：epoll_ctl_batch() 和 epoll_pwait1() Fam Zheng为epoll引入了两个新的系统调用。\nFam的第一个系统调用是epoll_ctl_batch，用来解决一个性能问题：每次调用epoll_ctl，都只能添加、修改和删除一个fd，如果有大量fd需要修改，那么需要调用相应次数的epoll_ctl来实现，这会导致大量系统调用发生，而这个场景却是经常发生的。Fam引入的epoll_ctl_batch()通过在一个系统调用中添加多个fd来解决这个问题：\n1  int epoll_ctl_batch(int epfd, int flags, int ncmds, struct epoll_ctl_cmd *cmds);   结构体epoll_ctl_cmd用于描述一个待添加的事件，可以看作是epoll_ctl参数的一次打包：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15  struct epoll_ctl_cmd { /* Reserved flags for future extension, must be 0. */ int flags; /* The same as epoll_ctl() op parameter. */ int op; /* The same as epoll_ctl() fd parameter. */ int fd; /* The same as the \u0026#34;events\u0026#34; field in struct epoll_event. */ uint32_t events; /* The same as the \u0026#34;data\u0026#34; field in struct epoll_event. */ uint64_t data; /* Output field, will be set to the return code after this * command is executed by kernel */ int result; };   将一个epoll_ctl_cmd数组cmds传入，则epoll_ctl_batch可以在一次系统调用中添加多个fd。\nFam的第二个系统调用是epoll_pwait1\n1 2 3 4 5 6 7 8 9  struct epoll_wait_params{ int clockid; struct timespec timeout; sigset_t *sigmask; size_t sigsetsize; } int epoll_pwait(int epfd, int flags, struct epoll_event *events, int maxevents, struct epoll_wait_params *params);   本版本的epoll_pwait1()添加了一个flags参数，但是并未定义任何flag值，所以flags置为0即可。其他参数，包括时间控制、信号屏蔽设置，都写在params参数中，目的是为应用程序提供更精细的时间控制。对于epoll_wait()来说，毫秒级的时钟分辨率已经被证明在一些场景中过于粗糙，新的系统供调用提供了纳秒级别的精度，解决了这个问题。\npatch2: 多线程环境下更好的性能，解决“惊群”问题 Jason Baron（Akamai公司）主要解决一个相对来说不那么常见的场景下，epoll现有的一个问题。通常情况下，一个给定的fd只被一个进程轮询，但是在Jason的场景中，会有多个进程轮询同一个fd集合。在这个场景设定下，一个fd有事件产生时将会唤醒所有监听进程，即使最后只有一个进程能够得到处理该事件的机会，这就是所谓的“惊群”问题。\nJason的解决方案是通过epoll_ctl向被轮询的fd再添加两个新的flag，第一个是EPOLLEXCLUSIVE，保证只有一个进程能被唤醒然后处理事件。该flag使得，有事件发生时，简单的用add_wait_queue_exclusive()代替add_wait_queue()，互斥的将进程放入等待队列中。很明显，所有轮询同一个fd的进程都要使用互斥模式来实现只有一个进程唤醒的效果。\n不过，这个变化没有完全解决问题，因为这会导致当有事件发生时，唤醒的都是同一个进程。由于Epoll存在的一个原因是，在两次epoll_wait()调用之间,，进程能留在epfd的等待唤醒队列中，处于等待队列头部的进程仍然在队列头部，所以这个进程将被唤醒并处理所有互斥模式的fd（这句翻译我有疑问）。但是我们的目的是，多个进程轮询同一fd集合时，能够散开执行，而每次都唤醒的是同一个进程与此相悖。为解决这个问题，Jason添加了另一个flag，叫做 EPOLLROUNDROBIN，使得内核按顺序处理唤醒每个进程。\n引入一个新的等待队列函数用来支持实现这种方式\n1  void add_wait_queue_rr(wait_queue_head_t *q, wait_queue_t *wait);   使用该函数后，当wait返回时，只有一个进程被唤醒，效果和add_wait_queue_exclusive()一样。但是，这个被唤醒的进程，将被从队列头移到队列尾，直到它前面的所有进程都得到唤醒机会后，才能再次被唤醒。\nJason的提交patch的同时也提交了一个用于压测的程序，压测结果显示，互斥模式使得执行时间降低了50%，当有大量的唤醒发生时，“惊群”效应带来的性能损耗就不会发生了。\n结语 以上提到的两个patch已经被多次review和comments，Fam的patch自从1月份提出后进行了多次修改。现在的编辑们对API相关的patch投入了越来越多的关注和审视，这是对的，因为API将会长期有效，(API lives forever),甚至是永远有效。所以最好在向用户推出之前就搞定所有bug，以提供永久支持的态度提交。这些patch目前看来已经接近就绪，可能将会在下一个窗口中合并。\n","date":"2016-03-22T01:48:41+08:00","permalink":"https://bg2bkk.github.io/p/new-evolvement-of-epoll/","title":"New evolvement of Epoll"},{"content":"openresty中如何写redis或者mysql的wraper  参考资料  ngx_lua获取post字段参数 在用户请求为POST方式时，如果想获取post中的各参数字段，比如post数据为 \u0026ldquo;uid=100\u0026amp;ip=10.13.112.53\u0026rdquo;，此时想获取该字段的话，可以调用ngx.req.get_post_args函数。按惯例返回table类型，post数据的各字段为table的key\n1 2 3 4 5  function get_uid() local args = ngx.req.get_post_args() local uid = args[\u0026#39;uid\u0026#39;] return uid end   关于HTTP的POST提交数据的方式，网上有很多讨论\n 四种常见的POST提交数据方式  application/x-www-form-urlencoded multipart/form-data application/json text/xml   HTTP header头的一些字段  ngx_lua同样提供了读写HTTP请求中uri参数，读写HTTP请求中的HEADER头部，这些在ngx_lua开发中为我们提供了丰富的工具，非常好的功能。\n最后回到主题，当我读出uid字段后，有时候会发现报错\u0026quot;requesty body in temp file not supported\u0026quot;，原因在于nginx会将用户请求的body字段缓存起来，如果超出缓存大小，则将用户body数据写到文件中；而ngx.req.get_post_args()是不支持从文件中读取数据的。因此解决办法是：适当加大 nginx 的 client_body_buffer_size 配置, 当 client_body_buffer_size 配置为和 client_max_body_size 一样大时，nginx就不会把请求体缓冲到文件系统了（但也要仔细内存占用）。\n对于client_max_body_size来说，\n1 2 3 4 5  Syntax:\tclient_max_body_size size; Default:\tclient_max_body_size 1m; Context:\thttp, server, location Sets the maximum allowed size of the client request body, specified in the “Content-Length” request header field. If the size in a request exceeds the configured value, the 413 (Request Entity Too Large) error is returned to the client. Please be aware that browsers cannot correctly display this error. Setting size to 0 disables checking of client request body size.   设置允许的client body最大值，对http server来说是种保护。\n对于client_body_buffer_size来说，\n1 2 3 4 5  Syntax:\tclient_body_buffer_size size; Default:\tclient_body_buffer_size 8k|16k; Context:\thttp, server, location Sets buffer size for reading client request body. In case the request body is larger than the buffer, the whole body or only its part is written to a temporary file. By default, buffer size is equal to two memory pages. This is 8K on x86, other 32-bit platforms, and x86-64. It is usually 16K on other 64-bit platforms.   如果client body size大于默认值，则nginx将会把body缓存在文件中。将client body buffer size设置为和client_max_body_size一样大，nginx将不会把它写进文件中。\nngx_lua中判断table为空  lua的table中，有两类kv，一类是以数字为index，比如{\u0026lsquo;abc\u0026rsquo;, \u0026lsquo;efg\u0026rsquo;}中，{k=1, v=abc}, {k=2, v=efg}，另一类以自己kv存储，比如{[\u0026lsquo;abc\u0026rsquo;] = \u0026lsquo;efg\u0026rsquo;}，k为abc的元素，v为efg #table中，#标识符是返回以数字为index的key，从1开始算，连续的key的数量  1 2 3 4 5 6 7  local t = {} t[1] = \u0026#39;a\u0026#39; t[2] = \u0026#39;b\u0026#39; t[20] = \u0026#39;c\u0026#39; print(#t) 2    因此#号不能获得table的真实大小，也不能用于判断table是否为空 table.maxn(tab)，maxn返回table中以数字为key的元素中，数字最大的那个  1 2 3 4 5 6 7  local t = {} t[1] = \u0026#39;a\u0026#39; t[2] = \u0026#39;b\u0026#39; t[20] = \u0026#39;c\u0026#39; print(table.maxn(t)) 20     next就是pairs遍历table时用来取下一个内容的函数，因此next(tab)可以用来判断table是否为空，如果next(tab)返回为nil的话，说明第一个元素不存在，所以该table为空\n  参考链接\n  ngx_lua中的参数获得 ngx_lua中获得req参数有如下几个方法：\n  ngx.var.arg_city 获取city参数\n host:port/uri?city=abc ngx.var.arg_city = abc host:port/uri?city= ngx.var.arg_city = \u0026lsquo;', and its length is 0 host:port/uri?city ngx.var.arg_city = nil，cause it doesn\u0026rsquo;t exist yet    ngx.req.get_headers()[\u0026lsquo;city\u0026rsquo;]，获取http请求头中的city参数\n host:port/uri -H \u0026lsquo;city:abc\u0026rsquo; 结果为abc host:port/uri -H \u0026lsquo;city:\u0026rsquo; 结果为nil      thread\n 虽然init_worker_by_lua阶段不能使用cosocket，不过可以先通过一个timer（定时时间为0让其立即调用）来发出对外的socket io操作，以实现一些初始化的目的。 openresty的两个缓存中，ngx shared dict是跨worker共享的，是一个单纯的kv缓存；预计接下来会有patch能够支持lpush等redis操作；lua-resty-lrucache是每个worker的Lua VM空间内缓存，不能跨worker共享，优点是可以存储所有lua对象，比如table，而不需要序列化和反序列化    worker 启动时，upstream 是空的，即 _M.data={}，所以这个时候是不能提供服务的。所以每次 reload config 都会导致一段时间内服务不可访问。\n 在 init_worker_by_lua 执行 cosocket 相关的 API 是不允许的（后期可能会添加支持），但可以调用标准 SOCKET 完成初始化加载，例如借助 luasocket 完成数据源获取并初始化 _M 。    不清楚 init_worker_by_lua 里是否可以进行文件操作？\n 是可以的，这个确定。    在ngx lua性能分析方面，agentzh提出一系列的工具，主要是nginx-systemtap-toolkit和stapxx两个工程。我们使用的脚本，说明文档\n  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27  引用： 我们有一整套的基于 systemtap 的工具链可以用于在线或者离线的性能分析。 你的 nginx 进程的 CPU 使用率如果很高的话，可以使用 C 级别的 on-CPU 时间火焰图工具对你最忙的 nginx worker 进程进行采样： https://github.com/agentzh/nginx-systemtap-toolkit#sample-bt 如果你的 nginx 进程的 CPU 很低，但请求延时很高，则有两种可能： 1. 你的 nginx 阻塞在了某些阻塞的 IO 操作（比如文件 IO）或者系统的同步锁上，此时你可以使用 C 级别的 off-CPU 时间火焰图工具对某个典型的 nginx worker 进程进行采样： https://github.com/agentzh/nginx-systemtap-toolkit#sample-bt-off-cpu 如果你发现 Lua 代码占用了大部分的 CPU 时间，则可以进一步使用 ngx-lua-exec-time 工具加以确认： https://github.com/agentzh/stapxx#ngx-lua-exec-time 进一步地，你可以使用 Lua 代码级别的 on-CPU 火焰图工具在 Lua 层面上分析 CPU 时间的分布。如果你使用的是 LuaJIT 2.0.x，则可以使用下面这个工具进行采样： https://github.com/agentzh/nginx-systemtap-toolkit#ngx-sample-lua-bt 如果你使用的是 LuaJIT 2.1，则可以使用 lj-lua-stacks 工具进行采样： https://github.com/agentzh/stapxx#lj-lua-stacks 2. 你的 nginx 通过 ngx_lua 的 cosocket 或者 ngx_proxy 这样的 upstream 模块和上游服务进行通信时，上游服务的延时过大。此时你可以分别使用 ngx-lua-tcp-recv-time、ngx-lua-udp-recv-time 以及 ngx-single-req-latency 工具进行分析： https://github.com/agentzh/stapxx#ngx-lua-tcp-recv-time https://github.com/agentzh/stapxx#ngx-lua-udp-recv-time https://github.com/agentzh/stapxx#ngx-single-req-latency - 我们主要使用四个工具来生成火焰图以分析性能，sample-bt、sample-bt-off-cpu、ngx-sample-lua-bt 和 lj-lua-stacks。 - 实际使用中命令：   1 2 3 4  1、sudo ./sample-bt -p 8736 -t 20 -u -a \u0026#39;-DMAXSKIPPED=10000\u0026#39; \u0026gt; a.bt 2、sudo ./sample-bt-off-cpu -p 8736 -t 20 -u \u0026gt; b.bt 3、sudo ./ngx-sample-lua-bt --luajit20 -p 44252 -t 20 \u0026gt; c.bt 4、sudo ./samples/lj-lua-stacks.sxx --skip-badvars -x 44250 -I tapset/ \u0026gt; d.bt   - 通过a.bt生成火焰图  1  ../FlameGraph/stackcollapse-stap.pl a.bt | ../FlameGraph/flamegraph.pl \u0026gt; a.svg   - 通过脚本ngx-lua-conn-pools来追踪ngx lua connection pool的工作情况。\t 1  sudo ./ngx-lua-conn-pools --luajit20 -p 28261   ","date":"2016-03-14T16:36:14+08:00","permalink":"https://bg2bkk.github.io/p/openresty%E5%AD%A6%E4%B9%A0%E8%BF%87%E7%A8%8B%E4%B8%AD%E7%9A%84%E4%B8%80%E4%BA%9Btips/","title":"openresty学习过程中的一些tips"},{"content":"协程是什么？ what is coroutine ?  协程的概念   Coroutines are computer program components that generalize subroutines for nonpreemptive multitasking, by allowing multiple entry points for suspending and resuming execution at certain locations. Coroutines are well-suited for implementing more familiar program components such as cooperative tasks, exceptions, event loop, iterators, infinite lists and pipes.\n  According to Donald Knuth, the term coroutine was coined by Melvin Conway in 1958, after he applied it to construction of an assembly program.[1] The first published explanation of the coroutine appeared later, in 1963.[2]\n 协程是为实现非抢占式多任务而提出的计算机子程序，通过提供多个程序入口使得程序可以在特定地址挂起和恢复执行。协程天生的支持实现常见程序组件，比如协作式任务、异常、时间循环、迭代器、无边界列表和管道等。\n根据祖师爷高纳德节说，协程的概念由Melvin Conway在1958年提出，随后他将协程应用在编写汇编程序上。协程的第一个公开发表的解释出现在1963年。\n可见协程的概念比多线程还早，而且按照Knuth的说法，”子例程是协程的特例“，一次子例程调用就是一次子函数调用，协程是类函数一样的组件，我们可以在单线程中创建N多个协程，只要内存够用。\n* 协程与子例程的[区别](https://en.wikipedia.org/wiki/Coroutine) * 子例程只有一个调用入口起点，子例程退出后，执行结束；子例程只返回一次，在两次调用之间不保存状态； * 协程有多个入口，调用起始点、或者从上一次返回点接着执行；从协程自己的角度来看，他放弃执行时不是退出，而是去调用另一个协程，或者说将CPU主动让给另一个协程；协程保存状态; * 计算机科学中，[yield](https://en.wikipedia.org/wiki/Yield_(multithreading))用于使处理器放弃当前运行的线程thread，并将它放入运行队列的末尾 * 协程coroutine中的yield需要显式调用 * 每个子例程可以转换为一个不带yield的协程  协程有关的四个概念：coroutine、[yield](https://en.wikipedia.org/wiki/Yield_(multithreading)、Continuation、cooperative multitasking。以及其他相关概念：call stack\n Coroutines in C  给你一个直观的认识    1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25  #include \u0026lt;stdio.h\u0026gt;#include \u0026lt;stdlib.h\u0026gt;int function(void) { static int i, state = 0; switch (state) { case 0: goto LABEL0; case 1: goto LABEL1; } LABEL0: /* start of function */ for (i = 0; i \u0026lt; 10; i++) { printf(\u0026#34;\\t\\t\\ti = %d\\n\u0026#34;, i); state = 1; /* so we will come back to LABEL1 */ return i; LABEL1: ; /* resume control straight after the return */ } } int main(){ int j = 0; for ( j = 0; j \u0026lt; 21; j++){ printf(\u0026#34;j = %d, func = %d\\n\u0026#34;, j, function()); } }   执行结果\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31  i = 0 j = 0, func = 0 i = 1 j = 1, func = 1 i = 2 j = 2, func = 2 i = 3 j = 3, func = 3 i = 4 j = 4, func = 4 i = 5 j = 5, func = 5 i = 6 j = 6, func = 6 i = 7 j = 7, func = 7 i = 8 j = 8, func = 8 i = 9 j = 9, func = 9 j = 10, func = 10 j = 11, func = 11 j = 12, func = 12 j = 13, func = 13 j = 14, func = 14 j = 15, func = 15 j = 16, func = 16 j = 17, func = 17 j = 18, func = 18 j = 19, func = 19 j = 20, func = 20     执行结果分析\n 变量i 和 state 都是 static类型的，是文件全局作用域的 首次执行function函数时，state = 0，goto 到 LABEL0，然后进行正常循环和返回 当function调用次数超过十次后，每次进入function函数内部时，由switch分发到LABEL1；执行完循环体后，对i增1，然后进行判断是否 i \u0026lt; 10，发现不满足，退出程序 可能我们会比较纠结function程序在 i \u0026gt; 10后不再执行return i语句，为什么还会返回自增后的结果呢？  从代码的汇编结果来看，每次function返回时，i都在之前赋值给寄存器eax了，而eax存储的是函数的返回值，所以每次function的返回结果是i，即使不执行return语句   return在这里并不是返回的意思，而是yield的意思    仍然有两种更优化的写法: 左耳朵耗子的例子和上例的来源都是天才程序员 imon Tatham对协程做的尝试，以及关于swtich-case写法的duff机器的讨论\n  理解协程实现的基础就是程序中的函数调用导致的栈帧切换，在切换前先保存被切换subroutine的上下文，在切换时用新subroutine替换当前程序的执行状态。以Linux中用于实现协程的一个api：ucontext来说，makecontext函数将context的eip设置为func参数的地址，所以当该context得以执行时，func函数就开始执行了；swapcontext(old, new)将当前程序处于的old状态切换到new状态，然后new状态的func就得以执行。\n  Linux对于协程实现提供了setjmp/longjmp和ucontext两种机制，现有的协程库，比如protothread，甚至是LuaVM中的协程实现，也会基于这两种机制之一来实现。当然，您也可以手动切换cpu的所有寄存器状态，以实现协程，也是可以的，但是是极不推荐的。\n  setjump \u0026amp; longjmp man longjmp 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23  NAME longjmp, siglongjmp - nonlocal jump to a saved stack context SYNOPSIS #include \u0026lt;setjmp.h\u0026gt; void longjmp(jmp_buf env, int val); void siglongjmp(sigjmp_buf env, int val); DESCRIPTION longjmp() and setjmp(3) are useful for dealing with errors and interrupts encountered in a low-level subroutine of a program. longjmp和setjmp在处理程序调用子例程过程中遇到错误或中断时非常有用。 longjmp() restores the environment saved by the last call of setjmp(3) with the corresponding env argument. After longjmp() is completed, program execution continues as if the corresponding call of setjmp(3) had just returned the value val. longjmp() cannot cause 0 to be returned. If longjmp() is invoked with a second argument of 0, 1 will be returned instead. longjmp将恢复最近一次调用setjmp时通过env参数保存的上下文环境。longjmp完成后，原来调用setjmp的地方将会返回，并且setjmp的返回值是longjmp的val参数。longjmp不会返回0。如果longjmp调用时第二个参数是0，那么它将会返回1. siglongjmp() is similar to longjmp() except for the type of its env argument. If, and only if, the sigsetjmp(3) call that set this env used a nonzero savesigs flag, siglongjmp() also restores the signal mask that was saved by sigsetjmp(3). RETURN VALUE These functions never return.   man setjmp 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21  NAME setjmp, sigsetjmp - save stack context for nonlocal goto SYNOPSIS #include \u0026lt;setjmp.h\u0026gt; int setjmp(jmp_buf env); int sigsetjmp(sigjmp_buf env, int savesigs); DESCRIPTION setjmp() and longjmp(3) are useful for dealing with errors and interrupts encountered in a low-level subroutine of a program. setjmp() saves the stack context/environment in env for later use by longjmp(3). The stack context will be invalidated if the function which called setjmp() returns. setjmp() 在参数env中保存栈帧，稍后longjmp调用时，将从setjmp执行； sigsetjmp() is similar to setjmp(). If, and only if, savesigs is nonzero, the process\u0026#39;s current signal mask is saved in env and will be restored if a siglongjmp(3) is later performed with this env. RETURN VALUE setjmp() and sigsetjmp() return 0 if returning directly, and nonzero when returning from longjmp(3) or siglongjmp(3) using the saved context.   示例代码 代码地址 博客评论区\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41  #include \u0026lt;stdio.h\u0026gt; #include \u0026lt;unistd.h\u0026gt;#include \u0026lt;setjmp.h\u0026gt; jmp_buf jmpbuf_th0; jmp_buf jmpbuf_th1; static int cnt1 = 0; static int cnt0 = 0; static void thread_0() { printf(\u0026#34;%s \\n\\n\u0026#34;, __FUNCTION__); sleep(1); longjmp(jmpbuf_th0, cnt0++); } static void thread_1() { printf(\u0026#34;%s \\n\\n\u0026#34;, __FUNCTION__); sleep(1); longjmp(jmpbuf_th1, cnt1++); } int main() { int rc0, rc1 = 0; entry_thread_0: rc0 = setjmp(jmpbuf_th0); printf(\u0026#34;rc0 = %d\\n\u0026#34;, rc0); if (rc0 != 0) thread_1(); entry_thread_1: rc1 = setjmp(jmpbuf_th1); printf(\u0026#34;rc1 = %d\\n\u0026#34;, rc1); thread_0(); return 0; }   执行结果\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28  rc0 = 0 rc1 = 0 thread_0 rc0 = 1 thread_1 rc1 = 1 thread_0 rc0 = 1 thread_1 rc1 = 1 thread_0 rc0 = 2 thread_1 rc1 = 2 thread_0 rc0 = 3 thread_1 rc1 = 3 thread_0     执行过程分析\n setjmp和longjmp都是基于程序空间中额外的jmpbuf setjmp将当前环境存储在jmpbuf中，longjmp到同一个jmpbuf时，setjmp将会再次返回，返回值是longjmp的第二个参数val 如果setjmp不是因为longjmp返回的，返回值为0 不知道为什么执行结果中，自增的cnt0和cnt1在值为1的时候停顿了一次？    setjump \u0026amp; longjmp 进阶实现\n  setjump \u0026amp; longjmp 进阶实现\n  ucontext man getcontext 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29  In a System V-like environment, one has the two types mcontext_t and ucontext_t defined in \u0026lt;ucontext.h\u0026gt; and the four functions getcontext(), setcontext(), makecontext(3), and swapcontext(3) that allow user-level context switching between multiple threads of control within a process. The mcontext_t type is machine-dependent and opaque. The ucontext_t type is a structure that has at least the following fields: typedef struct ucontext { struct ucontext *uc_link; sigset_t uc_sigmask; stack_t uc_stack; mcontext_t uc_mcontext; ... } ucontext_t; with sigset_t and stack_t defined in \u0026lt;signal.h\u0026gt;. Here uc_link points to the context that will be resumed when the current context terminates (in case the current context was created using makecon‐ text(3)), uc_sigmask is the set of signals blocked in this context (see sigprocmask(2)), uc_stack is the stack used by this context (see sigaltstack(2)), and uc_mcontext is the machine-specific rep‐ resentation of the saved context, that includes the calling thread\u0026#39;s machine registers. The function getcontext() initializes the structure pointed at by ucp to the currently active context. The function setcontext() restores the user context pointed at by ucp. A successful call does not return. The context should have been obtained by a call of getcontext(), or makecontext(3), or passed as third argument to a signal handler. If the context was obtained by a call of getcontext(), program execution continues as if this call just returned. If the context was obtained by a call of makecontext(3), program execution continues by a call to the function func specified as the second argument of that call to makecontext(3). When the function func returns, we continue with the uc_link member of the structure ucp specified as the first argument of that call to makecontext(3). When this member is NULL, the thread exits. If the context was obtained by a call to a signal handler, then old standard text says that \u0026#34;program execution continues with the program instruction following the instruction interrupted by the sig‐ nal\u0026#34;. However, this sentence was removed in SUSv2, and the present verdict is \u0026#34;the result is unspecified\u0026#34;.   man makecontext 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20  In a System V-like environment, one has the type ucontext_t defined in \u0026lt;ucontext.h\u0026gt; and the four functions getcontext(3), setcontext(3), makecontext() and swapcontext() that allow user-level context switching between multiple threads of control within a process. 在类System V环境下，结构体ucontext_t和四个函数 getcontext、setcontext、makecontext和swapcontext提供了在一个进程内通过用户级上下文切换的方式实现多线程的方式 For the type and the first two functions, see getcontext(3). The makecontext() function modifies the context pointed to by ucp (which was obtained from a call to getcontext(3)). Before invoking makecontext(), the caller must allocate a new stack for this context and assign its address to ucp-\u0026gt;uc_stack, and define a successor context and assign its address to ucp-\u0026gt;uc_link. makecontext函数将修改ucp指向的 ucontext_t（必须是从getcontext获得的对象），在调用makecontext前，调用者必须为改上下文分配一个新的栈，并将ucp-\u0026gt;uc_stack指向栈的地址，同时将ucp-\u0026gt;uc_link指向下一个context When this context is later activated (using setcontext(3) or swapcontext()) the function func is called, and passed the series of integer (int) arguments that follow argc; the caller must specify the number of these arguments in argc. When this function returns, the successor context is activated. If the successor context pointer is NULL, the thread exits. 当该context稍后被激活(通过setcontext或者swapcontext)，makecontext函数参数中的func将被调用，同时向func传递argc个参数。当func返回时，下一个context将被激活调用。如果下一个context是null的话，该线程结束。 The swapcontext() function saves the current context in the structure pointed to by oucp, and then activates the context pointed to by ucp. swapcontext函数将当前context保存在oucp结构体，同时激活ucp指向的context。   man page 中的例子 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61  #include \u0026lt;ucontext.h\u0026gt; #include \u0026lt;stdio.h\u0026gt; #include \u0026lt;stdlib.h\u0026gt; static ucontext_t uctx_main, uctx_func1, uctx_func2; #define handle_error(msg) \\ do { perror(msg); exit(EXIT_FAILURE); } while (0) static void func1(void) { printf(\u0026#34;func1: started\\n\u0026#34;); printf(\u0026#34;func1: swapcontext(\u0026amp;uctx_func1, \u0026amp;uctx_func2)\\n\u0026#34;); if (swapcontext(\u0026amp;uctx_func1, \u0026amp;uctx_func2) == -1) handle_error(\u0026#34;swapcontext\u0026#34;); printf(\u0026#34;func1: returning\\n\u0026#34;); } static void func2(void) { printf(\u0026#34;func2: started\\n\u0026#34;); printf(\u0026#34;func2: swapcontext(\u0026amp;uctx_func2, \u0026amp;uctx_func1)\\n\u0026#34;); if (swapcontext(\u0026amp;uctx_func2, \u0026amp;uctx_func1) == -1) handle_error(\u0026#34;swapcontext\u0026#34;); printf(\u0026#34;func2: returning\\n\u0026#34;); } int main(int argc, char *argv[]) { char func1_stack[16384]; char func2_stack[16384]; if (getcontext(\u0026amp;uctx_func1) == -1) handle_error(\u0026#34;getcontext\u0026#34;); uctx_func1.uc_stack.ss_sp = func1_stack; uctx_func1.uc_stack.ss_size = sizeof(func1_stack); uctx_func1.uc_link = \u0026amp;uctx_main; makecontext(\u0026amp;uctx_func1, func1, 0); if (getcontext(\u0026amp;uctx_func2) == -1) handle_error(\u0026#34;getcontext\u0026#34;); uctx_func2.uc_stack.ss_sp = func2_stack; uctx_func2.uc_stack.ss_size = sizeof(func2_stack); /* Successor context is f1(), unless argc \u0026gt; 1 */ uctx_func2.uc_link = (argc \u0026gt; 1) ? NULL : \u0026amp;uctx_func1; makecontext(\u0026amp;uctx_func2, func2, 0); printf(\u0026#34;main: swapcontext(\u0026amp;uctx_main, \u0026amp;uctx_func2)\\n\u0026#34;); if (swapcontext(\u0026amp;uctx_main, \u0026amp;uctx_func2) == -1) handle_error(\u0026#34;swapcontext\u0026#34;); printf(\u0026#34;main: exiting\\n\u0026#34;); exit(EXIT_SUCCESS); } // http://stackoverflow.com/questions/20778735/is-the-type-stack-t-no-longer-defined-on-linux   执行结果：\n1 2 3 4 5 6 7 8 9 10 11  $ gcc context.c -o context.o $ ./context.o main: swapcontext(\u0026amp;uctx_main, \u0026amp;uctx_func2) func2: started func2: swapcontext(\u0026amp;uctx_func2, \u0026amp;uctx_func1) func1: started func1: swapcontext(\u0026amp;uctx_func1, \u0026amp;uctx_func2) func2: returning func1: returning main: exiting   通过manpage中提供的ucontext_t源码，结合man page中的解释，我们对此做一分析。\n 执行过程详解  getcontext(\u0026amp;uctx_func2) 用当前上下文初始化ucontext_t结构体对象：uctx_func2 在调用makecontext前，调用者为该上下文分配一个新的栈，并将成员uc_stack指向栈的地址，同时将uc_link指向下一个context。此时argc为0，所以uctx_func2的uc_link指向uctx_func1，也就是说当uctx_func2执行完后，会自动激活uctx_func1执行 makecontext(\u0026amp;uctx_func2, func2, 0) 函数设置当uctx_func2激活时，调用func2函数，参数为0个；当func2返回时，下一个context将被激活，在这里是uctx_func1 swapcontext(\u0026amp;uctx_main, \u0026amp;uctx_func2) 函数将进程当前执行的上下文保存在uctx_main中，并激活uctx_func2；uctx_func2开始执行，首先被执行的是func2函数，  func2: started func2: swapcontext(\u0026amp;uctx_func2, \u0026amp;uctx_func1)   随后func2函数调用 swapcontext(\u0026amp;uctx_func2, \u0026amp;uctx_func1)，将当前上下文环境保存在uctx_func2中，激活uctx_func1；uctx_func1开始执行，首先执行的是func1，打印出：  func1: started func1: swapcontext(\u0026amp;uctx_func1, \u0026amp;uctx_func2)   随后func1函数调用swapcontext(\u0026amp;uctx_func1, \u0026amp;uctx_func2)，将当前上下文环境保存在uctx_func1中，并激活uctx_func2；上一次uctx_func2保存的上下文环境将被回复，然后进程执行返回到上次被swap_context的点，打印出:  func2: returning   func2函数返回后，uctx_func2也就返回了，下一个context将被激活，也就是uctx_func1；打印出：  func1: returning   uctx_func1也执行结束，而它的下一个context是uctx_main，在main函数中最开始调用swap_context(\u0026amp;uctx_main, \u0026amp;uctx_func2)时，当时程序执行的上下文的地址就是这句，那么在main中继续执行，打印出：  main: exiting      1 2 3 4 5 6 7  $ ./context.o x main: swapcontext(\u0026amp;uctx_main, \u0026amp;uctx_func2) func2: started func2: swapcontext(\u0026amp;uctx_func2, \u0026amp;uctx_func1) func1: started func1: swapcontext(\u0026amp;uctx_func1, \u0026amp;uctx_func2) func2: returning    执行过程详解  当argc个数不为0时，uctx_func2的uc_link为NULL，所以如果uctx_func2结束生命周期，那么整个进程将会退出 在第一次swap_context(\u0026amp;uctx_main, \u0026amp;uctx_func2)时开始调用func2，func2中打印出：  func2: started func2: swapcontext(\u0026amp;uctx_func2, \u0026amp;uctx_func1)   随后swap执行uctx_func1，打印：  func1: started func1: swapcontext(\u0026amp;uctx_func1, \u0026amp;uctx_func2)   随后swap返回uctx_func2，打印：  func2: returning   而func2返回时，由于uctx_func2-\u0026gt;uc_link为NULL，所以整个进程退出，并不会返回到之前保存过的uctx_main执行上下文中。    通过以上分析，您有没有对协程有一个直观的认识呢？本质上，协程调度只是将当前执行的上下文保存起来；调度协程的时候就是将两个执行上下文context切换；指定context的下一个context，在本context执行结束后自动激活下一个context，实现协作；本context执行过程中，通过swap_context主动让出CPU，而不是被抢占放弃CPU。\n 可以参考下进一步的实现，加深下印象：  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54  #include \u0026lt;stdio.h\u0026gt;#include \u0026lt;stdbool.h\u0026gt;#include \u0026lt;ucontext.h\u0026gt; static char stack[2][65536]; // a stack for each coroutine static ucontext_t coroutine_state[2]; // container to remember context  // switch current coroutine (0 -\u0026gt; 1 -\u0026gt; 0 -\u0026gt; 1 ...) static inline void yield_to_next(void) { static int current = 0; int prev = current; int next = 1 - current; current = next; swapcontext(\u0026amp;coroutine_state[prev], \u0026amp;coroutine_state[next]); } static void coroutine(int coroutine_number) { int i; for (i = 0; i \u0026lt; 5; i++) { printf(\u0026#34;Coroutine %d counts i=%d (\u0026amp;i=%p)\\n\u0026#34;, coroutine_number, i, \u0026amp;i); yield_to_next(); } } int main() { ucontext_t return_to_main; // set up  int i; for (i = 0; i \u0026lt; 2; i++) { // initialize ucontext_t  getcontext(\u0026amp;coroutine_state[i]); // set up per-context stack  coroutine_state[i].uc_stack.ss_sp = stack[i]; coroutine_state[i].uc_stack.ss_size = sizeof(stack[i]); // when done, resume \u0026#39;return_to_main\u0026#39; context  coroutine_state[i].uc_link = \u0026amp;return_to_main; // let context[i] perform a call to coroutine(i) when swapped to  makecontext(\u0026amp;coroutine_state[i], (void (*)(void))coroutine, 1, i); } printf(\u0026#34;Starting coroutines...\\n\u0026#34;); swapcontext(\u0026amp;return_to_main, \u0026amp;coroutine_state[0]); printf(\u0026#34;Done.\\n\u0026#34;); return 0; }     another demo\n lib    gnu portable threads\n pt    libtask\n libtask的coroutine    云风的实现\n  其他一些应用\n coroutine-libevent  在libevent里通过协程实现同步      Lua的协程   lua不支持那种真正的多线程（共享同一地址空间的抢占式线程），原因是\n ANSI C没有原生的多线程，所以lua不能直接调用实现 最重要的原因是，我们不认为多线程在lua中是个好主意    多线程是提供给底层编程的。多线程的同步机制，比如信号量和监控都是在操作系统上下文实现的，而非应用程序。调试多线程比较麻烦。而且，由于程序临界区的同步和竞争，多线程也会引起性能下降。\n  多线程引起的问题，主要是抢占式线程和共享内存导致的，lua解决这两个问题的方法是：lua coroutine是协作式的，非抢占式的，所以能避免线程切换导致的问题；lua coroutine之间不共享内存。\n  我见过的最lua的lua代码和博客\n  lua的coroutine and stack\n  lua的c runtime stack和lua runtime stack是什么样子的\n  https://www.zhihu.com/question/21483863\n  lua协程调度\n lua内部  当resume的时候，就切换lua_state环境，然后setjmp，紧接着由于pc指向新地址，所以会直接跳转到该位置 当yield时，直接回复环境，然后longjmp到该resume点   lua with C  当在C函数内入yield时，会恢复环境，longjmp到resume点，之后再次resume的时候，会因为环境被破坏，导致resume出错，此时lua会调用k系列函数，让resume继续下去      lua实现调度器\n  consumer-producer\n  lua yield 和 resume\n http://www.lua.org/manual/5.2/manual.html#pdf-coroutine.resume    1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20  -- http://my.oschina.net/wangxuanyihaha/blog/186401 function foo(a) print(\u0026#34;foo\u0026#34;, a) return coroutine.yield(2 * a) end co = coroutine.create(function ( a, b ) print(\u0026#34;co-body\u0026#34;, a, b) local r = foo(a + 1) print(\u0026#34;co-body\u0026#34;, r) local r, s = coroutine.yield(a + b, a - b) print(\u0026#34;co-body\u0026#34;, r, s) return b, \u0026#34;end\u0026#34; end) print(\u0026#34;main\u0026#34;, coroutine.resume(co, 1, 10)) print(\u0026#34;main\u0026#34;, coroutine.resume(co, \u0026#34;m\u0026#34;))\t-- resume的参数 \u0026#39;m\u0026#39; 是在调用yield传入的，所以本次是在第5行 return m print(\u0026#34;main\u0026#34;, coroutine.resume(co, \u0026#34;x\u0026#34;, \u0026#34;y\u0026#34;)) print(\u0026#34;main\u0026#34;, coroutine.resume(co, \u0026#34;x\u0026#34;, \u0026#34;y\u0026#34;))   1 2 3 4 5 6 7 8  co-body\t1\t10 foo\t2 main\ttrue\t4 co-body\tm main\ttrue\t11\t-9 co-body\tx\ty main\ttrue\t10\tend main\tfalse\tcannot resume dead coroutine   python的协程(待续)  http://www.ibm.com/developerworks/cn/opensource/os-cn-python-yield/  简单地讲，yield 的作用就是把一个函数变成一个 generator，带有 yield 的函数不再是一个普通函数，Python 解释器会将其视为一个 generator 产生一个iterable对象    golang的协程(待续)  http://stackoverflow.com/questions/13107958/what-exactly-does-runtime-gosched-do  stm32/ contiki/ coroutine stm32上的协程实现 * http://blog.linux.org.tw/~jserv/archives/001848.html\n","date":"2016-03-13T17:59:29+08:00","permalink":"https://bg2bkk.github.io/p/coroutine-and-goroutine/","title":"coroutine and goroutine"},{"content":"目录  前言 基于openssl自建证书  CentOS Ubuntu   nginx的支持HTTP/2的patch nghttp2安装，配置，使用  nghttpd作为http2 server nghttp作为http2 client nghttpx作为proxy，转向nginx后端 h2load作为压测工具    前言  在研究HTTP/2协议时，常常和https协议混在一起，而二者之间的关系是怎样的呢？ 现有的http2 server中，nginx基于1.9.*有HTTP/2协议的patch，还有nghttp2 server，已经有人运行在个人博客做前端。  只说实践过程，作为记录。\n关于涉及到的概念等，需要在别的文档中写。\n基于openssl自建证书 在线上配置HTTPS时，需要从权威CA申请证书，在nginx中配置证书crt和私钥key。\n listen 8443 ssl; ssl_certificate /usr/lib/ssl/nginx.crt; ssl_certificate_key /usr/lib/ssl/nginx.key;  而在线下调试时，如果需要配置https，则需要自建和签发证书。 基于openssl自建证书\n在CentOS系统上自建证书 1.自建CA，颁发证书\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50  CA首先需要自建证书，作为颁发证书所用的根证书。CA的openssl配置文件/etc/pki/tls/openssl.cnf： #################################################################### [ ca ] default_ca\t= CA_default\t# The default ca section #################################################################### [ CA_default ] dir\t= /etc/pki/CA\t# Where everything is kept certs\t= $dir/certs\t# Where the issued certs are kept crl_dir\t= $dir/crl\t# Where the issued crl are kept database\t= $dir/index.txt\t# database index file. #unique_subject\t= no\t# Set to \u0026#39;no\u0026#39; to allow creation of # several ctificates with same subject. new_certs_dir\t= $dir/newcerts\t# default place for new certs. certificate\t= $dir/cacert.pem # The CA certificate serial\t= $dir/serial # The current serial number crlnumber\t= $dir/crlnumber\t# the current crl number # must be commented out to leave a V1 CRL crl\t= $dir/crl.pem # The current CRL private_key\t= $dir/private/cakey.pem# The private key RANDFILE\t= $dir/private/.rand\t# private random number file x509_extensions\t= usr_cert\t# The extentions to add to the cert # Comment out the following two lines for the \u0026#34;traditional\u0026#34; # (and highly broken) format. name_opt = ca_default\t# Subject Name options cert_opt = ca_default\t# Certificate field options default_days\t= 365\t# how long to certify for default_crl_days= 30\t# how long before next CRL default_md\t= default\t# use public key default MD preserve\t= no\t# keep passed DN ordering policy\t= policy_match # For the CA policy [ policy_match ] countryName\t= match stateOrProvinceName\t= match organizationName\t= match organizationalUnitName\t= optional commonName\t= supplied emailAddress\t= optional ...   其中我们可以看到根地址在/etc/pki/CA下，且指明了CA证书certificate是该目录下的cacert.pem，私钥在private/cakey.pem中，CA需要匹配countryName、stateOrProvinceName和organizationName，且commonName需要提供，这点比较重要。\n1 2 3 4 5  在/etc/pki/CA下创建初始文件 $ touch serial index.txt $ echo 01 \u0026gt; serial   2.生成根密钥\n1 2  $ cd /etc/pki/CA $ openssl genrsa -out private/cakey.pem 2048   3.生成根证书\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16  使用req指令，通过私钥，生成自签证书 $ openssl req -new -x509 -key private/cakey.pem -out cacert.pem You are about to be asked to enter information that will be incorporated into your certificate request. What you are about to enter is what is called a Distinguished Name or a DN. There are quite a few fields but you can leave some blank For some fields there will be a default value, If you enter \u0026#39;.\u0026#39;, the field will be left blank. ----- Country Name (2 letter code) [XX]:CN State or Province Name (full name) []:BeiJing Locality Name (eg, city) [Default City]: Organization Name (eg, company) [Default Company Ltd]: Organizational Unit Name (eg, section) []: Common Name (eg, your name or your server\u0026#39;s hostname) []:root Email Address []:   4.为nginx server生成密钥\n1 2  $ mkdir /data/zhendong/nginx_ssl $ openssl genrsa -out nginx.key 2048   5.为nginx生成 证书签署请求\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23  $ openssl req -new -key nginx.key -out nginx.csr You are about to be asked to enter information that will be incorporated into your certificate request. What you are about to enter is what is called a Distinguished Name or a DN. There are quite a few fields but you can leave some blank For some fields there will be a default value, If you enter \u0026#39;.\u0026#39;, the field will be left blank. ----- Country Name (2 letter code) [XX]:CN State or Province Name (full name) []:BeiJing Locality Name (eg, city) [Default City]: Organization Name (eg, company) [Default Company Ltd]: Organizational Unit Name (eg, section) []: Common Name (eg, your name or your server\u0026#39;s hostname) []:localhost Email Address []: Please enter the following \u0026#39;extra\u0026#39; attributes to be sent with your certificate request A challenge password []: An optional company name []: Common Name填成nginx server的server_name用来访问。 在openssl.cnf中需要match的项目，一定要一样。   6.向CA请求证书\n1 2 3 4 5  $ openssl ca -in nginx.csr -out nginx.crt 如果失败，可以尝试以下命令 $ openssl x509 -req -in nginx.csr -CA /etc/pki/CA/cacert.pem -CAkey /etc/pki/CA/private/cakey.pem -CAcreateserial -out nginx.crt   7.配置nginx\n1 2 3  listen 8443 ssl ; ssl_certificate /data1/zhendong/nginx_ssl/nginx.crt; ssl_certificate_key /data1/zhendong/nginx_ssl/nginx.key;   8.通过curl访问\n1 2 3 4 5  $ curl --cacert /etc/pki/CA/cacert.pem https://localhost:8443/ this is abtesting server $ curl --cacert /etc/pki/CA/cacert.pem https://127.0.0.1:8443/ curl: (51) SSL: certificate subject name \u0026#39;localhost\u0026#39; does not match target host name \u0026#39;127.0.0.1\u0026#39;   ###在Ubuntu系统上自建证书\nUbuntu系统与CentOS的不同之处在于软件包管理不同，当克服这部分不同后，就可以执行与CentOS系统一样的操作。\n首先Ubuntu的openssl目录在**/usr/lib/ssl**下，而实际上这是一个软链接。\n1 2 3 4 5 6 7  huang@ThinkPad-X220:/usr/lib/ssl$ ll /usr/lib/ssl/ total 52 drwxr-xr-x 3 root root 4096 8月 27 12:18 ./ drwxr-xr-x 238 root root 40960 8月 26 11:18 ../ lrwxrwxrwx 1 root root 14 2月 4 2015 certs -\u0026gt; /etc/ssl/certs/ drwxr-xr-x 2 root root 4096 7月 8 14:34 misc/ lrwxrwxrwx 1 root root 20 6月 11 23:35 openssl.cnf -\u0026gt; /etc/ssl/openssl.cnf lrwxrwxrwx 1 root root 16 2月 4 2015 private -\u0026gt; /etc/ssl/private/   不管怎样，我们的工作将在**/usr/lib/ssl**进行。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25  /usr/lib/ssl/openssl.cnf内容如下： #################################################################### [ ca ] default_ca\t= CA_default\t# The default ca section #################################################################### [ CA_default ] dir\t= ./demoCA\t# Where everything is kept certs\t= $dir/certs\t# Where the issued certs are kept crl_dir\t= $dir/crl\t# Where the issued crl are kept database\t= $dir/index.txt\t# database index file. #unique_subject\t= no\t# Set to \u0026#39;no\u0026#39; to allow creation of # several ctificates with same subject. new_certs_dir\t= $dir/newcerts\t# default place for new certs. certificate\t= $dir/cacert.pem # The CA certificate serial\t= $dir/serial # The current serial number crlnumber\t= $dir/crlnumber\t# the current crl number # must be commented out to leave a V1 CRL crl\t= $dir/crl.pem # The current CRL private_key\t= $dir/private/cakey.pem# The private key RANDFILE\t= $dir/private/.rand\t# private random number file x509_extensions\t= usr_cert\t# The extentions to add to the cert   从配置文件中看到，我们需要在**/usr/lib/ssl下建立demoCA**目录以及其他。\n1 2 3 4  $ cd /usr/lib/ssl $ mkdir demoCA $ mkdir demoCA/newcerts $ mkdir demoCA/private   余下的操作将与在CentOS系统上没有区别。最后执行情况为：\n1 2  $ curl --cacert /usr/lib/ssl/demoCA/cacert.pem https://localhost:8443 this is abtesting server   nginx的支持HTTP/2的patch  nginx目前已正式支持HTTP/2，包括tengine-2.1.2+ 和 openresty\n nginx在8月份的时候从1.9.3版本推出了支持HTTP/2的patch，使用时与标准nginx并无区别。\n1 2 3 4 5  $ cd nginx-1.9.4/ $ patch -p1 \u0026lt; patch.http2-v3_1.9.4.txt $ ./configure --with-http_v2_module --with-http_ssl_module $ make $ make install   支持http2的nginx关键配置为\n1  listen 8443 http2;   只需要在listen后加http2就可以了。目前curl在基于nghttp2提供的HTTP/2库后，可以支持访问http2的server，但是目前没有配置成功，所以对支持http2的nginx进行访问的工作，将在介绍完nghttp2后一并记录。\nnghttp2安装，配置，使用 nghttp2是由tatsuhiro开发的，之前的spdylay也是他开发的，一直走在http2.0的前列。nghttp2包括了HTTP/2.0的库，基于这个库tatsu实现了HTTP/2.0的server、client和压测工具h2load。\n由于nghttp2所依赖的库太新了，目前只在Ubuntu系统成功安装。安装过程：\n1 2 3 4 5  $ autoreconf -i $ automake $ autoconf $ ./configure $ make   其中**./configure**的结果非常重要\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47  Version: 1.2.2-DEV shared 14:8:0 Host type: x86_64-unknown-linux-gnu Install prefix: /usr/local C compiler: gcc CFLAGS: -g -O2 WARNCFLAGS: LDFLAGS: LIBS: CPPFLAGS: C preprocessor: gcc -E C++ compiler: g++ CXXFLAGS: -g -O2 -std=c++11 CXXCPP: g++ -E Library types: Shared=yes, Static=yes Python: Python: /usr/bin/python PYTHON_VERSION: 2.7 pyexecdir: ${exec_prefix}/lib/python2.7/dist-packages Python-dev: yes PYTHON_CPPFLAGS:-I/usr/include/python2.7 PYTHON_LDFLAGS: -L/usr/lib -lpython2.7 Cython: cython Test: CUnit: yes Failmalloc: yes Libs: OpenSSL: yes Libxml2: yes Libev: yes Libevent(SSL): yes Spdylay: yes Jansson: yes Jemalloc: yes Zlib: yes Boost CPPFLAGS: Boost LDFLAGS: Boost::ASIO: Boost::System: Boost::Thread: Features: Applications: yes HPACK tools: yes Libnghttp2_asio:no Examples: yes Python bindings:yes Threading: yes Third-party: yes   编译帮助文档\n1  make html   nghttpd作为http2 server http2-no-tls\nnghttpd -v 8080 -n 24 --no-tls -d ~/workspace/Nginx_ABTesting/utils/html/  http2-with-tls\nnghttpd -v 8080 -n 24 /usr/lib/ssl/nginx.key /usr/lib/ssl/nginx.crt -d ~/workspace/Nginx_ABTesting/utils/html/  关于nghttpd的选项，其实可以与nginx配置做到一一对照的。目前对nghttpd的源码及实现了解的比较少，因其日本人的过于C++代码的风格实在晦涩难懂，所以很少做调优。\nnghttp作为http2 client http2-client-no-tls\nnghttp http://127.0.0.1:8080  http2-client-with-tls\nnghttp --cert /usr/lib/ssl/demoCA/cacert.pem https://127.0.0.1:8080 nghttp https://127.0.0.1:8080  经过使用，nghttp也可以对nginx发出http2请求并成功返回。\n通过strace和阅读源码，nghttp（包括压测工具h2load）作为client时，会读取系统的证书/usr/lib/ssl/demoCA/cacert.pem，因此可以不用指定。\nnghttpx作为proxy，转向nginx后端 client ——\u0026gt; http2-proxy-no-tls ——\u0026gt; http1.1 upstream(nginx):\n# nghttpx -f127.0.0.1,8080 -b127.0.0.1,8022 --frontend-no-tls # curl 127.0.0.1:8022 this is beta3 server # nghttp http://127.0.0.1:8080 this is beta3 server  client ——\u0026gt; http2-proxy-with-tls ——\u0026gt; http1.1 upstream(nginx):\n# nghttpx -f127.0.0.1,8080 -b127.0.0.1,8022 /usr/lib/ssl/nginx.key /usr/lib/ssl/nginx.crt # nghttp https://127.0.0.1:8080 this is beta3 server  采用nginx-http2作为proxy，使用方法与nginx-http1.1没有区别。\nh2load作为压测工具 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27  $ h2load -n100 -c10 -t4 https://127.0.0.1:8080 starting benchmark... spawning thread #0: 3 concurrent clients, 25 total requests spawning thread #1: 3 concurrent clients, 25 total requests spawning thread #2: 2 concurrent clients, 25 total requests spawning thread #3: 2 concurrent clients, 25 total requests Protocol: TLSv1.2 Cipher: ECDHE-RSA-AES128-GCM-SHA256 progress: 8% done progress: 16% done progress: 24% done progress: 32% done progress: 40% done progress: 48% done progress: 56% done progress: 64% done progress: 72% done progress: 80% done progress: 88% done progress: 96% done finished in 67.29ms, 1486 req/s, 111.02KB/s requests: 100 total, 100 started, 100 done, 100 succeeded, 0 failed, 0 errored status codes: 100 2xx, 0 3xx, 0 4xx, 0 5xx traffic: 7650 bytes total, 3450 bytes headers, 2100 bytes data min max mean sd +/- sd time for request: 343us 8.76ms 1.40ms 1.49ms 91.00%   h2load的使用与wrk没有区别，参数都是一样的。\n根据不同配置，我们有以下几种场景：\n1 2 3 4 5  server/proxy\thttps nginx-http/1.1\twith-tls nginx-http/2\tno-tls nghttpd   相结合，压测场景比较多。幸运的是，不论server是nginx还是nghttpd，其参数和调优都可以指定，比如线程数；不论wrk还是h2load，参数也可以指定，使用起来区别不大。\n","date":"2016-03-12T16:17:52+08:00","permalink":"https://bg2bkk.github.io/p/http2%E7%9A%84%E5%AE%9E%E8%B7%B5%E8%BF%87%E7%A8%8B/","title":"HTTP2的实践过程"},{"content":" register_filesystem中的二级指针，刚开始没看懂。怒了，如果这个都没看懂，还搞什么C语言编程 leetcode中的反转链表，二级指针操作巨好用 先上代码吧  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121  #include\u0026lt;stdio.h\u0026gt;#include\u0026lt;stdlib.h\u0026gt; typedef struct ListNode{ int val; struct ListNode *next; } ListNode; void printList(ListNode *head){ if(head){ printf(\u0026#34;%d\\n\u0026#34;, head-\u0026gt;val); printList(head-\u0026gt;next); } } void printList_r(ListNode *head){ if(!head) return; if(head -\u0026gt; next) printList_r(head-\u0026gt;next); printf(\u0026#34;%d\\n\u0026#34;, head-\u0026gt;val); } ListNode ** addToTail(ListNode **list){ ListNode **p ; for( p = list; *p; p = \u0026amp; (*p)-\u0026gt;next); return p; } void reverseList(ListNode **list){ ListNode *head = NULL; ListNode **p = list; while(*p){ ListNode *next = (*p)-\u0026gt;next; (*p)-\u0026gt;next = head; head = *p; *p = next; } *list = head; } /* * delete first item whose val equals to val * */ void deleteNodeAll(ListNode **l, int val){ while(*l){ if((*l)-\u0026gt;val == val){ ListNode *tmp = (*l)-\u0026gt;next; free(*l); *l = tmp; break; } l = \u0026amp;(*l)-\u0026gt;next; } } //delete all the items whose val equals to val void deleteNodeFirst(ListNode **l, int val){ while(*l){ if((*l)-\u0026gt;val == val){ ListNode *tmp = (*l)-\u0026gt;next; free(*l); *l = tmp; } else l = \u0026amp;(*l)-\u0026gt;next; } } int main() { int a[] = {1, 2, 3, 2, 3 ,4}; int len = sizeof(a) / sizeof(int); int i = 0; static ListNode *list; for( i = 0; i \u0026lt; len; i++) { ListNode *node = malloc(sizeof(struct ListNode)); node-\u0026gt;val = a[i]; ListNode **p = addToTail(\u0026amp;list); *p = node; } printf(\u0026#34;--------print list in sequence------------\\n\u0026#34;); printList(list); printf(\u0026#34;--------print list after reversed------------------\\n\u0026#34;); reverseList(\u0026amp;list); printList(list); printf(\u0026#34;--------delete first 1----------\\n\u0026#34;); deleteNodeFirst(\u0026amp;list, 1); printList(list); printf(\u0026#34;--------delete first 3----------------\\n\u0026#34;); deleteNodeFirst(\u0026amp;list, 3); printList(list); printf(\u0026#34;--------delete first 4-----------------\\n\u0026#34;); deleteNodeFirst(\u0026amp;list, 4); printList(list); printf(\u0026#34;--------delete non-existed item-------------\\n\u0026#34;); deleteNodeFirst(\u0026amp;list, 4); printList(list); printf(\u0026#34;--------delete the last item----------------\\n\u0026#34;); deleteNodeFirst(\u0026amp;list, 2); printList(list); printf(\u0026#34;--------reverse an empty list---------------\\n\u0026#34;); reverseList(\u0026amp;list); printList(list); }   ","date":"2016-03-01T11:00:34+08:00","permalink":"https://bg2bkk.github.io/p/%E9%87%87%E7%94%A8%E4%BA%8C%E7%BA%A7%E6%8C%87%E9%92%88%E5%AE%9E%E7%8E%B0%E5%8D%95%E9%93%BE%E8%A1%A8%E6%93%8D%E4%BD%9C-%E5%8D%95%E9%93%BE%E8%A1%A8%E7%BF%BB%E8%BD%AC-%E5%88%A0%E9%99%A4%E5%8D%95%E9%93%BE%E8%A1%A8%E7%BB%93%E7%82%B9/","title":"采用二级指针实现单链表操作 单链表翻转 删除单链表结点"},{"content":"虚拟文件系统  虚拟文件系统为用户空间程序提供了文件和文件系统的接口 通过VFS，程序可以通过标准的UNIX系统调用操作不同的文件系统和介质，包括各种软硬件设备 Linux等现代操作系统引入VFS作为抽象层，极大方便系统调用  Unix文件系统   UNIX系统使用了四种和文件系统相关的抽象概念: 文件、目录项、索引结点和挂载点。\n  将文件的相关信息和文件加以区分\n 文件相关信息单独存储在索引结点中，又称为元数据，包括文件的控制权限、文件大小、属主和创建与访问时间等 文件相关信息和文件系统的相关信息密不可分，后者存储在***超级块(super block)***中，超级块是包含文件系统信息的数据结构。 文件按照索引结点存储在单独的块中，文件系统的控制信息存在超级块中    VFS中有四个主要对象类型\n 超级块对象，代表一个具体的已安装的文件系统 索引结点对象，代表一个具体文件  inode才代表具体文件   目录项对象，代表一个目录项，是路径的组成部分  目录项不是目录，而是一个文件。不存在目录对象   文件对象，代表由进程打开的文件  每个进程都有自己的打开文件列表，文件对象是一个动态生成动态销毁的对象      超级块\n 超级块对象 super block  用于存储特定文件系统的信息 通常放在磁盘的特定扇区中，所以被称为超级块对象 并非基于磁盘的文件系统，内核会现场创建，并保存在内存中   超级块操作  super_operations        索引结点\n 索引结点对象 inode  索引结点对象包含了内核在操作文件或目录时的所有信息 索引结点对象都是在内存中创建的，不会写回硬盘的   索引结点操作  inode_operations      目录项\n 目录项对象  VFS把目录当做文件对待，解析目录时，将路径中的每个组成部分都是一个索引结点对象，比如\u0026quot;/bin/ls\u0026quot;中的‘/’、‘bin’和‘ls’。进行路径查找和解析是比较耗时的，为了方便操作，VFS引入了目录项dentry的概念，每个dentry都是路径的组成部分 目录项对象都是根据字符串形式现场创建的，并没有保存在磁盘   目录项状态  被使用  被使用的dentry对应一个有效的inode，即dentry结构体中d_inode指向的inode 该对象的引用计数d_count为正，至少有一个使用者，不能随意丢弃   未被使用  未被使用的dentry，其d_inode也指向一个inode，但是d_count为0 此时该dentry仍然在缓存中，可能会再次使用。不会立刻被释放，但如果系统要回收内存的话，可以被释放回收   负状态  负状态的dentry没有对应的有效inode，原因可能是inode已被删除，或者路径不再正确 此时将其缓存起来仍然有些用处，比如一个守护进程一直读一个不存在的文件，缓存dentry不至于让进程总是去搜索     目录项缓存  遍历路径名中所有元素并逐个解析成dentry，是非常费时费力的，所以内核引入目录项缓存dcache，将目录项对象都缓存起来 目录项缓存包括  \u0026ldquo;被使用的\u0026quot;目录项链表  该链表通过inode中的i_dentry指针连接相关inode 一个给定的inode可能有多个链接（软硬链接），所有就有可能有多个目录项对象，因此用一个链表链接   \u0026ldquo;最近使用的\u0026quot;双向链表  该链表包含所有 未被使用的 和 负状态的 dentry 总是在表头添加元素，所以回收dentry时从最后开始回收   哈希表和相应的哈希函数  快速将给定路径解析为相关目录项对象 哈希表由dentry_hashtable数组表示，每个元素都指向一个具有相同键值的目录项对象链表指针 哈希函数d_hash() 查找函数d_lookup()       目录项操作  dentry_operation      文件\n 文件对象  表示进程已经打开的文件。文件对象是已打开的文件（物理文件）在内存中的表示 多个进程可以打开同一个物理文件，所以一个物理文件会有多个文件对象 文件对象指向目录项对象，目录项对象指向索引结点inode 具体而言，是文件对象filep中的f_dentry指向目录项对象，目录项对象的d_inode指向索引结点inode   文件操作  file_operations      相关数据结构\n file_system_type  用于描述各种特定文件系统类型，用于支持不同文件系统 struct file_system_type {}  get_sb() 从磁盘读取超级块，并在文件系统安装时在内存中组装超级块对象   每个文件系统只有一个   vfsmount  系统挂载时，将有一个vfsmount结构体在挂载点创建，代表文件系统的实例 struct vfsmount {}  各种链表        和进程相关的数据结构\n  每个进程都有自己的一组打开爱的文件，比如根文件系统、当前工作目录、挂载点等\n  struct files_struct {}\n 该结构提由进程描述符中的files指向，一般都是current-\u0026gt;files    struct fs_struct {}\n 包含文件系统和进程相关的信息，由fs域指向，一般是current-\u0026gt;fs    struct namespace {}\n 使得每个进程在系统中能看到唯一的安装文件系统，mm-\u0026gt;namespace    每个进程都有指向自己的fs_struct和files_struct，多个进程可能指向同一个，比如通过带有CLONE_FILES和CLONE_FS标志创建的进程（其实是线程），所以这两个struct都有引用计数，以防出错\n  而对于namespace来说，除非使用CLONE_NEWS标志创建进程，会创建新的namespace结构体，否则所有进程共享一个namespace\n    ","date":"2016-02-29T15:33:32+08:00","permalink":"https://bg2bkk.github.io/p/vfs%E8%99%9A%E6%8B%9F%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/","title":"VFS虚拟文件系统"},{"content":"pipe在linux内核中的实现 在之前关于linux shell多线程并发数控制的博文中，我们使用了fifo作为token池，通过读写fifo实现token分发控制，进而实现了控制线程数的目的。 我对pipe这个*** *NIX ***系统中最常用的组件（|）产生了兴趣\n fifo和pipe是什么关系？ fifo或者pipe的使用方法？ pipe在linux kernel中的实现是怎样的？ fifo或者pipe的容量有多大，可以配置吗？  先说结论吧\n fifo和pipe的区别  pipe是匿名管道，没有名字，只能用于两个拥有pipe读写两端fd的进程通信； fifo在文件系统中有自己的名称，操作fifo与操作普通文件几无差别，可以用于两个没有关系的进程间通信   fifo和pipe在kernel层面上都实现在fs/pipe.c中，所以本质上二者是一个东西。 pipe作为linux文件系统的一部分，与epoll一样，都是在向kernel注册了自己的文件系统，可以使用VFS提供的通用接口，比如open、read和write等操作 pipe的容量不是无限大的，早期linux版本（kernel-2.4）中pipe容量只能是4KB大小，新版本可以在运行时根据需要扩大到64KB  本文主要基于linux-2.4.20内核中的pipe实现进行分析，理由是该版本的pipe实现与新版本kernel并没有太大差别，但是代码可读性要强很多，可以快速了解pipe的实际实现；从2.4.20内核中对pipe的架构有整体了解后，再阅读新版本(4.4.1)中的新feature，会比较顺遂。\npipe在fs/pipe.c中一些函数 - new_inode() - register_filesystem() - get_empty_file - get_unused_fd() - do_pipe作为pipe系统调用函数，在/arch/i386/sys_i386.c中定义 - pipe的module_init在initcall中调用，但是pipe.c是编译在fs.o中的，他的module_init是如何调用进去的，需要进一步查找 - struct dentry在include/linux/dcache.h中定义  具体实现 * pipe文件系统初始化 * 注册pipefs * register_filesystems * pipe系统调用 * pipe调用do_pipe * do_pipe() * f1\u0026amp;f2 get_empty_filep分配filep数据结构 * inode = get_pipe_inode()从pipe文件系统获得inode * new_inode() * pipe_new()新建pipe * __get_free_pages(GFP_USER)为该pipe分配一页内存（4KB） * inode-\u0026gt;i_pipe = kmalloc(sizeof(struct pipe_inde_info), GFP_KERNEL)分配pipe信息结构 * i\u0026amp;j = get_unused_fd()获取两个fd * dentry = d_alloc()从pipefs分配dentry * d_add(dentry, inode)将inode插入到dentry中 * 将f1设置成O_RDONLY，将f2设置成O_WRONLY * 进程的files列表中，files[i] = f1, files[j] = f2 * 实现函数 * pipe * pipe_read * pipe_write   tips   pipe不允许使用seek\n  低版本linux-2.4.20在pipe写的时候是固定大小，而高版本的是会按需分配直至64KB的。\n  高版本kernel内核中sysctl的配置参数fs.pipe-max-size 可以设置固定的pipe大小。但是也不能超过64KB大小，即使配置数据大于这个数字，pipe大小也会限制在64KB。\n  测试代码\n    1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22  #!/bin/bash test $# -ge 1 || { echo \u0026#34;usage: $0write-size [wait-time]\u0026#34;; exit 1; } test $# -ge 2 || set -- \u0026#34;$@\u0026#34; 1 bytes_written=$( { exec 3\u0026gt;\u0026amp;1 { perl -e \u0026#39; $size = $ARGV[0]; $block = q(a) x $size; $num_written = 0; sub report { print STDERR $num_written * $size, qq(\\n); } report; while (defined syswrite STDOUT, $block) { $num_written++; report; } \u0026#39; \u0026#34;$1\u0026#34; 2\u0026gt;\u0026amp;3 } | (sleep \u0026#34;$2\u0026#34;; exec 0\u0026lt;\u0026amp;-); } | tail -1 ) printf \u0026#34;write size: %10d; bytes successfully before error: %d\\n\u0026#34; \\  \u0026#34;$1\u0026#34; \u0026#34;$bytes_written\u0026#34;   * 测试结果  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21  huang@ThinkPad-X220:~/workspace/cpp$ /bin/bash -c \u0026#39;for p in {0..18}; do ./pipe.sh $((2 ** $p)) 0.5; done\u0026#39; write size: 1; bytes successfully before error: 65536 write size: 2; bytes successfully before error: 65536 write size: 4; bytes successfully before error: 65536 write size: 8; bytes successfully before error: 65536 write size: 16; bytes successfully before error: 65536 write size: 32; bytes successfully before error: 65536 write size: 64; bytes successfully before error: 65536 write size: 128; bytes successfully before error: 65536 write size: 256; bytes successfully before error: 65536 write size: 512; bytes successfully before error: 65536 write size: 1024; bytes successfully before error: 65536 write size: 2048; bytes successfully before error: 65536 write size: 4096; bytes successfully before error: 65536 write size: 8192; bytes successfully before error: 65536 write size: 16384; bytes successfully before error: 65536 write size: 32768; bytes successfully before error: 65536 write size: 65536; bytes successfully before error: 65536 write size: 131072; bytes successfully before error: 0 write size: 262144; bytes successfully before error: 0   * 内核中64KB大小的限制在哪里设置的？(TO DO) * 只有在高版本的pipe实现中才有64KB大小，低版本都是4KB的。 * ulimit -a 的结果中，\u0026quot;pipe size (512 bytes, -p) 8\u0026quot;，表示一个pipe拥有8个512KB的buffer，总共是4KB * 在include/linux/fs_pipe_i.h中，#define PIPE_DEF_BUFFERS 16, 这里是[按buffer的数量分配的](http://home.gna.org/pysfst/tests/pipe-limit.html)。 * 在fs/pipe.c中，pipe_write和pipe_read是在运行时按页大小分配的 * sysctl中fs.max_pipe_size的设置，fs.pipe-max-size = 1048576，又会起什么作用   函数分析  init_pipe_fs向文件系统注册pipe组件    1 2 3 4 5 6 7 8 9 10 11 12 13  static int __init init_pipe_fs(void) { int err = register_filesystem(\u0026amp;pipe_fs_type); if (!err) { pipe_mnt = kern_mount(\u0026amp;pipe_fs_type); err = PTR_ERR(pipe_mnt); if (IS_ERR(pipe_mnt)) unregister_filesystem(\u0026amp;pipe_fs_type); else err = 0; } return err; }   1  static DECLARE_FSTYPE(pipe_fs_type, \u0026#34;pipefs\u0026#34;, pipefs_read_super, FS_NOMOUNT);   1 2 3 4 5 6 7  #define DECLARE_FSTYPE(var,type,read,flags) \\ struct file_system_type var = { \\ name:\ttype, \\ read_super:\tread, \\ fs_flags:\tflags, \\ owner:\tTHIS_MODULE, \\ }   * pipe源码（2.4的实现中实在没什么可讲的，比较有价值的是pipe_write和pipe_read中处理缓冲队列源码可以参考）  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256  /* * We use a start+len construction, which provides full use of the * allocated memory. * -- Florian Coosmann (FGC) * * Reads with count = 0 should always return 0. * -- Julian Bradfield 1999-06-07. */ /* Drop the inode semaphore and wait for a pipe event, atomically */ void pipe_wait(struct inode * inode) { DECLARE_WAITQUEUE(wait, current); current-\u0026gt;state = TASK_INTERRUPTIBLE; add_wait_queue(PIPE_WAIT(*inode), \u0026amp;wait); up(PIPE_SEM(*inode)); schedule(); remove_wait_queue(PIPE_WAIT(*inode), \u0026amp;wait); current-\u0026gt;state = TASK_RUNNING; down(PIPE_SEM(*inode)); } static ssize_t pipe_read(struct file *filp, char *buf, size_t count, loff_t *ppos) { struct inode *inode = filp-\u0026gt;f_dentry-\u0026gt;d_inode; ssize_t size, read, ret; /* Seeks are not allowed on pipes. */ ret = -ESPIPE; read = 0; if (ppos != \u0026amp;filp-\u0026gt;f_pos) goto out_nolock; /* Always return 0 on null read. */ ret = 0; if (count == 0) goto out_nolock; /* Get the pipe semaphore */ ret = -ERESTARTSYS; if (down_interruptible(PIPE_SEM(*inode))) goto out_nolock; if (PIPE_EMPTY(*inode)) { do_more_read: ret = 0; if (!PIPE_WRITERS(*inode)) goto out; ret = -EAGAIN; if (filp-\u0026gt;f_flags \u0026amp; O_NONBLOCK) goto out; for (;;) { PIPE_WAITING_READERS(*inode)++; pipe_wait(inode); PIPE_WAITING_READERS(*inode)--; ret = -ERESTARTSYS; if (signal_pending(current)) goto out; ret = 0; if (!PIPE_EMPTY(*inode)) break; if (!PIPE_WRITERS(*inode)) goto out; } } /* Read what data is available. */ ret = -EFAULT; while (count \u0026gt; 0 \u0026amp;\u0026amp; (size = PIPE_LEN(*inode))) { char *pipebuf = PIPE_BASE(*inode) + PIPE_START(*inode); ssize_t chars = PIPE_MAX_RCHUNK(*inode); if (chars \u0026gt; count) chars = count; if (chars \u0026gt; size) chars = size; if (copy_to_user(buf, pipebuf, chars)) goto out; read += chars; PIPE_START(*inode) += chars; PIPE_START(*inode) \u0026amp;= (PIPE_SIZE - 1); PIPE_LEN(*inode) -= chars; count -= chars; buf += chars; } /* Cache behaviour optimization */ if (!PIPE_LEN(*inode)) PIPE_START(*inode) = 0; if (count \u0026amp;\u0026amp; PIPE_WAITING_WRITERS(*inode) \u0026amp;\u0026amp; !(filp-\u0026gt;f_flags \u0026amp; O_NONBLOCK)) { /* * We know that we are going to sleep: signal * writers synchronously that there is more * room. */ wake_up_interruptible_sync(PIPE_WAIT(*inode)); if (!PIPE_EMPTY(*inode)) BUG(); goto do_more_read; } /* Signal writers asynchronously that there is more room. */ wake_up_interruptible(PIPE_WAIT(*inode)); ret = read; out: up(PIPE_SEM(*inode)); out_nolock: if (read) ret = read; UPDATE_ATIME(inode); return ret; } static ssize_t pipe_write(struct file *filp, const char *buf, size_t count, loff_t *ppos) { struct inode *inode = filp-\u0026gt;f_dentry-\u0026gt;d_inode; ssize_t free, written, ret; /* Seeks are not allowed on pipes. */ ret = -ESPIPE; written = 0; if (ppos != \u0026amp;filp-\u0026gt;f_pos) goto out_nolock; /* Null write succeeds. */ ret = 0; if (count == 0) goto out_nolock; ret = -ERESTARTSYS; if (down_interruptible(PIPE_SEM(*inode))) goto out_nolock; /* No readers yields SIGPIPE. */ if (!PIPE_READERS(*inode)) goto sigpipe; /* If count \u0026lt;= PIPE_BUF, we have to make it atomic. */ free = (count \u0026lt;= PIPE_BUF ? count : 1); /* Wait, or check for, available space. */ if (filp-\u0026gt;f_flags \u0026amp; O_NONBLOCK) { ret = -EAGAIN; if (PIPE_FREE(*inode) \u0026lt; free) goto out; } else { while (PIPE_FREE(*inode) \u0026lt; free) { PIPE_WAITING_WRITERS(*inode)++; pipe_wait(inode); PIPE_WAITING_WRITERS(*inode)--; ret = -ERESTARTSYS; if (signal_pending(current)) goto out; if (!PIPE_READERS(*inode)) goto sigpipe; } } /* Copy into available space. */ ret = -EFAULT; while (count \u0026gt; 0) { int space; char *pipebuf = PIPE_BASE(*inode) + PIPE_END(*inode); ssize_t chars = PIPE_MAX_WCHUNK(*inode); if ((space = PIPE_FREE(*inode)) != 0) { if (chars \u0026gt; count) chars = count; if (chars \u0026gt; space) chars = space; if (copy_from_user(pipebuf, buf, chars)) goto out; written += chars; PIPE_LEN(*inode) += chars; count -= chars; buf += chars; space = PIPE_FREE(*inode); continue; } ret = written; if (filp-\u0026gt;f_flags \u0026amp; O_NONBLOCK) break; do { /* * Synchronous wake-up: it knows that this process * is going to give up this CPU, so it doesn\u0026#39;t have * to do idle reschedules. */ wake_up_interruptible_sync(PIPE_WAIT(*inode)); PIPE_WAITING_WRITERS(*inode)++; pipe_wait(inode); PIPE_WAITING_WRITERS(*inode)--; if (signal_pending(current)) goto out; if (!PIPE_READERS(*inode)) goto sigpipe; } while (!PIPE_FREE(*inode)); ret = -EFAULT; } /* Signal readers asynchronously that there is more data. */ wake_up_interruptible(PIPE_WAIT(*inode)); inode-\u0026gt;i_ctime = inode-\u0026gt;i_mtime = CURRENT_TIME; mark_inode_dirty(inode); out: up(PIPE_SEM(*inode)); out_nolock: if (written) ret = written; return ret; sigpipe: if (written) goto out; up(PIPE_SEM(*inode)); send_sig(SIGPIPE, current, 0); return -EPIPE; } /* No kernel lock held - fine */ static unsigned int pipe_poll(struct file *filp, poll_table *wait) { unsigned int mask; struct inode *inode = filp-\u0026gt;f_dentry-\u0026gt;d_inode; poll_wait(filp, PIPE_WAIT(*inode), wait); /* Reading only -- no need for acquiring the semaphore. */ mask = POLLIN | POLLRDNORM; if (PIPE_EMPTY(*inode)) mask = POLLOUT | POLLWRNORM; if (!PIPE_WRITERS(*inode) \u0026amp;\u0026amp; filp-\u0026gt;f_version != PIPE_WCOUNTER(*inode)) mask |= POLLHUP; if (!PIPE_READERS(*inode)) mask |= POLLERR; return mask; }   ","date":"2016-02-27T09:49:41+08:00","permalink":"https://bg2bkk.github.io/p/pipe%E5%9C%A8%E5%86%85%E6%A0%B8%E4%B8%AD%E7%9A%84%E5%AE%9E%E7%8E%B0/","title":"pipe在内核中的实现"},{"content":" redis主从复制  在介绍REDIS的RDB持久化方式时，我们提到了主从复制的实现过程  第一次同步，slave发送sync命令开始同步，master生成快照全量发送给slave，快照生成之后的变更命令缓存起来，也一块发送给slave 第二次及以后的同步，master收到命令后修改数据，并将数据修改后的结果同步给slave；如果此时发生断开重连情况，则重新进行第一步操作； redis-2.8版本后，如果发生断开重连，则进行增量传输，而不是全量传输   这里有两个值得介绍的地方  不论是否设置RDB持久化，主从复制都会产生快照 redis.conf中不设置save 900 1等配置时，只是不自动产生快照，如果执行save，还是会产生快照的 主从复制时，master执行完命令后会立刻将结果返回client，而不是等待同步给slave后再返回给client。这里可能会有一个不一致窗口，如果主从在master执行完指令和同步给client之间断开，这里会发生不一致现象，需要注意   主从复制的常见设计思路  用于保证数据持久化  master正常读写，不设置RDB或者AOF的持久化 slave设置RDB和AOF方式持久化，保证数据安全   基于实用目的的主从复制  master设置为只写模式，将结果同步给slave slave设置为只读模式，作为系统缓存        一般而言，了解以上知识，只能算是对redis的主从同步机制有了整体和粗糙的认识，而主从复制过程中发生了什么，如果能充分挖掘，也能感受到作者当时的苦心孤诣，学习到新的知识。\n好吧，装逼的话说完了，我想说，面试58的分布式存储工程师的时候，我被问到了这些问题，这些问题问的非常好，我需要补补课！\nreplication of redis replication\n抓包发现，首次同步时，master将rdb文件整体发给slave，之后master有变动的话，master将会同步命令给salve，比如直接同步set key val命令。\nv2.8之后的redis增量复制   redis slave有6个状态:\n REDIS_REPL_NONE  收到 slaveof host port，或者初始化进行主从同步时，进入下一状态   REDIS_REPL_CONNECT  redis会周期的执行replicationCron函数，而在该状态下，replicationCron函数将创建socket连接master，进入下一状态   REDIS_REPL_CONNECTING  同步socket将发送PING消息给master，随后关闭该socket的写事件，进入下一状态   REDIS_REPL_RECEIVE_PONG  socket收到PONG后，进行redis认证，认证成功后发送SYNC命令请求同步；新建临时文件接收rdb文件；进入下一状态   REDIS_REPL_TRANSFER  socket读取rdb数据，写到rdb文件   REDIS_REPL_CONNECTED  完全获取rdb文件后，进入CONNECTED状态；把master当做一个特殊的client，接收写命令      redis master 有4个状态\n REDIS_REPL_WAIT_BGSAVE_START REDIS_REPL_WAIT_BGSAVE_END_ REDIS_REPL_WAIT_BGSAVE_BULK REDIS_REPL_WAIT_BGSAVE_ONLINE    增量复制\n 增量复制的区别，同步时，客户端先做增量复制，不然时间太久也只能全量复制了。 rdb文件格式    ","date":"2016-02-26T16:15:31+08:00","permalink":"https://bg2bkk.github.io/p/redis%E4%B8%BB%E4%BB%8E%E5%A4%8D%E5%88%B6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/","title":"redis主从复制学习笔记"},{"content":"  虽然网络上关于redis持久化的相关内容数不胜数，但是一来作为我的学习笔记，好记性不如烂笔头；二来除去官方redis之外，很多有意义的修改或补充都非常值得讨论，所以我想做一下记录。\n  持久化用于重启后的数据恢复，而持久化的引入导致了redis可能产生的性能抖动\n  redis持久化的两种方法 \n  RDB方式\n  RDB方式是redis的默认持久化方式\n  快照RDB持久化过程:\n redis调用fork，产生子进程 父进程继续接受用户请求；子进程负责将内存内容写入临时文件，写入完成后rename为dump文件，实现替换 在子进程写内存内容期间，父进程如果要修改内存数据，os将会通过写时复制为父进程创建副本，所以此时子进程写入的仍然是fork时刻的整个数据库内容    不足之处在于:\n 如果redis出现问题崩溃了，此时的rdb文件可能不是最新的数据，从上次RDB文件生成到redis崩溃这段时间的数据全部丢掉。 产生快照时，redis最多将占用2倍于现有数据规模的内存，因此当内存占用过多时，RDB方式可能导致系统负载过高，甚至假死。（有个说法是，当redis的内存占用超过物理内存的3/5时，进行RDB主从复制就比较危险了）    主从复制过程\n 第一次同步  slave向master发送sync同步请求，master先dump出rdb文件，并将其全量传输给slave；master将产生rdb文件之后这段时间内的修改命令缓存起来，并发送给slave。首次同步完成。   第二次及以后的同步实现方式：  master将变量的快照（有修改的变量）直接实时发送给slave。 如果发生断开重连，则重复第一步第二步   reids-2.8版本之后，重连后进行第一步时，不用全量更新了。      AOF方式\n  AOF方式持久化过程\n Append Only File Redis将每次收到的命令都追加到文件中，类似于mysql的binlog；当redis重启时重新执行文件中的所有命令来重建数据 如果将所有命令不加甄别的都写入文件中，持久化文件会越来越大，比如INCR test命令执行100次，效果与SET test 100一样。此时需要进行rewrite，合并命令。 Redis提供了bgrewriteaof命令，执行过程与产生RDB文件的机制类似，fork出的子进程将内存中的数据以命令的方式重写持久化文件。本质上讲，该命令是将数据库中所有数据内容以命令的方式重写进新的AOF文件    AOF方式之我的想法\n AOF方式是redis在收到命名后将命令写入文件内，如果redis发生故障，重启时直接读取AOF文件重新执行命令即可恢复，可以克服RDB方式的缺点 bgrewriteaof指令是对AOF方式的一次优化，执行bgrewriteaof命令时是根据此时数据库内容来写入AOF文件，并替换旧的AOF文件。这个过程与RDB快照产生方式一样      RDB方式和AOF方式的对比\n RDB方式恢复起来快，而AOF方式需要一条条命令执行 RDB文件不需要经过编码，是数据库内容的直接克隆，所以文件比较小；而AOF文件内是一条条命令，需要依次执行 RDB文件可能会丢失部分数据，而AOF则专门解决这个问题    选择哪种方式\n 官方推荐：  如果想要很高的数据保障，则同时使用两种方式 如果可以接受数据丢失，则仅使用RDB方式   通常的设计思路是  利用replication机制弥补持久化在性能和设计上的不足 master上不做RDB和AOF，保证读写性能 slave同时开启两种方式，保证数据安全性        Redis数据恢复过程\n AOF优先级高于RDB方式，如果同时配置了AOF和RDB，AOF生效    1 2 3 4 5 6 7 8 9 10 11 12 13 14 15  void loadDataFromDisk(void) { long long start = ustime(); if (server.aof_state == REDIS_AOF_ON) { if (loadAppendOnlyFile(server.aof_filename) == REDIS_OK) redisLog(REDIS_NOTICE,\u0026#34;DB loaded from append only file: %.3f seconds\u0026#34;,(float)(ustime()-start)/1000000); } else { if (rdbLoad(server.rdb_filename) == REDIS_OK) { redisLog(REDIS_NOTICE,\u0026#34;DB loaded from disk: %.3f seconds\u0026#34;, (float)(ustime()-start)/1000000); } else if (errno != ENOENT) { redisLog(REDIS_WARNING,\u0026#34;Fatal error loading the DB: %s. Exiting.\u0026#34;,strerror(errno)); exit(1); } } }    持久化和RDB文件格式  ","date":"2016-02-26T14:56:41+08:00","permalink":"https://bg2bkk.github.io/p/redis%E6%8C%81%E4%B9%85%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/","title":"redis持久化学习笔记"},{"content":"在我尝试从kernel中深入了解TCP IP协议栈时，遇到了难题。 我选择的是linux-2.4.20 kernel，理由如下： 1. 首先是\u0026lt;TCP/IP Architecture, Design, And Implementation In Linux\u0026gt;一书采用的是该版本，会有按图索骥的效果。 2. 第二个理由，正如书中所说的，TCP/IP协议栈在2.4内核中就已经基本成型，而根据我实际对比，2.4.20内核与4.4.1内核在TCP/IP实现的框架上大体是相同的，区别是2.6 kernel以后完全将VFS中的各组件namespace化，另外是一些高版本内核引入的措施（比如对比net/socket.c中sock_create函数）。 3. 第三个理由是，linux-2.4 kernel的代码还没有开始爆炸，适合初学者入门，也适合我这样学力不足的人。 采用linux-2.4 kernel的不足之处在于，版本较老，想亲自动手实验，需要做一些兼容性的准备。  我搜到了这样一篇帖子， 收益颇深。解决了我的疑问，用我最能接受的方式，先从kernel启动的函数说起，然后调用到我能看到的net/socket.c中的函数；然后又通过修改kernel源码添加标记，打印运行log来标志函数执行；然后通过讲解module_init注册的静态模块是如何加进内核可执行文件里的，然后编译出linux.map文件，进一步确定函数执行顺序。这个方式让我非常容易接受，也很感慨写博客的人功力之深，通篇干货没有废话；更感慨的是，这个帖子写于2001左右，当时进行kernel修改还是比较容易的事情，现在的kernel代码越来越庞大，初学者为此望而却步，很难入手；新人难以入门的问题，近年来也多有讨论。\n那我就先把原作者的文章翻译过来，再继续下一步工作吧。\n先说结论  kernel启动时，第一个与network有关的函数是sock_init()，用来向kernel注册sock文件系统并挂载，以及加载其他模块，比如netfilter loopback设备随后被初始化，因为该设备比较简单。drivers/net/loopback.c dummy 和 Ethernet 设备随后被初始化 TCP/IP协议栈是在inet_init()中初始化的 Unix Domain Socket是在af_unix_init()中初始化的。1~5步按时间顺序排列。  Linux Kernel 2.4的入口函数 1.经过基本硬件设置后，启动代码(定义在head.S中)调用 /init/main.c 的 start_kernel()函数\n1 2 3 4  #arch/i386/kernel/head.S ... call SYMBOL_NAME(start_kernel) ...   2.sock_init()调用过程，向系统注册sock文件系统并挂载\nsock_init()将向系统注册sock文件系统 do_initcalls()中循环调用所有MODULE_INIT()的模块，包括系统中的inet_init和af_unix_init，至于如何关联起来的，稍后会有介绍。  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41  asmlinkage void __init start_kernel(void) { ... printk(linux_banner);\t// \u0026#34;linux_banner\u0026#34; is defined in init/version.c (W.N.). ... ... // Dozens of initialize routines ... kernel_thread(init, NULL, CLONE_FS | CLONE_FILES | CLONE_SIGNAL); ... cpu_idle(); } static int init(void * unused) { ... do_basic_setup(); ... execve(\u0026#34;/sbin/init\u0026#34;,argv_init,envp_init); ... } static void __init do_basic_setup(void) { ... sock_init();\t// net/socket.c (SEE BELOW) ... do_initcalls(); ... } static void __init do_initcalls(void) { initcall_t *call; call = \u0026amp;__initcall_start; do { (*call)(); call++; } while (call \u0026lt; \u0026amp;__initcall_end); ... }   3.sock_init()的内容\n欢迎信息printk() 清空协议栈数组，此时系统中没有任何协议  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38  ... /* * The protocol list. Each protocol is registered in here. */ static struct net_proto_family *net_families[NPROTO];\t// Current NPROTO is defined as 32 \t// in \u0026lt;linux/net.h\u0026gt; (W.N.). ... void __init sock_init(void) { int i; printk(KERN_INFO \u0026#34;Linux NET4.0 for Linux 2.4\\n\u0026#34;); printk(KERN_INFO \u0026#34;Based upon Swansea University Computer Society NET3.039\\n\u0026#34;); /* * Initialize all address (protocol) families. */ #清空所有协议  for (i = 0; i \u0026lt; NPROTO; i++) net_families[i] = NULL; ... /* * Initialize the protocols module. */ #注册文件系统并挂载，sock_fs_type之前被初始化  register_filesystem(\u0026amp;sock_fs_type); sock_mnt = kern_mount(\u0026amp;sock_fs_type); /* The real protocol initialization is performed when * do_initcalls is run. */ ... }   4.do_initcalls()中的调用函数\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18  #init/main.c  static void __init do_initcalls(void) { initcall_t *call; call = \u0026amp;__initcall_start; do { #添加打印语句，dmesg命令可以输出启动结果  printk(KERN_INFO \u0026#34;+++ do_initcall: %08X\\n\u0026#34;, call);\t// Dump the entry address of initializer (W.N.).  (*call)(); call++; } while (call \u0026lt; \u0026amp;__initcall_end); /* Make sure there is no pending stuff from the initcall sequence */ flush_scheduled_tasks(); }   同时也修改loopback_init函数\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16  #drivers/net/loopback.c  int __init loopback_init(struct net_device *dev) { #添加打印语句  printk(KERN_INFO \u0026#34;=== Executing loopback_init ===\\n\u0026#34;); dev-\u0026gt;mtu = PAGE_SIZE - LOOPBACK_OVERHEAD; dev-\u0026gt;hard_start_xmit = loopback_xmit; dev-\u0026gt;hard_header = eth_header; dev-\u0026gt;hard_header_cache = eth_header_cache; dev-\u0026gt;header_cache_update= eth_header_cache_update; dev-\u0026gt;hard_header_len = ETH_HLEN; /* 14 */ dev-\u0026gt;addr_len = ETH_ALEN; /* 6 */ ... };   重新编译内核，替换并重启，dmesg的输出结果为：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135  Linux version 2.4.3 (root@mebius) (gcc version 2.95.3 20010315 (Debian release)) #9 Tue Apr 3 17:37: 44 JST 2001 BIOS-provided physical RAM map: BIOS-e820: 0000000000000000 - 000000000009f800 (usable) BIOS-e820: 000000000009f800 - 00000000000a0000 (reserved) BIOS-e820: 00000000000ebc00 - 0000000000100000 (reserved) BIOS-e820: 0000000000100000 - 0000000007ff0000 (usable) BIOS-e820: 0000000007ff0000 - 0000000007fffc00 (ACPI data) BIOS-e820: 0000000007fffc00 - 0000000008000000 (ACPI NVS) BIOS-e820: 00000000fff80000 - 0000000100000000 (reserved) On node 0 totalpages: 32752 zone(0): 4096 pages. zone(1): 28656 pages. zone(2): 0 pages. Kernel command line: root=/dev/hda1 mem=131008K Initializing CPU#0 Detected 333.350 MHz processor. Console: colour VGA+ 80x25 Calibrating delay loop... 665.19 BogoMIPS Memory: 126564k/131008k available (1076k kernel code, 4056k reserved, 387k data, 184k init, 0k highm em) Dentry-cache hash table entries: 16384 (order: 5, 131072 bytes) Buffer-cache hash table entries: 4096 (order: 2, 16384 bytes) Page-cache hash table entries: 32768 (order: 5, 131072 bytes) Inode-cache hash table entries: 8192 (order: 4, 65536 bytes) CPU: Before vendor init, caps: 0183f9ff 00000000 00000000, vendor = 0 CPU: L1 I cache: 16K, L1 D cache: 16K CPU: L2 cache: 256K Intel machine check architecture supported. Intel machine check reporting enabled on CPU#0. CPU: After vendor init, caps: 0183f9ff 00000000 00000000 00000000 CPU: After generic, caps: 0183f9ff 00000000 00000000 00000000 CPU: Common caps: 0183f9ff 00000000 00000000 00000000 CPU: Intel Mobile Pentium II stepping 0a Enabling fast FPU save and restore... done. Checking \u0026#39;hlt\u0026#39; instruction... OK. POSIX conformance testing by UNIFIX PCI: PCI BIOS revision 2.10 entry at 0xfd9be, last bus=0 PCI: Using configuration type 1 PCI: Probing PCI hardware PCI: Using IRQ router PIIX [8086/7110] at 00:07.0 got res[10000000:10000fff] for resource 0 of Ricoh Co Ltd RL5c475 Limiting direct PCI/PCI transfers. #sock_init()的运行log Linux NET4.0 for Linux 2.4\t// Message from sock_init() Based upon Swansea University Computer Society NET3.039 #sock_init()运行结束  #do_initcalls()中的每个initcall +++ do_initcall: C029F4E8\t// do_initcalls() START +++ do_initcall: C029F4EC +++ do_initcall: C029F4F0\t// apm_init() in arch/i386/kernel/kernel.o apm: BIOS version 1.2 Flags 0x03 (Driver version 1.14) +++ do_initcall: C029F4F4 +++ do_initcall: C029F4F8 +++ do_initcall: C029F4FC\t// kswapd_init() in mm/mm.o Starting kswapd v1.8 +++ do_initcall: C029F500 +++ do_initcall: C029F504 +++ do_initcall: C029F508 +++ do_initcall: C029F50C +++ do_initcall: C029F510 +++ do_initcall: C029F514 +++ do_initcall: C029F518 +++ do_initcall: C029F51C +++ do_initcall: C029F520 +++ do_initcall: C029F524 +++ do_initcall: C029F528\t// partition_setup() in fs/fs.o pty: 256 Unix98 ptys configured block: queued sectors max/low 84058kB/28019kB, 256 slots per queue RAMDISK driver initialized: 16 RAM disks of 8000K size 1024 blocksize Uniform Multi-Platform E-IDE driver Revision: 6.31 ide: Assuming 33MHz system bus speed for PIO modes; override with idebus=xx PIIX4: IDE controller on PCI bus 00 dev 39 PIIX4: chipset revision 1 PIIX4: not 100% native mode: will probe irqs later ide0: BM-DMA at 0xfc90-0xfc97, BIOS settings: hda:DMA, hdb:pio ide1: BM-DMA at 0xfc98-0xfc9f, BIOS settings: hdc:pio, hdd:pio hda: TOSHIBA MK8113MAT, ATA DISK drive ide0 at 0x1f0-0x1f7,0x3f6 on irq 14 hda: 16006410 sectors (8195 MB), CHS=996/255/63, UDMA(33) Partition check: hda: hda1 hda2 hda3 hda4 \u0026lt; hda5 hda6 hda7 hda8 hda9 hda10 \u0026gt; Floppy drive(s): fd0 is 1.44M FDC 0 is a National Semiconductor PC87306 #在loopback_init()中添加printk函数的结果 === Executing loopback_init ===\t// loopback initialization is here!  +++ do_initcall: C029F52C\t// ext2_fs() in fs/fs.o +++ do_initcall: C029F530 +++ do_initcall: C029F534 +++ do_initcall: C029F538 +++ do_initcall: C029F53C +++ do_initcall: C029F540 loop: loaded (max 8 devices) +++ do_initcall: C029F544 Serial driver version 5.05 (2000-12-13) with MANY_PORTS SHARE_IRQ SERIAL_PCI enabled ttyS00 at 0x03f8 (irq = 4) is a 16550A +++ do_initcall: C029F548\t// dummy_init_module() in drivers/net/net.o +++ do_initcall: C029F54C\t// rtl8139_init_module() in drivers/net/net.o 8139too Fast Ethernet driver 0.9.15c loaded PCI: Found IRQ 9 for device 00:03.0 PCI: The same IRQ used for device 00:07.2 eth0: RealTek RTL8139 Fast Ethernet at 0xc8800c00, 08:00:1f:06:79:20, IRQ 9 eth0: Identified 8139 chip type \u0026#39;RTL-8139B\u0026#39; +++ do_initcall: C029F550 +++ do_initcall: C029F554 +++ do_initcall: C029F558 +++ do_initcall: C029F55C +++ do_initcall: C029F560 #inet_init()的initcall结果 +++ do_initcall: C029F564\t// inet_init() in net/network.o NET4: Linux TCP/IP 1.0 for NET4.0 IP Protocols: ICMP, UDP, TCP IP: routing cache hash table of 512 buckets, 4Kbytes TCP: Hash tables configured (established 8192 bind 8192) #af_unix_inet()的initcall结果 +++ do_initcall: C029F568\t// af_unix_init() in net/network.o NET4: Unix domain sockets 1.0/SMP for Linux NET4.0. +++ do_initcall: C029F56C +++ do_initcall: C029F570 +++ do_initcall: C029F574\t// atalk_init() in net/network.o NET4: AppleTalk 0.18a for Linux NET4.0\t// do_initcalls() END fatfs: bogus cluster size reiserfs: checking transaction log (device 03:01) ... Using r5 hash to sort names ReiserFS version 3.6.25 VFS: Mounted root (reiserfs filesystem) readonly. Freeing unused kernel memory: 184k freed Adding Swap: 128516k swap-space (priority -1) eth0: Setting half-duplex based on auto-negotiated partner ability 0000.   5.initcalls的实现机制\n首先我们可以看到每个module都有使用module_init宏。\n1 2 3 4 5 6 7 8 9 10  #net/ipv4/af_inet.c  static int __init inet_init(void) { ... printk(KERN_INFO \u0026#34;NET4: Linux TCP/IP 1.0 for NET4.0\\n\u0026#34;); ... } module_init(inet_init);   __init宏和 module_init宏在 include/linux/init.h 中定义\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43  #ifndef MODULE #ifndef __ASSEMBLY__ ... typedef int (*initcall_t)(void); ... extern initcall_t __initcall_start, __initcall_end; #define __initcall(fn) \\ static initcall_t __initcall_##fn __init_call = fn ... #endif /* __ASSEMBLY__ */ /* * Mark functions and data as being only used at initialization * or exit time. */ #define __init __attribute__ ((__section__ (\u0026#34;.text.init\u0026#34;))) ... #define __init_call __attribute__ ((unused,__section__ (\u0026#34;.initcall.init\u0026#34;))) ... /** * module_init() - driver initialization entry point * @x: function to be run at kernel boot time or module insertion * * module_init() will add the driver initialization routine in * the \u0026#34;__initcall.int\u0026#34; code segment if the driver is checked as * \u0026#34;y\u0026#34; or static, or else it will wrap the driver initialization * routine with init_module() which is used by insmod and * modprobe when the driver is used as a module. */ #define module_init(x) __initcall(x); ... #else // MODULE ... #define __init ... #define __initcall(fn) ... #define module_init(x) \\ int init_module(void) __attribute__((alias(#x))); \\ extern inline __init_module_func_t __init_module_inline(void) \\ { return x; } ... #endif // MODULE   * init.h中的#define MODULE是在Makefile中的 -DMODULE 设置的，表示可以动态添加MODULE * 目前 CONFIG_INET (/arch/i386/defconfig) 不是 可选module （M），而是静态编译进内核的（y）。静态模块由init.h中的#ifndef MODULE块预处理，而可动态加载的模块（M）则会调用 #else //MODULE 后的初始化代码 * 所以经过预编译后，inet_init()函数将由上述代码的#ifndef MODULE 预处理为  1 2 3 4 5 6 7 8 9  #include/linux/init.h static int __attribute__ ((__section__ (\u0026#34;.text.init\u0026#34;))) inet_init(void) { ... printk(KERN_INFO \u0026#34;NET4: Linux TCP/IP 1.0 for NET4.0\\n\u0026#34;); ... } initcall_t __initcall_inet_init __attribute__ ((unused,__section__ (\u0026#34;.initcall.init\u0026#34;))) = inet_init;    这个扩展过程意味着：  inet_init()函数的代码段text code将编译进kernel可执行文件的***.text.init***段中，这种机制的目的是kernel启动，注册模块后能够释放所占用的内存 预处理后的***__initcall_inet_init***作为inet_init()函数的入口，将被存储在kernel可执行文件的 .initcall.init 段中。  注意这个宏定义是static类型的，所以我们并不能确定这个宏定义的结果是否在kernel的全局符号表中。（只有全局变量才在符号表中）     为了能够一探究竟，移除该宏定义的static标志，然后***_initcall*** ***这些入口就是全局变量了，然后我们就能在内核编译后的符号表中看到这些入口函数。  注意如果这些入口函数不是static作用域后，会导致一些链接错误，原因是命名冲突，比如netfilter中有类似命名   Let\u0026rsquo;s hack the kernel!!!  5.如何从内部观察linux kernel\n* linux kernel只是一个ELF可执行目标文件，和/bin/ls之类的可执行文件没有区别 * 所以作为kernel ELF文件，vmlinux可以通过nm、objdump和readelf等工具观察 * 默认情况下，linux kernel的顶层Makefile编译成功后将生成System.map文件，以方便调试，而这个文件不过是一个符号表。所以我向这个编译添加\u0026quot;--cref -Map linux.map\u0026quot;选项，可以生成一个包含更多信息的符号表  1 2 3 4 5 6 7 8 9 10 11 12 13 14  #Makefile #添加 --cref -Map linux.map 选项 vmlinux: $(CONFIGURATION) init/main.o init/version.o linuxsubdirs $(LD) $(LINKFLAGS) $(HEAD) init/main.o init/version.o \\ --start-group \\ $(CORE_FILES) \\ $(DRIVERS) \\ $(NETWORKS) \\ $(LIBS) \\ --end-group \\ --cref -Map linux.map \\ -o vmlinux $(NM) vmlinux | grep -v \u0026#39;\\(compiled\\)\\|\\(\\.o$$\\)\\|\\( [aUw] \\)\\|\\(\\.\\.ng$$\\)\\|\\(LASH[RL]DI\\)\u0026#39; | sort \u0026gt; System.map   * make vmlinux编译kernle源码，生成vmlinux和linux.map，通过objdump -h vmlinux查看各段信息 * 可以看到***.text.init***段和***.initcall.init***段  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39  #objdump -h vmlinux vmlinux: file format elf32-i386 Sections: Idx Name Size VMA LMA File off Algn 0 .text 0010bf68 c0100000 c0100000 00001000 2**4 CONTENTS, ALLOC, LOAD, READONLY, CODE 1 .text.lock 00001130 c020bf68 c020bf68 0010cf68 2**2 CONTENTS, ALLOC, LOAD, READONLY, CODE 2 .rodata 0004407c c020d0a0 c020d0a0 0010e0a0 2**5 CONTENTS, ALLOC, LOAD, READONLY, DATA 3 .kstrtab 000062fe c0251120 c0251120 00152120 2**5 CONTENTS, ALLOC, LOAD, READONLY, DATA 4 __ex_table 00001418 c0257420 c0257420 00158420 2**2 CONTENTS, ALLOC, LOAD, READONLY, DATA 5 __ksymtab 00001d68 c0258838 c0258838 00159838 2**2 CONTENTS, ALLOC, LOAD, READONLY, DATA 6 .data 00013abc c025a5a0 c025a5a0 0015b5a0 2**5 CONTENTS, ALLOC, LOAD, DATA 7 .data.init_task 00002000 c0270000 c0270000 00170000 2**5 CONTENTS, ALLOC, LOAD, DATA 8 .text.init 0000f56c c0272000 c0272000 00172000 2**4 CONTENTS, ALLOC, LOAD, READONLY, CODE 9 .data.init 0001de60 c0281580 c0281580 00181580 2**5 CONTENTS, ALLOC, LOAD, DATA 10 .setup.init 00000108 c029f3e0 c029f3e0 0019f3e0 2**2 CONTENTS, ALLOC, LOAD, DATA 11 .initcall.init 00000090 c029f4e8 c029f4e8 0019f4e8 2**2 CONTENTS, ALLOC, LOAD, DATA 12 .data.page_aligned 00000800 c02a0000 c02a0000 001a0000 2**5 CONTENTS, ALLOC, LOAD, DATA 13 .data.cacheline_aligned 00001fe0 c02a0800 c02a0800 001a0800 2**5 CONTENTS, ALLOC, LOAD, DATA 14 .bss 0002b3d8 c02a27e0 c02a27e0 001a27e0 2**5 ALLOC 15 .comment 00003bc9 00000000 00000000 001a27e0 2**0 CONTENTS, READONLY 16 .note 00001a90 00000000 00000000 001a63a9 2**0 CONTENTS, READONLY   * 如下是linux.map的文件内容 * __initcall_start 和 __initcall_end 定义了***.initcall.init***段的起始和结束，并出现在do_initcalls()函数中 * 之前将__initcall()宏的static关键字去掉了，所以__initcall_***这些入口函数地址，比如__initcall_inet_init就全局可见了，我们可以在文件中看到内核启动过程 * 内核开发者们总是喜欢用grep等工具来找某个函数在哪个文件中，而我们在linux.map中可以看到每个函数在哪个模块中，只需要less linux.map就可以了  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66  0xc029f4e8 __initcall_start=. .initcall.init 0xc029f4e8 0x90 *(.initcall.init) .initcall.init 0xc029f4e8 0xc arch/i386/kernel/kernel.o 0xc029f4e8 __initcall_dmi_scan_machine 0xc029f4ec __initcall_cpuid_init 0xc029f4f0 __initcall_apm_init .initcall.init 0xc029f4f4 0x4 kernel/kernel.o 0xc029f4f4 __initcall_uid_cache_init .initcall.init 0xc029f4f8 0xc mm/mm.o 0xc029f4f8 __initcall_kmem_cpucache_init 0xc029f4fc __initcall_kswapd_init 0xc029f500 __initcall_init_shmem_fs .initcall.init 0xc029f504 0x3c fs/fs.o 0xc029f504 __initcall_bdflush_init 0xc029f508 __initcall_init_pipe_fs 0xc029f50c __initcall_fasync_init 0xc029f510 __initcall_filelock_init 0xc029f514 __initcall_dnotify_init 0xc029f518 __initcall_init_misc_binfmt 0xc029f51c __initcall_init_script_binfmt 0xc029f520 __initcall_init_elf_binfmt 0xc029f524 __initcall_init_proc_fs 0xc029f528 __initcall_partition_setup 0xc029f52c __initcall_init_ext2_fs 0xc029f530 __initcall_init_fat_fs 0xc029f534 __initcall_init_msdos_fs 0xc029f538 __initcall_init_iso9660_fs 0xc029f53c __initcall_init_reiserfs_fs .initcall.init 0xc029f540 0x4 drivers/block/block.o 0xc029f540 __initcall_loop_init .initcall.init 0xc029f544 0x4 drivers/char/char.o 0xc029f544 __initcall_rs_init .initcall.init 0xc029f548 0x8 drivers/net/net.o 0xc029f548 __initcall_dummy_init_module 0xc029f54c __initcall_rtl8139_init_module .initcall.init 0xc029f550 0x4 drivers/ide/idedriver.o 0xc029f550 __initcall_ide_cdrom_init .initcall.init 0xc029f554 0x4 drivers/cdrom/driver.o 0xc029f554 __initcall_cdrom_init .initcall.init 0xc029f558 0x4 drivers/pci/driver.o 0xc029f558 __initcall_pci_proc_init .initcall.init 0xc029f55c 0x1c net/network.o 0xc029f55c __initcall_p8022_init 0xc029f560 __initcall_snap_init 0xc029f564 __initcall_inet_init 0xc029f568 __initcall_af_unix_init 0xc029f56c __initcall_netlink_proto_init 0xc029f570 __initcall_packet_init 0xc029f574 __initcall_atalk_init 0xc029f578 __initcall_end=. 0xc02a0000 .=ALIGN(0x1000) 0xc029f578 __init_end=. 0xc02a0000 .=ALIGN(0x1000)   linux.map中的信息可以帮助我们和dmesg的输出信息对照起来，可以看到内核中每个我们感兴趣的静态模块的加载顺序。\n以上工作都是基于linux-2.4内核实现的，新版本内核该如何实现呢？\n","date":"2016-02-24T23:13:13+08:00","permalink":"https://bg2bkk.github.io/p/tcp-ip%E5%8D%8F%E8%AE%AE%E6%A0%88%E5%9C%A8linux%E5%86%85%E6%A0%B8%E5%90%AF%E5%8A%A8%E4%B8%AD%E7%9A%84%E9%A1%BA%E5%BA%8F/","title":"tcp ip协议栈在linux内核启动中的顺序"},{"content":"一、tegine编译+高版本的httpluamodule  tengine-2.1.0的ngx_lua模块随着tengine软件发布，和以往版本的tengine不一样。 tengine自带的ngx_lua模块版本太老，与openresty相比要差几个版本，导致openresty里的一些好的软件工具不可用。  编译tengine+lua时需要手动指定ngx_lua模块和LuaJIT2.1  新版本ngx_lua能弥补openrestysystemtap在probe某些函数时的错误  关键脚本ngx-lua-exec-time.sxx中第58行@pfunc(ngx_http_lua_free_fake_request)函数找不到   使用LuaJIT2.1能解决lua执行时的stap问题。      1 2 3 4 5 6 7 8  #关键脚本stapxx/samples/ngx-lua-exec-time.sxx中第58行@pfunc(ngx_http_lua_free_fake_request)函数找不到的问题： 2\u0026gt; sudo ./samples/ngx-lua-exec-time.sxx -x 11070 semantic error: while resolving probe point: identifier \u0026#39;process\u0026#39; at \u0026lt;input\u0026gt;:58:7 source: process(\u0026#34;/usr/local/nginx/sbin/nginx\u0026#34;).function(\u0026#34;ngx_http_lua_free_fake_request\u0026#34;) ^ semantic error: no match (similar functions: ngx_http_lua_get_request, ngx_http_create_request, ngx_http_free_request, ngx_http_lua_post_subrequest, ngx_http_scgi_create_request) Pass 2: analysis failed. [man error::pass2]   1 2 3 4 5 6 7 8 9 10 11 12  #tegine编译选项 CFLAGS=\u0026#34;-g -O2\u0026#34; ./configure --add-module=/path/ngx_openresty-1.7.10.1/bundle/ngx_lua-0.9.15/ --with-luajit-lib=/usr/local/lib/ --with-luajit-inc=/usr/local/include/luajit-2.1/ --with-ld-opt=-Wl,-rpath,/usr/local/lib make -j16 #新版本ngx_lua在openresty的bundle的ngx_lua-0.9.15/里，也可以从https://github.com/openresty/lua-nginx-module获得 #LuaJIT2.1的源代码也在bundle里，也可以从https://github.com/openresty/luajit2获得。 LuaJIT2.1的编译选项是： make CCDEBUG=-g -B -j8 make -j16   二、axel下载工具和debuginfo.centos.org  我们在安装kernel的debuginfo包的时候，由于只有debuginfo.centos.org上面有rpm包，所以yum源设置为它，但是在国内访问实在是慢，因此推荐使用axel或者mwget下载。mwget顾名思义是多线程的wget工具，下载rpm包非常快，值得推荐。 当我们为了系统中的systemtap安装时，需要安装kernel-debuginfo，这时需要注意严格按照自己的kernel版本来，centos系的需要注意是否2.6.32-431.11.2.el6.toa.2.x86_64后面有toa之类的  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16  #debian系的可以使用dpkg -l linux-image*命令，查看具体内核版本； huang@ThinkPad-X220:~$ dpkg -l linux-image* Desired=Unknown/Install/Remove/Purge/Hold | Status=Not/Inst/Conf-files/Unpacked/halF-conf/Half-inst/trig-aWait/Trig-pend |/ Err?=(none)/Reinst-required (Status,Err: uppercase=bad) ||/ Name Version Architecture Description +++-======================-================-================-================================================== un linux-image \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; (no description available) un linux-image-3.0 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; (no description available) ii linux-image-3.13.0-24- 3.13.0-24.47 amd64 Linux kernel image for version 3.13.0 on 64 bit x8 ii linux-image-3.13.0-24- 3.13.0-24.47 amd64 Linux kernel debug image for version 3.13.0 on 64 ii linux-image-extra-3.13 3.13.0-24.46 amd64 Linux kernel extra modules for version 3.13.0 on 6 ii linux-image-generic 3.13.0.24.28 amd64 Generic Linux kernel image #特别要注意的是3.13.0-24.46、 47、 48，小版本很重要的。   三、centos6.4的gcc版本过低导致的问题  在使用stapxx的工具追踪nginx运行情况时，发现有如下情况，比如：  1 2 3 4 5 6 7 8 9 10 11 12  2\u0026gt; sudo ./samples/ngx-rewrite-latency-distr.sxx -x 11070 semantic error: not accessible at this address [man error::dwarf] (0x44a53b, dieoffset: 0x13a891): identifier \u0026#39;$r\u0026#39; at \u0026lt;input\u0026gt;:67:9 source: r = $r ^ Pass 2: analysis failed. [man error::pass2] # 这个r是没有问题的，在nginx-systemtap-toolkit/ngx-active-reqs中有类似定义： my $c = \u0026#39;@cast(c, \u0026#34;ngx_connection_t\u0026#34;)\u0026#39;; my $r = \u0026#39;@cast(r, \u0026#34;ngx_http_request_t\u0026#34;)\u0026#39;; my $u = \u0026#39;@cast(u, \u0026#34;ngx_http_upstream_t\u0026#34;)\u0026#39;; my $p = \u0026#39;@cast(p, \u0026#34;ngx_event_pipe_t\u0026#34;)\u0026#39;;     产生原因\n tengine的DWARF信息不完整  低版本的gcc在O2时优化掉很多东西，而高版本gcc智能的多      解决方法有两种\n 一种方法是将gcc的编译优化选项降低为O0，以前是O2，则这个ngx_http_request_t的符号，即nginx的DWARF信息可以保留下来。 另一种方法是升级gcc，考虑到在线上服务器升级gcc不太好，这里介绍一种暂时升级gcc的办法    1 2 3 4 5 6  $ sudo wget http://people.centos.org/tru/devtools-1.1/devtools-1.1.repo -P /etc/yum.repos.d $ sudo sh -c \u0026#39;echo \u0026#34;enabled=1\u0026#34; \u0026gt;\u0026gt; /etc/yum.repos.d/devtools-1.1.repo\u0026#39; $ sudo yum install devtoolset-1.1 $ scl enable devtoolset-1.1 bash $ gcc --version # 通过devtoolset工具可以暂时提高gcc版本，而不更改之前服务器的配置，这个很有效果，高版本的gcc会智能保留symbol。   四、apache的压测工具ab升级高版本 在压测过程中，我们想微观的通过tcpdump抓包分析通信过程。\n 发现ab工具在发起keepalive请求时，在完成-n的请求数后额外的发出一个请求，随后又发出F包关闭连接，导致通信过程多出一次。 已有的httpd 2.3自带的ab工具有这个bug，升级httpd2.4后这个问题修复。  五、tcpdump抓取fin包 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15  # tcp包里有个flags字段表示包的类型，tcpdump可以根据该字段抓取相应类型的包： # tcp[13] 就是 TCP flags (URG,ACK,PSH,RST,SYN,FIN) # Unskilled 32 # Attackers 16 # Pester 8 # Real 4 # Security 2 # Folks 1 #抓取fin包： tcpdump -ni any port 9001 and \u0026#39;tcp[13] \u0026amp; 1 != 0 \u0026#39; -s0 -w fin.cap -vvv #抓取syn+fin包： tcpdump -ni any port 9001 and \u0026#39;tcp[13] \u0026amp; 3 != 0 \u0026#39; -s0 -w syn_fin.cap -vvv #抓取rst包： tcpdump -ni any port 9001 and \u0026#39;tcp[13] \u0026amp; 4 != 0 \u0026#39; -s0 -w rst.cap -vvv   参考链接\n六、redis内部监测工具 redis-faina redis-faina是facebook出品的，用于监控redis内部情况统计的一个工具。使用方法是：\n1  redis-cli -p 6379 MONITOR | head -n 100000 | ./redis-faina/redis-faina.py   七、压测过程中发现lua获取用户特征时，如果用户特征不存在，比如headers中的UID参数，事实上它是处于ngx.req.get_headers()函数返回值(table类型)中，如果lua提取用户特征时，找不到UID，则会扩大范围去上一级里找，此时性能会大大下降\n","date":"2016-02-22T11:15:36+08:00","permalink":"https://bg2bkk.github.io/p/openresty%E5%8E%8B%E6%B5%8B%E8%BF%87%E7%A8%8B%E5%92%8Csystemtap%E5%B7%A5%E5%85%B7%E4%BD%BF%E7%94%A8%E4%B8%AD%E7%9A%84%E4%B8%80%E4%BA%9B%E9%97%AE%E9%A2%98/","title":"openresty压测过程和systemtap工具使用中的一些问题"},{"content":"shell脚本多线程应用  同事将新上线APP的一部分log交给我，让我统计下这些log中供出现了哪些deviceid  采用awk就可以实现这部分匹配和统计功能，还是比较简单的 挑战在于，这批log文件非常多非常大，单进程工作处理起来非常的慢。因此我想到了多进程方式。 以往需要使用shell来实现多进程时，采用以下模板    1 2 3 4 5  for seq; do { task }\u0026amp; done   * 当任务较为简单，并发数不多时，这招很管用。然而现在log文件有好几K个，grep处理文件非常耗CPU，上述模板将会按文件数启动进程，系统的CPU Load一跃而起，泪目。 * 此时的情况是，解决问题的思路和方向没有错，方式上还需要改进。关键在于：控制并发任务，合理使用CPU。 * 如何在shell中控制并发进程数呢，我找到这样一个帖子 * http://blog.sciencenet.cn/blog-548663-750136.html   shell脚本中控制并发任务数的大体方式是：  初始化token池，形成一定token空间，又能在为空时阻塞想拿token的进程  生成一个数组，执行任务前先从数组中获得一个元素，能够获得就继续执行，否则阻塞。数组大小最好为CPU核心数。任务执行完成后将元素放回，以供别的进程使用。 生成一个阻塞访问的管道pipe，先向管道中写入若干行，任务执行前从管道中获取token，任务结束后放回。     最终脚本如下所示  我在理解这个脚本的时候感到吃力，比如exec、read等既熟悉又陌生的指令，毕竟没写过shell脚本 后来我发现，man bash和man sh是第一手消息资料  if -p $directory中，-p是什么意思，在man bash的CONDITIONAL EXPRESSIONS中 read -u999中，-u又是什么意思，在man bash的 SHELL BUILTIN COMMANDS的read指令中 exec 999\u0026lt;\u0026gt;$Pfifo中，man bash的REDIRECTION小节中      1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49  #!/bin/sh  awk=/usr/bin/awk uniq=/usr/bin/uniq Nproc=24 #$$是进程pid Pfifo=\u0026#34;/tmp/$$.fifo\u0026#34; mkfifo $Pfifo #以999为文件描述符打开管道,\u0026lt;\u0026gt;表示可读可写 exec 999\u0026lt;\u0026gt;$Pfifo rm -f $Pfifo #向管道中写入Nproc行,作为令牌 for((i=1; i\u0026lt;=$Nproc; i++)); do echo done \u0026gt;\u0026amp;999 echo \u0026#39;\u0026#39; \u0026gt; out echo \u0026#39;\u0026#39; \u0026gt; ooo filenames=`ls *.log` for filename in $filenames; do #从管道中取出1行作为token，如果管道为空，read将会阻塞 #man bash可以知道-u是从fd中读取一行 read -u999 { #所要执行的任务 `$awk -F\u0026#39;,\u0026#39; \u0026#39;/did/ {for(i=1;i\u0026lt;=NF;i++) if($i ~ /did/) print $i i}\u0026#39; $filename | $awk -F\u0026#39;:\u0026#39; \u0026#39;{print $2}\u0026#39; | $awk -F\u0026#39;\u0026#34;\u0026#39; \u0026#39;{print $2}\u0026#39; | $uniq | $awk \u0026#39;{count[$1]++}END{for(name in count)print name \u0026gt;\u0026gt; \u0026#34;out\u0026#34;}\u0026#39;` \u0026amp;\u0026amp; { echo \u0026#34;$filenamedone\u0026#34; } || { echo \u0026#34;$filenameerror\u0026#34; } sleep 1 #归还token echo \u0026gt;\u0026amp;999 }\u0026amp; done #等待所有子进程结束 wait #关闭管道 exec 999\u0026gt;\u0026amp;- echo `$awk \u0026#39;{count[$0]++}END{for(name in count)print name}\u0026#39; out \u0026gt; ooo; awk \u0026#39;END{print NR}\u0026#39; ooo`    单进程方式处理这些log需要3个小时，而控制并发进程数的话只需要10分钟不到。  可见大部分计算资源都浪费在CPU切换上了。    参考链接 linux shell 和 lsof 等工具使用的一些tips linux shell数据输入输出的重定向分析 lsof 一切皆文件\n文件描述符与进程间通信 IO重定向和文件描述符 文件描述符与进程间通信的关联\nlinux shell cocurrency并发控制 Bash脚本实现批量作业并行化\nA script for running processes in parallel in Bash\nawk\u0026amp;sed sed\u0026amp;awk入门 一 sed\u0026amp;awk入门 二 awk用法汇总\n","date":"2016-02-19T10:54:28+08:00","permalink":"https://bg2bkk.github.io/p/linux-shell%E6%8E%A7%E5%88%B6%E5%B9%B6%E5%8F%91%E8%BF%9B%E7%A8%8B%E6%95%B0%E5%AE%9E%E8%B7%B5/","title":"linux shell控制并发进程数实践"},{"content":"实时视频流解决方案 mjpg-streamer site: https://github.com/codewithpassion/mjpg-streamer/tree/master/mjpg-streamer link: http://www.cnblogs.com/hnrainll/archive/2011/06/08/2074909.html link: http://blog.163.com/chenhongswing@126/blog/static/1335924432011825104144612/  1 2 3 4 5 6 7 8 9  #提供一个可以直接使用的demo git clone https://github.com/codewithpassion/mjpg-streamer/tree/master/mjpg-streamer cd mjpg-streamer ./mjpg_streamer -i \u0026#34;input_uvc.so -d /dev/video0 -r 640x480 -y\u0026#34; -o \u0026#34;output_http.so -w ./www\u0026#34; 127.0.0.1:8080访问主页，可以获得stream、static以及通过vlc和mplayer播放   ffmpeg+websocket播放 site: https://github.com/phoboslab/jsmpeg link: http://segmentfault.com/a/1190000000392586  1 2 3 4 5 6 7 8 9 10 11 12 13  #提供可以直接运行的demo git clone https://github.com/phoboslab/jsmpeg nodejs stream-server.js huang\t监听随机端口 Listening for MPEG Stream on http://127.0.0.1:8082/\u0026lt;secret\u0026gt;/\u0026lt;width\u0026gt;/\u0026lt;height\u0026gt; Awaiting WebSocket connections on ws://127.0.0.1:8084/ Stream Connected: 127.0.0.1:52460 size: 640x480 New WebSocket Connection (1 total) Disconnected WebSocket (0 total) ffmpeg -s 640x480 -f video4linux2 -i /dev/video0 -f mpeg1video -b 800k -r 30 http://localhost:8082/huang/640/480/\tffmpeg采集编码并发送视频流 google-chrome stream-example.html\t使用websocket在线观看   vlc视频流输出和vlc视频流播放 site: http://www.cnblogs.com/fx2008/p/4315416.html  1 2 3 4 5 6  a、关掉防火墙，至少将视频流端口开启 b、vlc --ttl 12 -vvv --color -I telnet --telnet-password videolan --rtsp-host 0.0.0.0 --rtsp-port 5554 c、通过telnet启动vlc的vlm管理中，telnet 127.0.0.1 4212，密码是刚才的videolan d、new Test vod enabled 新建一个vod，名字是Test e、setup Test input /path/video.file 给Test输入视频 f、vlc rtsp://127.0.0.1:5554/Test 启动vlc观看视频流，这里还差声音   nginx的rtmp转播 site: http://itony.me/619.html site: https://github.com/arut/nginx-rtmp-module site: http://openresty.org site: http://blog.csdn.net/leixiaohua1020/article/details/12029543 site: http://blog.csdn.net/leixiaohua1020/article/details/39803457 site: http://blog.csdn.net/fireroll/article/details/18899285  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19  a、安装openresty和rtmp模块 b、配置nginx，rtmp配置块与http配置块平级 rtmp { server { listen 1935; application live1 { live on; record off; } } } c、ffmpeg转发视频流 ffmpeg -re -i xxx.mp4 -c copy -f flv rtmp://localhost:1935/live1/room1 其中的live1是应用，room1是将来要打开的节点 d、在vlc中打开视频流： rtmp://localhost:1935/live1/room1 e、ffmpeg -f video4linux2 -i /dev/video0 -c:v libx264 -an -f flv rtmp://localhost:1935/live1/room1 试试摄像头   aliyun+nginx+rtmp在线转播 site: http://blog.csdn.net/xdwyyan/article/details/43198985 注意：在nginx中配置rtmp后，reload是不够的，需要kill掉重新启动nginx。 通过sudo netstat -tlnp | grep 1935来观察nginx是否将端口打开  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37  sudo ffmpeg -i /dev/video0 -acodec acc -strict experimental -ar 44100 -ac 2 -b:a 96k -r 25 -b:v 500k -s 640*480 -f flv rtmp://101.200.124.174:1935/live1/room1 vlc rtmp://101.200.124.174:1935/live1/room1 ffmpeg -loglevel verbose -re -i xxx.mp4 -vcodec libx264 -vprofile baseline -acodec libmp3lame -ar 44100 -ac 1 -f flv rtmp://101.200.124.174:1935/hls/movie vlc http://101.200.124.174/hls/movie.m3u8 #v4l2读取摄像头，进行x264编码并将视频流发送至阿里云，进而进行hls播放 ffmpeg -f video4linux2 -s 320x240 -i /dev/video0 -vcodec libx264 -f flv rtmp://101.200.124.174:1935/hls/movie vlc http://101.200.124.174/hls/movie.m3u8 手机浏览器打开也行 #http config block rtmp { server { listen 1935; application live1 { live on; record off; } application hls{ live on; hls on; hls_path /tmp/hls; } } } #server config block location /hls { types { application/vnd.apple.mpegurl m3u8; video/mp2t ts; } root /tmp; add_header Cache-Control no-cache; }   Extra Info 防火墙设置，配置1985端口可以被外网访问 huang@vultr:~$ sudo iptables -A INPUT -m state --state NEW -m tcp -p tcp --dport 1985 -j ACCEPT  ","date":"2016-02-18T20:57:19+08:00","permalink":"https://bg2bkk.github.io/p/livestream%E7%9A%84%E5%87%A0%E7%A7%8D%E5%88%9D%E6%AD%A5%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88/","title":"livestream的几种初步解决方案"}]